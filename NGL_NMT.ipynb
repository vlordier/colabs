{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NGL-NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zvZvjKWXyhe-",
        "VmKYAZVnTQQp",
        "21dL-BMfUQGK",
        "Ii9bwAzbTMdl",
        "bNTDTy-STWpi",
        "-QZH_37kS9mw",
        "dq_7GlNWTmZw",
        "CvGL3wwkfue2",
        "c-4Qm3wyfKIg"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlordier/colabs/blob/main/NGL_NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJMn9V3InXaA",
        "outputId": "ebbbb77c-0f75-4c33-e447-1cccd37e7813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Data\n"
      ],
      "metadata": {
        "id": "zvZvjKWXyhe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks\")\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError:\n",
        "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
        "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        # Optionally tie weights as in:\n",
        "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "        # https://arxiv.org/abs/1608.05859\n",
        "        # and\n",
        "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
        "        # https://arxiv.org/abs/1611.01462\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
      ],
      "metadata": {
        "id": "6kZvxPElzRO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'dev.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        # assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        return ids"
      ],
      "metadata": {
        "id": "UOusJammy8nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQRnE5_mfu7L",
        "outputId": "7764b080-3db2-4820-af1b-4d6e40bfadf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/  766 batches | lr 20.00 | ms/batch 25.97 | loss  8.06 | ppl  3176.30\n",
            "| epoch   1 |   400/  766 batches | lr 20.00 | ms/batch 25.65 | loss  6.84 | ppl   932.36\n",
            "| epoch   1 |   600/  766 batches | lr 20.00 | ms/batch 25.81 | loss  6.41 | ppl   606.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 21.85s | valid loss  6.19 | valid ppl   490.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.18 | test ppl   484.93\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "# coding: utf-8\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.onnx\n",
        "\n",
        "# import data\n",
        "# import model\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM Language Model')\n",
        "# parser.add_argument('--data', type=str, default='./data',\n",
        "#                     help='location of the data corpus')\n",
        "# parser.add_argument('--model', type=str, default='LSTM',\n",
        "#                     help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)')\n",
        "# parser.add_argument('--emsize', type=int, default=200,\n",
        "#                     help='size of word embeddings')\n",
        "# parser.add_argument('--nhid', type=int, default=200,\n",
        "#                     help='number of hidden units per layer')\n",
        "# parser.add_argument('--nlayers', type=int, default=2,\n",
        "#                     help='number of layers')\n",
        "# parser.add_argument('--lr', type=float, default=20,\n",
        "#                     help='initial learning rate')\n",
        "# parser.add_argument('--clip', type=float, default=0.25,\n",
        "#                     help='gradient clipping')\n",
        "# parser.add_argument('--epochs', type=int, default=40,\n",
        "#                     help='upper epoch limit')\n",
        "# parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
        "#                     help='batch size')\n",
        "# parser.add_argument('--bptt', type=int, default=35,\n",
        "#                     help='sequence length')\n",
        "# parser.add_argument('--dropout', type=float, default=0.2,\n",
        "#                     help='dropout applied to layers (0 = no dropout)')\n",
        "# parser.add_argument('--tied', action='store_true',\n",
        "#                     help='tie the word embedding and softmax weights')\n",
        "# parser.add_argument('--seed', type=int, default=1111,\n",
        "#                     help='random seed')\n",
        "# parser.add_argument('--cuda', action='store_true',\n",
        "#                     help='use CUDA')\n",
        "# parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
        "#                     help='report interval')\n",
        "# parser.add_argument('--save', type=str, default='model.pt',\n",
        "#                     help='path to save the final model')\n",
        "# parser.add_argument('--onnx-export', type=str, default='',\n",
        "#                     help='path to export the final model in onnx format')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "args = {}\n",
        "args['data'] = 'data'\n",
        "args['model'] = 'LSTM'\n",
        "args['emsize'] = 200\n",
        "args['nhid'] = 200\n",
        "args['nlayers'] = 2\n",
        "args['lr'] = 20\n",
        "args['clip'] = 0.25\n",
        "args['epochs'] = 1\n",
        "args['batch_size'] = 20\n",
        "args['bptt'] = 35\n",
        "args['dropout'] = 0.2\n",
        "args['tied'] = True\n",
        "args['seed'] = 1111\n",
        "args['cuda'] = True\n",
        "args['log_interval'] = 200\n",
        "args['save'] = 'model.pt'\n",
        "args['onnx_export'] = ''\n",
        "\n",
        "\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(args['seed'])\n",
        "if torch.cuda.is_available():\n",
        "    if not args['cuda']:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "device = torch.device(\"cuda\" if args['cuda'] else \"cpu\")\n",
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "corpus = Corpus(args['data'])\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, args['batch_size'])\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = RNNModel(args['model'], ntokens, args['emsize'], args['nhid'], args['nlayers'], args['dropout'], args['tied']).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length args.bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# └ b h n t ┘ └ c i o u ┘\n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(args['bptt'], len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, args['bptt']):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "            hidden = repackage_hidden(hidden)\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(args['batch_size'])\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args['bptt'])):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip'])\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % args['log_interval'] == 0 and batch > 0:\n",
        "            cur_loss = total_loss / args['log_interval']\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // args['bptt'], lr,\n",
        "                elapsed * 1000 / args['log_interval'], cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def export_onnx(path, batch_size, seq_len):\n",
        "    print('The model is also exported in ONNX format at {}'.\n",
        "          format(os.path.realpath(args['onnx_export'])))\n",
        "    model.eval()\n",
        "    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    torch.onnx.export(model, (dummy_input, hidden), path)\n",
        "\n",
        "\n",
        "# Loop over epochs.\n",
        "lr = args['lr']\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, args['epochs']+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(args['save'], 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args['save'], 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "if len(args['onnx_export']) > 0:\n",
        "    # Export the model in ONNX format.\n",
        "    export_onnx(args['onnx_export'], batch_size=1, seq_len=args['bptt'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create_data.py"
      ],
      "metadata": {
        "id": "VmKYAZVnTQQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import csv\n",
        "import os\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "input_file = '/content/drive/MyDrive/Colab Notebooks/data/the_office/the_office_scripts.csv'\n",
        "NAME_IND = 5\n",
        "input_file = '/content/drive/MyDrive/Colab Notebooks/data/the_office/the_office_scripts.csv'\n",
        "train_file = '/content/drive/MyDrive/Colab Notebooks/data/train.txt'\n",
        "valid_file = '/content/drive/MyDrive/Colab Notebooks/data/valid.txt'\n",
        "test_file = '/content/drive/MyDrive/Colab Notebooks/data/test.txt'\n",
        "\n",
        "def get_num_lines():\n",
        "    num_lines = 0\n",
        "    with open(input_file, encoding='ISO-8859-1') as input:\n",
        "        reader = csv.reader(input)\n",
        "        next(reader)\n",
        "        for row in enumerate(reader):\n",
        "            num_lines += 1\n",
        "    return num_lines\n",
        "\n",
        "def create(num_lines):\n",
        "    with open(input_file, encoding='ISO-8859-1') as input:\n",
        "        reader = csv.reader(input)\n",
        "        next(reader)\n",
        "        train = open(train_file, 'w')\n",
        "        valid = open(valid_file, 'w')\n",
        "        test = open(test_file, 'w')\n",
        "        for i, row in enumerate(reader):\n",
        "            if i < 0.8 * num_lines:\n",
        "                train.write(row[NAME_IND].strip() + '\\n')\n",
        "            elif i < 0.9 * num_lines:\n",
        "                valid.write(row[NAME_IND].strip() + '\\n')\n",
        "            else:\n",
        "                test.write(row[NAME_IND].strip() + '\\n')\n",
        "        train.close()\n",
        "        valid.close()\n",
        "        test.close()\n",
        "\n",
        "def main():\n",
        "    if os.stat(train_file).st_size == 0:\n",
        "        print('Adding names to data files')\n",
        "        num_lines = get_num_lines()\n",
        "        create(num_lines)\n",
        "    else:\n",
        "        print('Already added names to data files')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "sljQY2Oz2QrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2227a442-ecba-4c6a-d7cd-735c02dbf0f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already added names to data files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# generate.py"
      ],
      "metadata": {
        "id": "21dL-BMfUQGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "import torch\n",
        "\n",
        "# import data\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 Language Model')\n",
        "\n",
        "# # Model parameters.\n",
        "# parser.add_argument('--data', type=str, default='./data',\n",
        "#                     help='location of the data corpus')\n",
        "# parser.add_argument('--checkpoint', type=str, default='./model.pt',\n",
        "#                     help='model checkpoint to use')\n",
        "# parser.add_argument('--outf', type=str, default='generated.txt',\n",
        "#                     help='output file for generated text')\n",
        "# parser.add_argument('--words', type=int, default='1000',\n",
        "#                     help='number of words to generate')\n",
        "# parser.add_argument('--seed', type=int, default=1111,\n",
        "#                     help='random seed')\n",
        "# parser.add_argument('--cuda', action='store_true',\n",
        "#                     help='use CUDA')\n",
        "# parser.add_argument('--temperature', type=float, default=1.0,\n",
        "#                     help='temperature - higher will increase diversity')\n",
        "# parser.add_argument('--log-interval', type=int, default=100,\n",
        "#                     help='reporting interval')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "args = {}\n",
        "args['data'] = 'data'\n",
        "args['checkpoint'] = './model.pt'\n",
        "args['outf'] = 'generated.txt'\n",
        "args['words'] = 1000\n",
        "args['seed'] = 1111\n",
        "args['cuda'] = True\n",
        "args['temperature'] = 1.0\n",
        "args['log_interval'] = 100\n",
        "\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(args['seed'])\n",
        "if torch.cuda.is_available():\n",
        "    if not args['cuda']:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "device = torch.device(\"cuda\" if args['cuda'] else \"cpu\")\n",
        "\n",
        "if args['temperature'] < 1e-3:\n",
        "    parser.error(\"--temperature has to be greater or equal 1e-3\")\n",
        "\n",
        "with open(args['checkpoint'], 'rb') as f:\n",
        "    model = torch.load(f).to(device)\n",
        "model.eval()\n",
        "\n",
        "corpus = Corpus(args['data'])\n",
        "ntokens = len(corpus.dictionary)\n",
        "hidden = model.init_hidden(1)\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "with open(args['outf'], 'w') as outf:\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        for i in range(args['words']):\n",
        "            output, hidden = model(input, hidden)\n",
        "            word_weights = output.squeeze().div(args['temperature']).exp().cpu()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            input.fill_(word_idx)\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "            if i % args['log_interval'] == 0:\n",
        "                print('| Generated {}/{} words'.format(i, args['words']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7__QKUyET4ZB",
        "outputId": "ca847980-bd41-466f-cc04-bc1e8e007b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NMT"
      ],
      "metadata": {
        "id": "oBxZiGqUWCwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils.py"
      ],
      "metadata": {
        "id": "pRHoOCHUWNJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 5\n",
        "nmt.py: NMT Model\n",
        "Pencheng Yin <pcyin@cs.cmu.edu>\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def pad_sents_char(sents, char_pad_token):\n",
        "    \"\"\" Pad list of sentences according to the longest sentence in the batch and max_word_length.\n",
        "    @param sents (list[list[list[int]]]): list of sentences, result of `words2charindices()` \n",
        "        from `vocab.py`\n",
        "    @param char_pad_token (int): index of the character-padding token\n",
        "    @returns sents_padded (list[list[list[int]]]): list of sentences where sentences/words shorter\n",
        "        than the max length sentence/word are padded out with the appropriate pad token, such that\n",
        "        each sentence in the batch now has same number of words and each word has an equal \n",
        "        number of characters\n",
        "        Output shape: (batch_size, max_sentence_length, max_word_length)\n",
        "    \"\"\"\n",
        "    # Words longer than 21 characters should be truncated\n",
        "    max_word_length = 21 \n",
        "\n",
        "    ### YOUR CODE HERE for part 1f\n",
        "    ### TODO:\n",
        "    ###     Perform necessary padding to the sentences in the batch similar to the pad_sents() \n",
        "    ###     method below using the padding character from the arguments. You should ensure all \n",
        "    ###     sentences have the same number of words and each word has the same number of \n",
        "    ###     characters. \n",
        "    ###     Set padding words to a `max_word_length` sized vector of padding characters.  \n",
        "    ###\n",
        "    ###     You should NOT use the method `pad_sents()` below because of the way it handles \n",
        "    ###     padding and unknown words.\n",
        "    sents_padded = []\n",
        "    max_sent_length = max(len(s) for s in sents)\n",
        "\n",
        "    for s in sents:\n",
        "        words_padded = []\n",
        "        for w in s:\n",
        "            padded = [char_pad_token] * max_word_length\n",
        "            padded[:len(w)] = w[:max_word_length]\n",
        "            words_padded.append(padded)\n",
        "        while len(words_padded) != max_sent_length:\n",
        "            words_padded.append([char_pad_token] * max_word_length)\n",
        "        sents_padded.append(words_padded)\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return sents_padded\n",
        "\n",
        "\n",
        "def pad_sents(sents, pad_token):\n",
        "    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n",
        "    @param sents (list[list[int]]): list of sentences, where each sentence\n",
        "                                    is represented as a list of words\n",
        "    @param pad_token (int): padding token\n",
        "    @returns sents_padded (list[list[int]]): list of sentences where sentences shorter\n",
        "        than the max length sentence are padded out with the pad_token, such that\n",
        "        each sentences in the batch now has equal length.\n",
        "        Output shape: (batch_size, max_sentence_length)\n",
        "    \"\"\"\n",
        "    sents_padded = []\n",
        "\n",
        "    max_len = max(len(s) for s in sents)\n",
        "    batch_size = len(sents)\n",
        "\n",
        "    for s in sents:\n",
        "        padded = [pad_token] * max_len\n",
        "        padded[:len(s)] = s\n",
        "        sents_padded.append(padded)\n",
        "\n",
        "    return sents_padded\n",
        "\n",
        "def read_corpus(file_path, source):\n",
        "    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
        "    @param file_path (str): path to file containing corpus\n",
        "    @param source (str): \"tgt\" or \"src\" indicating whether text\n",
        "        is of the source language or target language\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for line in open(file_path):\n",
        "        sent = line.strip().split(' ')\n",
        "        # only append <s> and </s> to the target sentence\n",
        "        if source == 'tgt':\n",
        "            sent = ['<s>'] + sent + ['</s>']\n",
        "        data.append(sent)\n",
        "\n",
        "    return data\n",
        "\n",
        "def read_corpus_nlg(file_path):\n",
        "    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
        "    @param file_path (str): path to file containing corpus\n",
        "    @param source (str): \"tgt\" or \"src\" indicating whether text\n",
        "        is of the source language or target language\n",
        "    \"\"\"\n",
        "    speakers = []\n",
        "    src = []\n",
        "    tgt = []\n",
        "    prevLine = None\n",
        "    for line in open(file_path):\n",
        "        if prevLine != None:\n",
        "            sent = line.strip().replace(\"\\n\", \"\").split(' ')\n",
        "            i = 0\n",
        "            while \":\" not in sent[i]:\n",
        "                i += 1\n",
        "            speaker = sent[i][:-1]\n",
        "            if speaker not in speakers:\n",
        "                speakers.append(speaker)\n",
        "            # only append <s> and </s> to the target sentence\n",
        "            sent = ['<s>'] + sent + ['</s>']\n",
        "            tgt.append(sent)\n",
        "            src.append(prevLine)\n",
        "            prevLine = sent[1:-1]\n",
        "        else:\n",
        "            sent = line.strip().replace(\"\\n\", \"\").split(' ')\n",
        "            speaker = sent[0][:-1]\n",
        "            if speaker not in speakers:\n",
        "                speakers.append(speaker)\n",
        "            prevLine = sent\n",
        "\n",
        "    print(speakers)\n",
        "    return speakers, src, tgt\n",
        "\n",
        "def batch_iter(data, batch_size, shuffle=False):\n",
        "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
        "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
        "    @param batch_size (int): batch size\n",
        "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
        "    \"\"\"\n",
        "    batch_num = math.ceil(len(data) / batch_size)\n",
        "    index_array = list(range(len(data)))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(index_array)\n",
        "\n",
        "    for i in range(batch_num):\n",
        "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
        "        examples = [data[idx] for idx in indices]\n",
        "\n",
        "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
        "        src_sents = [e[0] for e in examples]\n",
        "        tgt_sents = [e[1] for e in examples]\n",
        "\n",
        "        yield src_sents, tgt_sents"
      ],
      "metadata": {
        "id": "h88zeJzFU2PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vocab.py"
      ],
      "metadata": {
        "id": "Vm4xscTZWgN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 5\n",
        "vocab.py: Vocabulary Generation\n",
        "Pencheng Yin <pcyin@cs.cmu.edu>\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\n",
        "Usage:\n",
        "    vocab.py --train-src=<file> --train-tgt=<file> [options] VOCAB_FILE\n",
        "\n",
        "Options:\n",
        "    -h --help                  Show this screen.\n",
        "    --train-src=<file>         File of training source sentences\n",
        "    --train-tgt=<file>         File of training target sentences\n",
        "    --size=<int>               vocab size [default: 50000]\n",
        "    --freq-cutoff=<int>        frequency cutoff [default: 2]\n",
        "\"\"\"\n",
        "\n",
        "from collections import Counter\n",
        "from docopt import docopt\n",
        "from itertools import chain\n",
        "import json\n",
        "import torch\n",
        "from typing import List\n",
        "# from utils import read_corpus, pad_sents, pad_sents_char\n",
        "\n",
        "class VocabEntry(object):\n",
        "    \"\"\" Vocabulary Entry, i.e. structure containing either\n",
        "    src or tgt language terms.\n",
        "    \"\"\"\n",
        "    def __init__(self, word2id=None):\n",
        "        \"\"\" Init VocabEntry Instance.\n",
        "        @param word2id (dict): dictionary mapping words 2 indices\n",
        "        \"\"\"\n",
        "        if word2id:\n",
        "            self.word2id = word2id\n",
        "        else:\n",
        "            self.word2id = dict()\n",
        "            self.word2id['<pad>'] = 0   # Pad Token\n",
        "            self.word2id['<s>'] = 1 # Start Token\n",
        "            self.word2id['</s>'] = 2    # End Token\n",
        "            self.word2id['<unk>'] = 3   # Unknown Token\n",
        "        self.unk_id = self.word2id['<unk>']\n",
        "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
        "        \n",
        "        ## Additions to the A4 code:\n",
        "        self.char_list = list(\"\"\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]\"\"\")\n",
        "        self.char_list.append('\\t')\n",
        "        # print(self.char_list)\n",
        "        self.char2id = dict() # Converts characters to integers\n",
        "        self.char2id['<pad>'] = 0\n",
        "        self.char2id['{'] = 1\n",
        "        self.char2id['}'] = 2\n",
        "        self.char2id['<unk>'] = 3\n",
        "        for i, c in enumerate(self.char_list):\n",
        "            self.char2id[c] = len(self.char2id)\n",
        "        self.char_unk = self.char2id['<unk>']\n",
        "        self.start_of_word = self.char2id[\"{\"]\n",
        "        self.end_of_word = self.char2id[\"}\"]\n",
        "        assert self.start_of_word+1 == self.end_of_word\n",
        "\n",
        "        self.id2char = {v: k for k, v in self.char2id.items()} # Converts integers to characters\n",
        "        ## End additions to the A4 code\n",
        "\n",
        "    def __getitem__(self, word):\n",
        "        \"\"\" Retrieve word's index. Return the index for the unk\n",
        "        token if the word is out of vocabulary.\n",
        "        @param word (str): word to look up.\n",
        "        @returns index (int): index of word \n",
        "        \"\"\"\n",
        "        return self.word2id.get(word, self.unk_id)\n",
        "\n",
        "    def __contains__(self, word):\n",
        "        \"\"\" Check if word is captured by VocabEntry.\n",
        "        @param word (str): word to look up\n",
        "        @returns contains (bool): whether word is contained    \n",
        "        \"\"\"\n",
        "        return word in self.word2id\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        \"\"\" Raise error, if one tries to edit the VocabEntry.\n",
        "        \"\"\"\n",
        "        raise ValueError('vocabulary is readonly')\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Compute number of words in VocabEntry.\n",
        "        @returns len (int): number of words in VocabEntry\n",
        "        \"\"\"\n",
        "        return len(self.word2id)\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\" Representation of VocabEntry to be used\n",
        "        when printing the object.\n",
        "        \"\"\"\n",
        "        return 'Vocabulary[size=%d]' % len(self)\n",
        "\n",
        "    def id2word(self, wid):\n",
        "        \"\"\" Return mapping of index to word.\n",
        "        @param wid (int): word index\n",
        "        @returns word (str): word corresponding to index\n",
        "        \"\"\"\n",
        "        return self.id2word[wid]\n",
        "\n",
        "    def add(self, word):\n",
        "        \"\"\" Add word to VocabEntry, if it is previously unseen.\n",
        "        @param word (str): word to add to VocabEntry\n",
        "        @return index (int): index that the word has been assigned\n",
        "        \"\"\"\n",
        "        if word not in self:\n",
        "            wid = self.word2id[word] = len(self)\n",
        "            self.id2word[wid] = word\n",
        "            return wid\n",
        "        else:\n",
        "            return self[word]\n",
        "\n",
        "    def words2charindices(self, sents):\n",
        "        \"\"\" Convert list of sentences of words into list of list of list of character indices.\n",
        "        @param sents (list[list[str]]): sentence(s) in words\n",
        "        @return word_ids (list[list[list[int]]]): sentence(s) in indices\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE for part 1e\n",
        "        ### TODO: \n",
        "        ###     This method should convert characters in the input sentences into their \n",
        "        ###     corresponding character indices using the character vocabulary char2id \n",
        "        ###     defined above.\n",
        "        ###\n",
        "        ###     You must prepend each word with the `start_of_word` character and append \n",
        "        ###     with the `end_of_word` character. \n",
        "\n",
        "        return [[[self.char2id[c] for c in (self.id2char[self.start_of_word] + w + self.id2char[self.end_of_word])] for w in s] for s in sents]\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def words2indices(self, sents):\n",
        "        \"\"\" Convert list of sentences of words into list of list of indices.\n",
        "        @param sents (list[list[str]]): sentence(s) in words\n",
        "        @return word_ids (list[list[int]]): sentence(s) in indices\n",
        "        \"\"\"\n",
        "        return [[self[w] for w in s] for s in sents]\n",
        "\n",
        "    def indices2words(self, word_ids):\n",
        "        \"\"\" Convert list of indices into words.\n",
        "        @param word_ids (list[int]): list of word ids\n",
        "        @return sents (list[str]): list of words\n",
        "        \"\"\"\n",
        "        return [self.id2word[w_id] for w_id in word_ids]\n",
        "\n",
        "    def to_input_tensor_char(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
        "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n",
        "        shorter sentences.\n",
        "\n",
        "        @param sents (List[List[str]]): list of sentences (words)\n",
        "        @param device: device on which to load the tensor, i.e. CPU or GPU\n",
        "\n",
        "        @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE for part 1g\n",
        "        ### TODO: \n",
        "        ###     Connect `words2charindices()` and `pad_sents_char()` which you've defined in \n",
        "        ###     previous parts\n",
        "        embedding = self.words2charindices(sents)\n",
        "        padded = pad_sents_char(embedding, self.char2id['<pad>'])\n",
        "        #print(len(padded))\n",
        "        #print(max(len(i) for i in padded))\n",
        "        #print(len(padded[0][0]))\n",
        "        tensor = torch.tensor(padded, dtype=torch.long, device=device)\n",
        "        tensor = tensor.permute(1, 0, 2)\n",
        "        #print(tensor.size())\n",
        "        return tensor\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
        "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n",
        "        shorter sentences.\n",
        "\n",
        "        @param sents (List[List[str]]): list of sentences (words)\n",
        "        @param device: device on which to load the tesnor, i.e. CPU or GPU\n",
        "\n",
        "        @returns sents_var: tensor of (max_sentence_length, batch_size)\n",
        "        \"\"\"\n",
        "        word_ids = self.words2indices(sents)\n",
        "        sents_t = pad_sents(word_ids, self['<pad>'])\n",
        "        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n",
        "        return torch.t(sents_var)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_corpus(corpus, size, freq_cutoff=2):\n",
        "        \"\"\" Given a corpus construct a Vocab Entry.\n",
        "        @param corpus (list[str]): corpus of text produced by read_corpus function\n",
        "        @param size (int): # of words in vocabulary\n",
        "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n",
        "        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n",
        "        \"\"\"\n",
        "        vocab_entry = VocabEntry()\n",
        "        word_freq = Counter(chain(*corpus))\n",
        "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
        "        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n",
        "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
        "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n",
        "        for word in top_k_words:\n",
        "            vocab_entry.add(word)\n",
        "        return vocab_entry\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    \"\"\" Vocab encapsulating src and target langauges.\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n",
        "        \"\"\" Init Vocab.\n",
        "        @param src_vocab (VocabEntry): VocabEntry for source language\n",
        "        @param tgt_vocab (VocabEntry): VocabEntry for target language\n",
        "        \"\"\"\n",
        "        self.src = src_vocab\n",
        "        self.tgt = tgt_vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def build(src_sents, tgt_sents, vocab_size, freq_cutoff) -> 'Vocab':\n",
        "        \"\"\" Build Vocabulary.\n",
        "        @param src_sents (list[str]): Source sentences provided by read_corpus() function\n",
        "        @param tgt_sents (list[str]): Target sentences provided by read_corpus() function\n",
        "        @param vocab_size (int): Size of vocabulary for both source and target languages\n",
        "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word.\n",
        "        \"\"\"\n",
        "\n",
        "        assert len(src_sents) == len(tgt_sents)\n",
        "\n",
        "        print('initialize source vocabulary ..')\n",
        "        src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n",
        "\n",
        "        print('initialize target vocabulary ..')\n",
        "        tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n",
        "\n",
        "        return Vocab(src, tgt)\n",
        "\n",
        "    def save(self, file_path):\n",
        "        \"\"\" Save Vocab to file as JSON dump.\n",
        "        @param file_path (str): file path to vocab file\n",
        "        \"\"\"\n",
        "        json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), open(file_path, 'w'), indent=2)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(file_path):\n",
        "        \"\"\" Load vocabulary from JSON dump.\n",
        "        @param file_path (str): file path to vocab file\n",
        "        @returns Vocab object loaded from JSON dump\n",
        "        \"\"\"\n",
        "        entry = json.load(open(file_path, 'r'))\n",
        "        src_word2id = entry['src_word2id']\n",
        "        tgt_word2id = entry['tgt_word2id']\n",
        "\n",
        "        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\" Representation of Vocab to be used\n",
        "        when printing the object.\n",
        "        \"\"\"\n",
        "        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # args = docopt(__doc__)\n",
        "    args = {}\n",
        "    args['train-src'] = 'en_es_data/train.es'\n",
        "    args['train-tgt'] = 'en_es_data/train.en'\n",
        "    args['size'] = 200\n",
        "    args['freq-cutoff'] = 1\n",
        "    args['VOCAB_FILE'] = 'vocab.json'\n",
        "\n",
        "    print('read in source sentences: %s' % args['train-src'])\n",
        "    print('read in target sentences: %s' % args['train-tgt'])\n",
        "\n",
        "    src_sents = read_corpus(args['train-src'], source='src')\n",
        "    tgt_sents = read_corpus(args['train-tgt'], source='tgt')\n",
        "\n",
        "    vocab = Vocab.build(src_sents, tgt_sents, int(args['size']), int(args['freq-cutoff']))\n",
        "    print('generated vocabulary, source %d words, target %d words' % (len(vocab.src), len(vocab.tgt)))\n",
        "\n",
        "    vocab.save(args['VOCAB_FILE'])\n",
        "    print('vocabulary saved to %s' % args['VOCAB_FILE'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz5n4HwMWjPT",
        "outputId": "0a24ca50-3f2b-48ee-fbb1-df8f80393229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read in source sentences: en_es_data/train.es\n",
            "read in target sentences: en_es_data/train.en\n",
            "initialize source vocabulary ..\n",
            "number of word types: 14256, number of word types w/ frequency >= 1: 14256\n",
            "initialize target vocabulary ..\n",
            "number of word types: 17724, number of word types w/ frequency >= 1: 17724\n",
            "generated vocabulary, source 204 words, target 202 words\n",
            "vocabulary saved to vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn.py"
      ],
      "metadata": {
        "id": "Ii9bwAzbTMdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 5\n",
        "\"\"\"\n",
        "\n",
        "### YOUR CODE HERE for part 1i\n",
        "from collections import namedtuple\n",
        "import sys\n",
        "from typing import List, Tuple, Dict, Set, Union\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, e_char, filters, kernel_size=5):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.e_char = e_char\n",
        "        self.kernel_size = kernel_size\n",
        "        self.filters = filters\n",
        "\n",
        "        self.conv_layer = nn.Conv1d(e_char, filters, kernel_size, bias=True)\n",
        "\n",
        "    def forward(self, x_reshaped) -> torch.Tensor:\n",
        "        x_conv = self.conv_layer(x_reshaped)\n",
        "        x_conv_out = torch.max(F.relu(x_conv), 2)[0]\n",
        "        return x_conv_out\n",
        "\n",
        "### END YOUR CODE"
      ],
      "metadata": {
        "id": "i1t_Lk_WTKKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## highway.py"
      ],
      "metadata": {
        "id": "bNTDTy-STWpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 5\n",
        "\"\"\"\n",
        "\n",
        "### YOUR CODE HERE for part 1h\n",
        "from collections import namedtuple\n",
        "import sys\n",
        "from typing import List, Tuple, Dict, Set, Union\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "\n",
        "class Highway(nn.Module):\n",
        "    def __init__(self, e_word):\n",
        "        super(Highway, self).__init__()\n",
        "        \n",
        "        self.e_word = e_word\n",
        "\n",
        "        self.proj = nn.Linear(self.e_word, self.e_word, bias=True)\n",
        "        self.gate = nn.Linear(self.e_word, self.e_word, bias=True)\n",
        "\n",
        "    def forward(self, x_conv_out) -> torch.Tensor:\n",
        "        x_gate = torch.sigmoid(self.gate(x_conv_out))\n",
        "        x_proj = F.relu(self.proj(x_conv_out))\n",
        "        x_highway = x_gate * x_proj + (1 - x_gate) * x_proj\n",
        "        return x_highway\n",
        "\n",
        "### END YOUR CODE "
      ],
      "metadata": {
        "id": "HwNOQHXjTVwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model_embeddings.py"
      ],
      "metadata": {
        "id": "-QZH_37kS9mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 5\n",
        "model_embeddings.py: Embeddings for the NMT model\n",
        "Pencheng Yin <pcyin@cs.cmu.edu>\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "Anand Dhoot <anandd@stanford.edu>\n",
        "Michael Hahn <mhahn2@stanford.edu>\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "# Do not change these imports; your module names should be\n",
        "#   `CNN` in the file `cnn.py`\n",
        "#   `Highway` in the file `highway.py`\n",
        "# Uncomment the following two imports once you're ready to run part 1(j)\n",
        "\n",
        "# from cnn import CNN\n",
        "# from highway import Highway\n",
        "\n",
        "# End \"do not change\" \n",
        "\n",
        "class ModelEmbeddings(nn.Module): \n",
        "    \"\"\"\n",
        "    Class that converts input words to their CNN-based embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, vocab):\n",
        "        \"\"\"\n",
        "        Init the Embedding layer for one language\n",
        "        @param embed_size (int): Embedding size (dimensionality) for the output \n",
        "        @param vocab (VocabEntry): VocabEntry object. See vocab.py for documentation.\n",
        "        \"\"\"\n",
        "        super(ModelEmbeddings, self).__init__()\n",
        "\n",
        "        ## A4 code\n",
        "        # pad_token_idx = vocab.src['<pad>']\n",
        "        # self.embeddings = nn.Embedding(len(vocab.src), embed_size, padding_idx=pad_token_idx)\n",
        "        ## End A4 code\n",
        "\n",
        "        ### YOUR CODE HERE for part 1j\n",
        "        pad_token_idx = vocab.char2id['<pad>']\n",
        "        self.e_char = 50\n",
        "        self.embed_size = embed_size\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.embeddings = nn.Embedding(len(self.vocab.char2id), self.e_char, padding_idx=pad_token_idx)\n",
        "        self.cnn = CNN(self.e_char, self.embed_size)\n",
        "        self.highway = Highway(self.embed_size)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Looks up character-based CNN embeddings for the words in a batch of sentences.\n",
        "        @param input: Tensor of integers of shape (sentence_length, batch_size, max_word_length) where\n",
        "            each integer is an index into the character vocabulary\n",
        "\n",
        "        @param output: Tensor of shape (sentence_length, batch_size, embed_size), containing the \n",
        "            CNN-based embeddings for each word of the sentences in the batch\n",
        "        \"\"\"\n",
        "        ## A4 code\n",
        "        # output = self.embeddings(input)\n",
        "        # return output\n",
        "        ## End A4 code\n",
        "\n",
        "        ### YOUR CODE HERE for part 1j\n",
        "        #print(self.embed_size)\n",
        "        #print(input.size())\n",
        "        sentence_length = input.size()[0]\n",
        "        batch_size = input.size()[1]\n",
        "        e = self.embeddings(input)\n",
        "        #print(e.size())\n",
        "        e = e.permute(0, 1, 3, 2)\n",
        "        e = e.contiguous()\n",
        "        e = e.view(-1, e.size()[2], e.size()[3])\n",
        "        #print(e.size())\n",
        "        x_conv_out = self.cnn.forward(e)\n",
        "        #print(x_conv_out.size())\n",
        "        x_highway = self.highway.forward(x_conv_out)\n",
        "        #print(x_highway.size())\n",
        "        x_word_emb = self.dropout(x_highway)\n",
        "        #print(x_word_emb.size())\n",
        "        x_word_emb = x_word_emb.view(sentence_length, batch_size, self.embed_size)\n",
        "        return x_word_emb\n",
        "\n",
        "        ### END YOUR CODE"
      ],
      "metadata": {
        "id": "VKsrFPwpTAGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## char_decoder.py"
      ],
      "metadata": {
        "id": "dq_7GlNWTmZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 5\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CharDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size, char_embedding_size=50, target_vocab=None):\n",
        "        \"\"\" Init Character Decoder.\n",
        "\n",
        "        @param hidden_size (int): Hidden size of the decoder LSTM\n",
        "        @param char_embedding_size (int): dimensionality of character embeddings\n",
        "        @param target_vocab (VocabEntry): vocabulary for the target language. See vocab.py for documentation.\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE for part 2a\n",
        "        ### TODO - Initialize as an nn.Module.\n",
        "        ###      - Initialize the following variables:\n",
        "        ###        self.charDecoder: LSTM. Please use nn.LSTM() to construct this.\n",
        "        ###        self.char_output_projection: Linear layer, called W_{dec} and b_{dec} in the PDF\n",
        "        ###        self.decoderCharEmb: Embedding matrix of character embeddings\n",
        "        ###        self.target_vocab: vocabulary for the target language\n",
        "        ###\n",
        "        ### Hint: - Use target_vocab.char2id to access the character vocabulary for the target language.\n",
        "        ###       - Set the padding_idx argument of the embedding matrix.\n",
        "        ###       - Create a new Embedding layer. Do not reuse embeddings created in Part 1 of this assignment.\n",
        "        super(CharDecoder, self).__init__()\n",
        "\n",
        "        self.target_vocab = target_vocab\n",
        "        self.charDecoder = nn.LSTM(char_embedding_size, hidden_size, bidirectional=False)\n",
        "        self.char_output_projection = nn.Linear(hidden_size, len(self.target_vocab.char2id), bias=True)\n",
        "        pad_token_idx = self.target_vocab.char2id['<pad>']\n",
        "        self.decoderCharEmb = nn.Embedding(len(self.target_vocab.char2id), char_embedding_size, padding_idx=pad_token_idx)\n",
        "\n",
        "        ### END YOUR CODE\n",
        "    \n",
        "    def forward(self, input, dec_hidden=None):\n",
        "        \"\"\" Forward pass of character decoder.\n",
        "\n",
        "        @param input: tensor of integers, shape (length, batch)\n",
        "        @param dec_hidden: internal state of the LSTM before reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)\n",
        "\n",
        "        @returns scores: called s_t in the PDF, shape (length, batch, self.vocab_size)\n",
        "        @returns dec_hidden: internal state of the LSTM after reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE for part 2b\n",
        "        ### TODO - Implement the forward pass of the character decoder.\n",
        "        embeddings = self.decoderCharEmb(input)\n",
        "        dec_hidden, dec_cell = self.charDecoder(embeddings, dec_hidden)\n",
        "        scores = self.char_output_projection(dec_hidden)\n",
        "\n",
        "        return (scores, dec_cell)\n",
        "        \n",
        "        ### END YOUR CODE \n",
        "\n",
        "\n",
        "    def train_forward(self, char_sequence, dec_hidden=None):\n",
        "        \"\"\" Forward computation during training.\n",
        "\n",
        "        @param char_sequence: tensor of integers, shape (length, batch). Note that \"length\" here and in forward() need not be the same.\n",
        "        @param dec_hidden: initial internal state of the LSTM, obtained from the output of the word-level decoder. A tuple of two tensors of shape (1, batch, hidden_size)\n",
        "\n",
        "        @returns The cross-entropy loss, computed as the *sum* of cross-entropy losses of all the words in the batch.\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE for part 2c\n",
        "        ### TODO - Implement training forward pass.\n",
        "        ###\n",
        "        ### Hint: - Make sure padding characters do not contribute to the cross-entropy loss.\n",
        "        ###       - char_sequence corresponds to the sequence x_1 ... x_{n+1} from the handout (e.g., <START>,m,u,s,i,c,<END>).\n",
        "        #print(char_sequence)\n",
        "        #print(char_sequence.size())\n",
        "        inpt = char_sequence.narrow(0, 0, char_sequence.size()[0] - 1)\n",
        "        #print(inpt)\n",
        "        #print(inpt.size())\n",
        "        target = char_sequence.narrow(0, 1, char_sequence.size()[0] - 1)\n",
        "        #print(target)\n",
        "        #print(target.size())\n",
        "        scores, dec_hidden = self.forward(inpt, dec_hidden)\n",
        "        #target = self.decoderCharEmb(target)\n",
        "        #print(target.size())\n",
        "        #print(scores.size())\n",
        "        target = target.contiguous()\n",
        "        target = target.view(target.size()[0] * target.size()[1])\n",
        "        scores = scores.view(scores.size()[0] * scores.size()[1], scores.size()[2])\n",
        "        #print(target.size())\n",
        "        #print(scores.size())\n",
        "        loss = nn.CrossEntropyLoss(reduction='sum', ignore_index=self.target_vocab.char2id['<pad>'])\n",
        "        return loss(scores, target)\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def decode_greedy(self, initialStates, device, max_length=21):\n",
        "        \"\"\" Greedy decoding\n",
        "        @param initialStates: initial internal state of the LSTM, a tuple of two tensors of size (1, batch, hidden_size)\n",
        "        @param device: torch.device (indicates whether the model is on CPU or GPU)\n",
        "        @param max_length: maximum length of words to decode\n",
        "\n",
        "        @returns decodedWords: a list (of length batch) of strings, each of which has length <= max_length.\n",
        "                              The decoded strings should NOT contain the start-of-word and end-of-word characters.\n",
        "        \"\"\"\n",
        "\n",
        "        ### YOUR CODE HERE for part 2d\n",
        "        ### TODO - Implement greedy decoding.\n",
        "        ### Hints:\n",
        "        ###      - Use target_vocab.char2id and target_vocab.id2char to convert between integers and characters\n",
        "        ###      - Use torch.tensor(..., device=device) to turn a list of character indices into a tensor.\n",
        "        ###      - We use curly brackets as start-of-word and end-of-word characters. That is, use the character '{' for <START> and '}' for <END>.\n",
        "        ###        Their indices are self.target_vocab.start_of_word and self.target_vocab.end_of_word, respectively.\n",
        "        output_chars = []\n",
        "        #print(initialStates[0].size()[1])\n",
        "        for i in range(initialStates[0].size()[1]):\n",
        "            output_chars.append([])\n",
        "        #print(output_chars)\n",
        "        #output_chars = [[] * initialStates[0].size()[1]]\n",
        "        current_char = [[self.target_vocab.start_of_word] * initialStates[0].size()[1]]\n",
        "        dec_hidden = initialStates\n",
        "        for i in range(max_length):\n",
        "            current_char = torch.tensor(current_char, device=device)\n",
        "            scores, dec_hidden = self.forward(current_char, dec_hidden=dec_hidden)\n",
        "            #embedding = self.decoderCharEmb(current_char)\n",
        "            #print(embedding.size())\n",
        "            #dec_hidden = self.charDecoder(embedding, dec_hidden)\n",
        "            #print('hi')\n",
        "            #scores = self.char_output_projection(dec_hidden[0])\n",
        "            softmax = nn.Softmax(dim=2)\n",
        "            p = softmax(scores)\n",
        "            current_char = torch.argmax(p, dim=2)\n",
        "            #print(current_char.size())\n",
        "            chars = current_char.tolist()[0]\n",
        "            #print(output_chars)\n",
        "            #print(len(chars))\n",
        "            for i in range(len(chars)):\n",
        "                output_chars[i].append(chars[i])\n",
        "        #print(output_chars)\n",
        "        '''\n",
        "        output_words = []\n",
        "        idx = 0\n",
        "        i = 0\n",
        "        word = \"\"\n",
        "        while i < len(output_chars):\n",
        "            if idx < 21:\n",
        "                print(self.target_vocab.id2char[self.target_vocab.end_of_word])\n",
        "                if output_chars[i] != self.target_vocab.id2char[self.target_vocab.end_of_word]:\n",
        "                    word += output_chars[i]\n",
        "                    idx += 1\n",
        "                else:\n",
        "                    i += 21 - idx\n",
        "                    idx = 0\n",
        "                    output_words.append(word)\n",
        "                    word = output_chars[i]\n",
        "            else:\n",
        "                idx = 0\n",
        "                output_words.append(word)\n",
        "                word = output_chars[i]\n",
        "            i += 1\n",
        "        output_words.append(word)\n",
        "        print(output_words)\n",
        "        '''\n",
        "        #print(len(output_chars))\n",
        "        '''\n",
        "        output_chars_list = []\n",
        "        idx = 0\n",
        "        chars_list = []\n",
        "        for i in range(len(output_chars)):\n",
        "            if idx < 21:\n",
        "                idx += 1\n",
        "                chars_list.append(output_chars[i])\n",
        "            else:\n",
        "                output_chars_list.append(chars_list)\n",
        "                idx = 0\n",
        "                chars_list = [output_chars[i]]\n",
        "        output_chars_list.append(chars_list)\n",
        "        '''\n",
        "\n",
        "        output_words = []\n",
        "        for chars_list in output_chars:\n",
        "            word = \"\"\n",
        "            for c in chars_list:\n",
        "                if c != self.target_vocab.end_of_word and c!= self.target_vocab.char2id['<pad>']:\n",
        "                    word += self.target_vocab.id2char[c]\n",
        "                else:\n",
        "                    break\n",
        "            output_words.append(word)\n",
        "\n",
        "        return output_words\n",
        "        \n",
        "        ### END YOUR CODE"
      ],
      "metadata": {
        "id": "72ktjor8Tpc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nmt_model.py"
      ],
      "metadata": {
        "id": "8nzGJfkwSyry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 5\n",
        "nmt_model.py: NMT Model\n",
        "Pencheng Yin <pcyin@cs.cmu.edu>\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\"\"\"\n",
        "from collections import namedtuple\n",
        "import sys\n",
        "from typing import List, Tuple, Dict, Set, Union\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "import random\n",
        "\n",
        "# from model_embeddings import ModelEmbeddings\n",
        "# from char_decoder import CharDecoder\n",
        "\n",
        "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n",
        "\n",
        "import random\n",
        "\n",
        "class NMT(nn.Module):\n",
        "    \"\"\" Simple Neural Machine Translation Model:\n",
        "        - Bidrectional LSTM Encoder\n",
        "        - Unidirection LSTM Decoder\n",
        "        - Global Attention Model (Luong, et al. 2015)\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2, no_char_decoder=False):\n",
        "        \"\"\" Init NMT Model.\n",
        "\n",
        "        @param embed_size (int): Embedding size (dimensionality)\n",
        "        @param hidden_size (int): Hidden Size (dimensionality)\n",
        "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
        "                              See vocab.py for documentation.\n",
        "        @param dropout_rate (float): Dropout probability, for attention\n",
        "        \"\"\"\n",
        "        super(NMT, self).__init__()\n",
        "\n",
        "        self.model_embeddings_source = ModelEmbeddings(embed_size, vocab.src)\n",
        "        self.model_embeddings_target = ModelEmbeddings(embed_size, vocab.tgt)\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.encoder = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n",
        "        self.decoder = nn.LSTMCell(embed_size + hidden_size, hidden_size)\n",
        "\n",
        "        self.h_projection = nn.Linear(hidden_size * 2, hidden_size, bias=False)\n",
        "        self.c_projection = nn.Linear(hidden_size * 2, hidden_size, bias=False)\n",
        "        self.att_projection = nn.Linear(hidden_size * 2, hidden_size, bias=False)    \n",
        "        self.combined_output_projection = nn.Linear(hidden_size * 2 + hidden_size, hidden_size, bias=False)        \n",
        "        self.target_vocab_projection = nn.Linear(hidden_size, len(vocab.tgt), bias=False)\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "        if not no_char_decoder:\n",
        "           self.charDecoder = CharDecoder(hidden_size, target_vocab=vocab.tgt) \n",
        "        else:\n",
        "           self.charDecoder = None\n",
        "\n",
        "    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n",
        "        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n",
        "        target sentences under the language models learned by the NMT system.\n",
        "\n",
        "        @param source (List[List[str]]): list of source sentence tokens\n",
        "        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n",
        "\n",
        "        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n",
        "                                    log-likelihood of generating the gold-standard target sentence for\n",
        "                                    each example in the input batch. Here b = batch size.\n",
        "        \"\"\"\n",
        "        # Compute sentence lengths\n",
        "        source_lengths = [len(s) for s in source]\n",
        "\n",
        "        # Convert list of lists into tensors\n",
        "\n",
        "        ## A4 code\n",
        "        # source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)\n",
        "        # target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)\n",
        " \n",
        "        # enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n",
        "        # enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n",
        "        # combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n",
        "        ## End A4 code\n",
        "        \n",
        "        ### YOUR CODE HERE for part 1k\n",
        "        ### TODO: \n",
        "        ###     Modify the code lines above as needed to fetch the character-level tensor \n",
        "        ###     to feed into encode() and decode(). You should:\n",
        "        ###     - Keep `target_padded` from A4 code above for predictions\n",
        "        ###     - Add `source_padded_chars` for character level padded encodings for source\n",
        "        ###     - Add `target_padded_chars` for character level padded encodings for target\n",
        "        ###     - Modify calls to encode() and decode() to use the character level encodings\n",
        "        source_padded_chars = self.vocab.src.to_input_tensor_char(source, device=self.device)\n",
        "        target_padded_chars = self.vocab.src.to_input_tensor_char(target, device=self.device)\n",
        "        target_padded_chars = target_padded_chars.contiguous()\n",
        "        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)\n",
        " \n",
        "        enc_hiddens, dec_init_state = self.encode(source_padded_chars, source_lengths)\n",
        "        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n",
        "        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded_chars)\n",
        " \n",
        "        ### END YOUR CODE\n",
        "\n",
        "        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n",
        "\n",
        "        # Zero out, probabilities for which we have nothing in the target text\n",
        "        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()\n",
        "\n",
        "        # Compute log probability of generating true target words\n",
        "        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n",
        "        scores = target_gold_words_log_prob.sum() # mhahn2 Small modification from A4 code.\n",
        "\n",
        "\n",
        "\n",
        "        if self.charDecoder is not None:\n",
        "            max_word_len = target_padded_chars.shape[-1]\n",
        "\n",
        "            target_words = target_padded[1:].contiguous().view(-1)\n",
        "            target_chars = target_padded_chars[1:].view(-1, max_word_len)\n",
        "            target_outputs = combined_outputs.view(-1, 256)\n",
        "    \n",
        "            target_chars_oov = target_chars #torch.index_select(target_chars, dim=0, index=oovIndices)\n",
        "            rnn_states_oov = target_outputs #torch.index_select(target_outputs, dim=0, index=oovIndices)\n",
        "            oovs_losses = self.charDecoder.train_forward(target_chars_oov.t(), (rnn_states_oov.unsqueeze(0), rnn_states_oov.unsqueeze(0)))\n",
        "            scores = scores - oovs_losses\n",
        "    \n",
        "        return scores\n",
        "\n",
        "\n",
        "    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n",
        "            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n",
        "        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b, max_word_length), where\n",
        "                                        b = batch_size, src_len = maximum source sentence length. Note that \n",
        "                                       these have already been sorted in order of longest to shortest sentence.\n",
        "        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n",
        "        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n",
        "                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
        "        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n",
        "                                                hidden state and cell.\n",
        "        \"\"\"\n",
        "        enc_hiddens, dec_init_state = None, None\n",
        "\n",
        "        X = self.model_embeddings_source(source_padded)\n",
        "        X_packed = pack_padded_sequence(X, source_lengths)\n",
        "        enc_hiddens, (last_hidden, last_cell) = self.encoder(X_packed)\n",
        "        (enc_hiddens, _) = pad_packed_sequence(enc_hiddens)\n",
        "        enc_hiddens = enc_hiddens.permute(1, 0, 2)\n",
        "\n",
        "        init_decoder_hidden = self.h_projection(torch.cat((last_hidden[0], last_hidden[1]), dim=1))\n",
        "        init_decoder_cell = self.c_projection(torch.cat((last_cell[0], last_cell[1]), dim=1))\n",
        "        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n",
        "\n",
        "        return enc_hiddens, dec_init_state\n",
        "\n",
        "\n",
        "    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,\n",
        "                dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute combined output vectors for a batch.\n",
        "        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n",
        "                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
        "        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n",
        "                                     b = batch size, src_len = maximum source sentence length.\n",
        "        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n",
        "        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b, max_word_length), where\n",
        "                                       tgt_len = maximum target sentence length, b = batch size. \n",
        "        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n",
        "                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n",
        "        \"\"\"\n",
        "        # Chop of the <END> token for max length sentences.\n",
        "        target_padded = target_padded[:-1]\n",
        "\n",
        "        # Initialize the decoder state (hidden and cell)\n",
        "        dec_state = dec_init_state\n",
        "\n",
        "        # Initialize previous combined output vector o_{t-1} as zero\n",
        "        batch_size = enc_hiddens.size(0)\n",
        "        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n",
        "\n",
        "        # Initialize a list we will use to collect the combined output o_t on each step\n",
        "        combined_outputs = []\n",
        "\n",
        "        enc_hiddens_proj = self.att_projection(enc_hiddens)\n",
        "        Y = self.model_embeddings_target(target_padded)\n",
        "\n",
        "        for Y_t in torch.split(Y, split_size_or_sections=1):\n",
        "            Y_t = Y_t.squeeze(0)\n",
        "            Ybar_t = torch.cat([Y_t, o_prev], dim=-1)\n",
        "            dec_state, o_t, _ = self.step(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks)\n",
        "            combined_outputs.append(o_t)\n",
        "            o_prev = o_t\n",
        "\n",
        "        combined_outputs = torch.stack(combined_outputs)\n",
        "\n",
        "        return combined_outputs\n",
        "\n",
        "\n",
        "    def step(self, Ybar_t: torch.Tensor,\n",
        "            dec_state: Tuple[torch.Tensor, torch.Tensor],\n",
        "            enc_hiddens: torch.Tensor,\n",
        "            enc_hiddens_proj: torch.Tensor,\n",
        "            enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n",
        "        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n",
        "                                where b = batch size, e = embedding size, h = hidden size.\n",
        "        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n",
        "                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n",
        "        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n",
        "                                    src_len = maximum source length, h = hidden size.\n",
        "        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n",
        "                                    where b = batch size, src_len = maximum source length, h = hidden size.\n",
        "        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n",
        "                                    where b = batch size, src_len is maximum source length. \n",
        "        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n",
        "                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n",
        "        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n",
        "        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n",
        "                                Note: You will not use this outside of this function.\n",
        "                                      We are simply returning this value so that we can sanity check\n",
        "                                      your implementation.\n",
        "        \"\"\"\n",
        "\n",
        "        combined_output = None\n",
        "\n",
        "        dec_state = self.decoder(Ybar_t, dec_state)\n",
        "        (dec_hidden, dec_cell) = dec_state\n",
        "        e_t = torch.bmm(enc_hiddens_proj, dec_hidden.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "\n",
        "        # Set e_t to -inf where enc_masks has 1\n",
        "        if enc_masks is not None:\n",
        "            e_t.data.masked_fill_(enc_masks.byte(), -float('inf'))\n",
        "\n",
        "        alpha_t = F.softmax(e_t, dim=-1)\n",
        "        alpha_t_view = (alpha_t.size(0), 1, alpha_t.size(1))\n",
        "        a_t = torch.bmm(alpha_t.view(*alpha_t_view), enc_hiddens).squeeze(1)\n",
        "        U_t = torch.cat([dec_hidden, a_t], 1)\n",
        "        V_t = self.combined_output_projection(U_t)\n",
        "        O_t = self.dropout(torch.tanh(V_t))\n",
        "\n",
        "        combined_output = O_t\n",
        "        return dec_state, combined_output, e_t\n",
        "\n",
        "    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n",
        "        \"\"\" Generate sentence masks for encoder hidden states.\n",
        "\n",
        "        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n",
        "                                     src_len = max source length, h = hidden size. \n",
        "        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n",
        "        \n",
        "        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n",
        "                                    where src_len = max source length, h = hidden size.\n",
        "        \"\"\"\n",
        "        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n",
        "        for e_id, src_len in enumerate(source_lengths):\n",
        "            enc_masks[e_id, src_len:] = 1\n",
        "        return enc_masks.to(self.device)\n",
        "\n",
        "\n",
        "    def beam_search(self, src_sent: List[str], beam_size: int=100, max_decoding_time_step: int=70) -> List[Hypothesis]:\n",
        "        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n",
        "        @param src_sent (List[str]): a single source sentence (words)\n",
        "        @param beam_size (int): beam size\n",
        "        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n",
        "        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n",
        "                value: List[str]: the decoded target sentence, represented as a list of words\n",
        "                score: float: the log-likelihood of the target sentence\n",
        "        \"\"\"\n",
        "        ## A4 code\n",
        "        # src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n",
        "        ## End A4 code\n",
        "\n",
        "        src_sents_var = self.vocab.src.to_input_tensor_char([src_sent], self.device)\n",
        "\n",
        "        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n",
        "        src_encodings_att_linear = self.att_projection(src_encodings)\n",
        "\n",
        "        h_tm1 = dec_init_vec\n",
        "        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n",
        "\n",
        "        eos_id = self.vocab.tgt['</s>']\n",
        "\n",
        "        hypotheses = [['<s>']]\n",
        "        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n",
        "        completed_hypotheses = []\n",
        "\n",
        "        t = 0\n",
        "        while len(completed_hypotheses) < beam_size:\n",
        "            t += 1\n",
        "            hyp_num = len(hypotheses)\n",
        "\n",
        "            exp_src_encodings = src_encodings.expand(hyp_num,\n",
        "                                                     src_encodings.size(1),\n",
        "                                                     src_encodings.size(2))\n",
        "\n",
        "            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n",
        "                                                                           src_encodings_att_linear.size(1),\n",
        "                                                                           src_encodings_att_linear.size(2))\n",
        "\t\t\t\n",
        "            ## A4 code\n",
        "            # y_tm1 = self.vocab.tgt.to_input_tensor(list([hyp[-1]] for hyp in hypotheses), device=self.device)\n",
        "            # y_t_embed = self.model_embeddings_target(y_tm1)\n",
        "            ## End A4 code\n",
        "\n",
        "            y_tm1 = self.vocab.tgt.to_input_tensor_char(list([hyp[-1]] for hyp in hypotheses), device=self.device)\n",
        "            y_t_embed = self.model_embeddings_target(y_tm1)\n",
        "            y_t_embed = torch.squeeze(y_t_embed, dim=0)\n",
        "\n",
        "\n",
        "            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n",
        "\n",
        "            (h_t, cell_t), att_t, _  = self.step(x, h_tm1,\n",
        "                                                      exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n",
        "\n",
        "            # log probabilities over target words\n",
        "            log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n",
        "            #p_t = self.target_vocab_projection(att_t)\n",
        "\n",
        "            live_hyp_num = beam_size - len(completed_hypotheses)\n",
        "            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n",
        "            contiuating_hyp_scores_prob = F.softmax(contiuating_hyp_scores)\n",
        "            #contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(p_t) + p_t).view(-1)\n",
        "\n",
        "            #top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n",
        "            top_cand_hyp_scores = []\n",
        "            top_cand_hyp_pos = []\n",
        "\n",
        "            indices = torch.multinomial(contiuating_hyp_scores_prob, live_hyp_num)\n",
        "            for index in indices:\n",
        "                top_cand_hyp_scores.append(contiuating_hyp_scores[index.item()])\n",
        "                top_cand_hyp_pos.append(index)\n",
        "\n",
        "            top_cand_hyp_scores = torch.tensor(top_cand_hyp_scores, device=self.device)\n",
        "            #top_cand_hyp_scores = F.softmax(top_cand_hyp_scores)\n",
        "            top_cand_hyp_pos = torch.tensor(top_cand_hyp_pos, dtype=torch.long, device=self.device)\n",
        "\n",
        "            prev_hyp_ids = top_cand_hyp_pos / len(self.vocab.tgt)\n",
        "            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n",
        "\n",
        "            new_hypotheses = []\n",
        "            live_hyp_ids = []\n",
        "            new_hyp_scores = []\n",
        "\n",
        "            decoderStatesForUNKsHere = []\n",
        "            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n",
        "                prev_hyp_id = prev_hyp_id.item()\n",
        "                hyp_word_id = hyp_word_id.item()\n",
        "                cand_new_hyp_score = cand_new_hyp_score.item()\n",
        "\n",
        "                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n",
        "\n",
        "                # Record output layer in case UNK was generated\n",
        "                if hyp_word == \"<unk>\":\n",
        "                    hyp_word = \"<unk>\"+str(len(decoderStatesForUNKsHere))\n",
        "                    decoderStatesForUNKsHere.append(att_t[int(prev_hyp_id)])\n",
        "\n",
        "                new_hyp_sent = hypotheses[int(prev_hyp_id)] + [hyp_word]\n",
        "                if \".\" in hyp_word or \"?\" in hyp_word or \"!\" in hyp_word:\n",
        "                    new_hyp_sent += ['</s>']\n",
        "                    hyp_word = '</s>'\n",
        "                '''\n",
        "                s1 = \"\"\n",
        "                s2 = \"\"\n",
        "                new_hyp_sent_copy = [x for x in new_hyp_sent if x != \"<s>\" and x != \"</s>\"]\n",
        "                if hyp_word == '</s>' and s1.join(new_hyp_sent_copy) != \"\":\n",
        "                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n",
        "                                                           score=cand_new_hyp_score))\n",
        "                else:\n",
        "                    new_hypotheses.append(new_hyp_sent)\n",
        "                    live_hyp_ids.append(prev_hyp_id)\n",
        "                    new_hyp_scores.append(cand_new_hyp_score)\n",
        "                '''\n",
        "                #s1 = \"\"\n",
        "                #new_hyp_sent_copy = [x for x in new_hyp_sent if x != \"<s>\" and x != \"</s>\"]\n",
        "                #print(s1.join(new_hyp_sent_copy))\n",
        "                if hyp_word == '</s>':\n",
        "                    # print(new_hyp_sent[1:-1])\n",
        "                    #print(len(completed_hypotheses))\n",
        "                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n",
        "                                                           score=cand_new_hyp_score))\n",
        "                else:\n",
        "                    new_hypotheses.append(new_hyp_sent)\n",
        "                    live_hyp_ids.append(prev_hyp_id)\n",
        "                    new_hyp_scores.append(cand_new_hyp_score)\n",
        "\n",
        "            if len(decoderStatesForUNKsHere) > 0 and self.charDecoder is not None: # decode UNKs\n",
        "                decoderStatesForUNKsHere = torch.stack(decoderStatesForUNKsHere, dim=0)\n",
        "                decodedWords = self.charDecoder.decode_greedy((decoderStatesForUNKsHere.unsqueeze(0), decoderStatesForUNKsHere.unsqueeze(0)), max_length=21, device=self.device)\n",
        "                assert len(decodedWords) == decoderStatesForUNKsHere.size()[0], \"Incorrect number of decoded words\" \n",
        "                #new_hypotheses = decodedWords[int([-1][5:])]\n",
        "                for hyp in new_hypotheses:\n",
        "                  if hyp[-1].startswith(\"<unk>\"):\n",
        "                        hyp[-1] = decodedWords[int(hyp[-1][5:])]#[:-1]\n",
        "\n",
        "            if len(completed_hypotheses) == beam_size:\n",
        "                break\n",
        "\n",
        "            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n",
        "            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n",
        "            att_tm1 = att_t[live_hyp_ids]\n",
        "\n",
        "            hypotheses = new_hypotheses\n",
        "            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n",
        "\n",
        "        if len(completed_hypotheses) == 0:\n",
        "            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n",
        "                                                   score=hyp_scores[0].item()))\n",
        "\n",
        "        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n",
        "        # print(len(completed_hypotheses))\n",
        "        return completed_hypotheses\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n",
        "        \"\"\"\n",
        "        return self.att_projection.weight.device\n",
        "\n",
        "    @staticmethod\n",
        "    def load(model_path: str, no_char_decoder=False):\n",
        "        \"\"\" Load the model from a file.\n",
        "        @param model_path (str): path to model\n",
        "        \"\"\"\n",
        "        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "        args = params['args']\n",
        "        model = NMT(vocab=params['vocab'], no_char_decoder=no_char_decoder, **args)\n",
        "        model.load_state_dict(params['state_dict'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\" Save the odel to a file.\n",
        "        @param path (str): path to the model\n",
        "        \"\"\"\n",
        "        print('save model parameters to [%s]' % path, file=sys.stderr)\n",
        "\n",
        "        params = {\n",
        "            'args': dict(embed_size=self.model_embeddings_source.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate),\n",
        "            'vocab': self.vocab,\n",
        "            'state_dict': self.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(params, path)"
      ],
      "metadata": {
        "id": "D_7wgoqaSzT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nlg_model.py"
      ],
      "metadata": {
        "id": "CvGL3wwkfue2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from collections import namedtuple\n",
        "import sys\n",
        "from typing import List, Tuple, Dict, Set, Union\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "\n",
        "# from nmt_model import NMT\n",
        "import os\n",
        "\n",
        "class NLG(nn.Module):\n",
        "    # Natural Language Generation model using a Neural Machine Translation context \n",
        "    # https://arxiv.org/abs/1702.07826\n",
        "\n",
        "    def __init__(self, speakers, embed_size, hidden_size, dropout_rate, vocab, no_char_decoder, lr, clip_grad, lr_decay):\n",
        "        # Need: NMT Model for each speaker (ex: translate from \"person speaking to Michael\" to \"Michael\")\n",
        "        # Model for determining who speaks after who\n",
        "        super(NLG, self).__init__()\n",
        "        self.NMT_speakers = []\n",
        "        self.NMT_models = []\n",
        "        self.NMT_optimizers = []\n",
        "        self.clip_grad = clip_grad\n",
        "        self.lrs = []\n",
        "        self.lr_decay = lr_decay\n",
        "        # find a way to not have to hard-code speakers?\n",
        "        for speaker in speakers:\n",
        "            model = NMT(embed_size=embed_size,\n",
        "                hidden_size=hidden_size,\n",
        "                dropout_rate=dropout_rate,\n",
        "                vocab=vocab, no_char_decoder=no_char_decoder)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "            self.NMT_speakers.append(speaker.replace(\"/\", \"-\").replace(\" \",\"-\"))\n",
        "            self.NMT_models.append(model)\n",
        "            self.NMT_optimizers.append(optimizer)\n",
        "            self.lrs.append(lr)\n",
        "\n",
        "    # change for double training?\n",
        "    # def forward(self, speaker: str, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n",
        "    def forward(self, speaker: str, source: List[str], target: List[str]):\n",
        "        if speaker in self.NMT_speakers:\n",
        "            i = self.NMT_speakers.index(speaker)\n",
        "            model = self.NMT_models[i]\n",
        "            optimizer = self.NMT_optimizers[i]\n",
        "            lr = self.lrs[i]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_size = 1\n",
        "\n",
        "            example_losses = -model([source], [target]) # (batch_size,)\n",
        "            batch_loss = example_losses.sum()\n",
        "            loss = batch_loss / batch_size\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # clip gradient\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip_grad)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            lr = optimizer.param_groups[0]['lr'] * self.lr_decay\n",
        "            self.lrs[i] = lr\n",
        "\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "            return batch_loss\n",
        "        return 0\n",
        "\n",
        "    def beam_search(self, speaker, src_sent, beam_size=50, max_decoding_time_step=70):\n",
        "        i = self.NMT_speakers.index(speaker)\n",
        "        model = self.NMT_models[i]\n",
        "\n",
        "        was_training = model.training\n",
        "        model.eval()\n",
        "\n",
        "        example_hyps = []\n",
        "        with torch.no_grad():\n",
        "            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n",
        "\n",
        "        if was_training: model.train(was_training)\n",
        "\n",
        "        return example_hyps\n",
        "\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def load(directory: str, no_char_decoder=False):\n",
        "        \"\"\" Load the model from a file.\n",
        "        @param model_path (str): path to model\n",
        "        \"\"\"\n",
        "        args = params['args']\n",
        "        model = NMT(vocab=params['vocab'], no_char_decoder=no_char_decoder, **args)\n",
        "        model.load_state_dict(params['state_dict'])\n",
        "        for filename in os.listdir(directory):\n",
        "            NMT_model = NMT.load(directory + \"/\" + filename, no_char_decoder=no_char_decoder)\n",
        "        return model\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\" Save the odel to a file.\n",
        "        @param path (str): path to the model\n",
        "        \"\"\"\n",
        "        print('save model parameters to [%s]' % path, file=sys.stderr)\n",
        "\n",
        "        params = {\n",
        "            'args': dict(embed_size=self.embed_size,\n",
        "                hidden_size=self.hidden_size,\n",
        "                dropout_rate=self.dropout_rate,\n",
        "                no_char_decoder=self.no_char_decoder),\n",
        "            'vocab': self.vocab,\n",
        "            'state_dict': self.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(params, path)\n",
        "\n",
        "        for i in range(len(self.NMT_speakers)):\n",
        "            self.NMT_models[i].save(\"models/\" + self.NMT_speakers[i] + \"_\" + path)\n",
        "    '''"
      ],
      "metadata": {
        "id": "zs2CnxmvfzJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run.py (train)"
      ],
      "metadata": {
        "id": "w2eo9GXuSjca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 5\n",
        "run.py: Run Script for Simple NMT Model\n",
        "Pencheng Yin <pcyin@cs.cmu.edu>\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\n",
        "Usage:\n",
        "    run.py train --train-src=<file> --train-tgt=<file> --dev-src=<file> --dev-tgt=<file> --vocab=<file> [options]\n",
        "    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE OUTPUT_FILE\n",
        "    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE TEST_TARGET_FILE OUTPUT_FILE\n",
        "\n",
        "Options:\n",
        "    -h --help                               show this screen.\n",
        "    --cuda                                  use GPU\n",
        "    --train-src=<file>                      train source file\n",
        "    --train-tgt=<file>                      train target file\n",
        "    --dev-src=<file>                        dev source file\n",
        "    --dev-tgt=<file>                        dev target file\n",
        "    --vocab=<file>                          vocab file\n",
        "    --seed=<int>                            seed [default: 0]\n",
        "    --batch-size=<int>                      batch size [default: 32]\n",
        "    --embed-size=<int>                      embedding size [default: 256]\n",
        "    --hidden-size=<int>                     hidden size [default: 256]\n",
        "    --clip-grad=<float>                     gradient clipping [default: 5.0]\n",
        "    --log-every=<int>                       log every [default: 10]\n",
        "    --max-epoch=<int>                       max epoch [default: 30]\n",
        "    --input-feed                            use input feeding\n",
        "    --patience=<int>                        wait for how many iterations to decay learning rate [default: 5]\n",
        "    --max-num-trial=<int>                   terminate training after how many trials [default: 5]\n",
        "    --lr-decay=<float>                      learning rate decay [default: 0.5]\n",
        "    --beam-size=<int>                       beam size [default: 50]\n",
        "    --sample-size=<int>                     sample size [default: 5]\n",
        "    --lr=<float>                            learning rate [default: 0.001]\n",
        "    --uniform-init=<float>                  uniformly initialize all parameters [default: 0.1]\n",
        "    --save-to=<file>                        model save path [default: model.bin]\n",
        "    --valid-niter=<int>                     perform validation after how many iterations [default: 2000]\n",
        "    --dropout=<float>                       dropout [default: 0.3]\n",
        "    --max-decoding-time-step=<int>          maximum number of decoding time steps [default: 70]\n",
        "    --no-char-decoder                       do not use the character decoder\n",
        "\"\"\"\n",
        "import math\n",
        "import sys\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "\n",
        "from docopt import docopt\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
        "# from nmt_model import Hypothesis, NMT\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict, Set, Union\n",
        "from tqdm import tqdm\n",
        "# from utils import read_corpus, batch_iter\n",
        "# from vocab import Vocab, VocabEntry\n",
        "\n",
        "import torch\n",
        "import torch.nn.utils\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "def evaluate_ppl(model, dev_data, batch_size=32):\n",
        "    \"\"\" Evaluate perplexity on dev sentences\n",
        "    @param model (NMT): NMT Model\n",
        "    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
        "    @param batch_size (batch size)\n",
        "    @returns ppl (perplixty on dev sentences)\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    cum_loss = 0.\n",
        "    cum_tgt_words = 0.\n",
        "\n",
        "    # no_grad() signals backend to throw away all gradients\n",
        "    with torch.no_grad():\n",
        "        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n",
        "            loss = -model(src_sents, tgt_sents).sum()\n",
        "\n",
        "            cum_loss += loss.item()\n",
        "            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
        "            cum_tgt_words += tgt_word_num_to_predict\n",
        "\n",
        "        ppl = np.exp(cum_loss / cum_tgt_words)\n",
        "\n",
        "    if was_training:\n",
        "        model.train()\n",
        "\n",
        "    return ppl\n",
        "\n",
        "\n",
        "def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n",
        "    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n",
        "    @param references (List[List[str]]): a list of gold-standard reference target sentences\n",
        "    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n",
        "    @returns bleu_score: corpus-level BLEU score\n",
        "    \"\"\"\n",
        "    if references[0][0] == '<s>':\n",
        "        references = [ref[1:-1] for ref in references]\n",
        "    bleu_score = corpus_bleu([[ref] for ref in references],\n",
        "                             [hyp.value for hyp in hypotheses])\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "def train(args: Dict):\n",
        "    \"\"\" Train the NMT Model.\n",
        "    @param args (Dict): args from cmd line\n",
        "    \"\"\"\n",
        "    train_data_src = read_corpus(args['--train-src'], source='src')\n",
        "    train_data_tgt = read_corpus(args['--train-tgt'], source='tgt')\n",
        "\n",
        "    dev_data_src = read_corpus(args['--dev-src'], source='src')\n",
        "    dev_data_tgt = read_corpus(args['--dev-tgt'], source='tgt')\n",
        "\n",
        "    train_data = list(zip(train_data_src, train_data_tgt))\n",
        "    dev_data = list(zip(dev_data_src, dev_data_tgt))\n",
        "\n",
        "    train_batch_size = int(args['--batch-size'])\n",
        "\n",
        "    clip_grad = float(args['--clip-grad'])\n",
        "    valid_niter = int(args['--valid-niter'])\n",
        "    log_every = int(args['--log-every'])\n",
        "    model_save_path = args['--save-to']\n",
        "\n",
        "    vocab = Vocab.load(args['--vocab'])\n",
        "\n",
        "    model = NMT(embed_size=int(args['--embed-size']),\n",
        "                hidden_size=int(args['--hidden-size']),\n",
        "                dropout_rate=float(args['--dropout']),\n",
        "                vocab=vocab, no_char_decoder=args['--no-char-decoder'])\n",
        "    model.train()\n",
        "\n",
        "    uniform_init = float(args['--uniform-init'])\n",
        "    if np.abs(uniform_init) > 0.:\n",
        "        print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n",
        "        for p in model.parameters():\n",
        "            p.data.uniform_(-uniform_init, uniform_init)\n",
        "\n",
        "    vocab_mask = torch.ones(len(vocab.tgt))\n",
        "    vocab_mask[vocab.tgt['<pad>']] = 0\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if args['--cuda'] else \"cpu\")\n",
        "    print('use device: %s' % device, file=sys.stderr)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=float(args['--lr']))\n",
        "\n",
        "    num_trial = 0\n",
        "    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n",
        "    cum_examples = report_examples = epoch = valid_num = 0\n",
        "    hist_valid_scores = []\n",
        "    train_time = begin_time = time.time()\n",
        "    print('begin Maximum Likelihood training')\n",
        "\n",
        "    epoch = 0\n",
        "    while epoch < int(args['--max-epoch']):\n",
        "        epoch += 1\n",
        "\n",
        "        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n",
        "            train_iter += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_size = len(src_sents)\n",
        "\n",
        "            example_losses = -model(src_sents, tgt_sents) # (batch_size,)\n",
        "            batch_loss = example_losses.sum()\n",
        "            loss = batch_loss / batch_size\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # clip gradient\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_losses_val = batch_loss.item()\n",
        "            report_loss += batch_losses_val\n",
        "            cum_loss += batch_losses_val\n",
        "\n",
        "            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
        "            report_tgt_words += tgt_words_num_to_predict\n",
        "            cum_tgt_words += tgt_words_num_to_predict\n",
        "            report_examples += batch_size\n",
        "            cum_examples += batch_size\n",
        "\n",
        "            if train_iter % log_every == 0:\n",
        "                print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n",
        "                      'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n",
        "                                                                                         report_loss / report_examples,\n",
        "                                                                                         math.exp(report_loss / report_tgt_words),\n",
        "                                                                                         cum_examples,\n",
        "                                                                                         report_tgt_words / (time.time() - train_time),\n",
        "                                                                                         time.time() - begin_time), file=sys.stderr)\n",
        "\n",
        "                train_time = time.time()\n",
        "                report_loss = report_tgt_words = report_examples = 0.\n",
        "\n",
        "            # perform validation\n",
        "            if train_iter % valid_niter == 0:\n",
        "                print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n",
        "                                                                                         cum_loss / cum_examples,\n",
        "                                                                                         np.exp(cum_loss / cum_tgt_words),\n",
        "                                                                                         cum_examples), file=sys.stderr)\n",
        "\n",
        "                cum_loss = cum_examples = cum_tgt_words = 0.\n",
        "                valid_num += 1\n",
        "\n",
        "                print('begin validation ...', file=sys.stderr)\n",
        "\n",
        "                # compute dev. ppl and bleu\n",
        "                dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n",
        "                valid_metric = -dev_ppl\n",
        "\n",
        "                print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl), file=sys.stderr)\n",
        "\n",
        "                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n",
        "                hist_valid_scores.append(valid_metric)\n",
        "\n",
        "                if is_better:\n",
        "                    patience = 0\n",
        "                    print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n",
        "                    model.save(model_save_path)\n",
        "\n",
        "                    # also save the optimizers' state\n",
        "                    torch.save(optimizer.state_dict(), model_save_path + '.optim')\n",
        "                elif patience < int(args['--patience']):\n",
        "                    patience += 1\n",
        "                    print('hit patience %d' % patience, file=sys.stderr)\n",
        "\n",
        "                    if patience == int(args['--patience']):\n",
        "                        num_trial += 1\n",
        "                        print('hit #%d trial' % num_trial, file=sys.stderr)\n",
        "                        if num_trial == int(args['--max-num-trial']):\n",
        "                            print('early stop!', file=sys.stderr)\n",
        "                            exit(0)\n",
        "\n",
        "                        # decay lr, and restore from previously best checkpoint\n",
        "                        lr = optimizer.param_groups[0]['lr'] * float(args['--lr-decay'])\n",
        "                        print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n",
        "\n",
        "                        # load model\n",
        "                        params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n",
        "                        model.load_state_dict(params['state_dict'])\n",
        "                        model = model.to(device)\n",
        "\n",
        "                        print('restore parameters of the optimizers', file=sys.stderr)\n",
        "                        optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n",
        "\n",
        "                        # set new lr\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "\n",
        "                        # reset patience\n",
        "                        patience = 0\n",
        "\n",
        "            # if epoch == int(args['--max-epoch']):\n",
        "            #     print('reached maximum number of epochs!', file=sys.stderr)\n",
        "            #     # exit(0)\n",
        "            #     break\n",
        "\n",
        "\n",
        "def decode(args: Dict[str, str]):\n",
        "    \"\"\" Performs decoding on a test set, and save the best-scoring decoding results.\n",
        "    If the target gold-standard sentences are given, the function also computes\n",
        "    corpus-level BLEU score.\n",
        "    @param args (Dict): args from cmd line\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"load test source sentences from [{}]\".format(args['TEST_SOURCE_FILE']), file=sys.stderr)\n",
        "    test_data_src = read_corpus(args['TEST_SOURCE_FILE'], source='src')\n",
        "    if args['TEST_TARGET_FILE']:\n",
        "        print(\"load test target sentences from [{}]\".format(args['TEST_TARGET_FILE']), file=sys.stderr)\n",
        "        test_data_tgt = read_corpus(args['TEST_TARGET_FILE'], source='tgt')\n",
        "\n",
        "    print(\"load model from {}\".format(args['MODEL_PATH']), file=sys.stderr)\n",
        "    model = NMT.load(args['MODEL_PATH'], no_char_decoder=args['--no-char-decoder'])\n",
        "\n",
        "    if args['--cuda']:\n",
        "        model = model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "    hypotheses = beam_search(model, test_data_src,\n",
        "                             beam_size=int(args['--beam-size']),\n",
        "                             max_decoding_time_step=int(args['--max-decoding-time-step']))\n",
        "\n",
        "    if args['TEST_TARGET_FILE']:\n",
        "        top_hypotheses = [random.choice(hyps) for hyps in hypotheses]\n",
        "        bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n",
        "        print('Corpus BLEU: {}'.format(bleu_score * 100), file=sys.stderr)\n",
        "\n",
        "    with open(args['OUTPUT_FILE'], 'w') as f:\n",
        "        for src_sent, hyps in zip(test_data_src, hypotheses):\n",
        "            top_hyp = hyps[0]\n",
        "            hyp_sent = ' '.join(top_hyp.value)\n",
        "            f.write(hyp_sent + '\\n')\n",
        "\n",
        "\n",
        "def beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n",
        "    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n",
        "    @param model (NMT): NMT Model\n",
        "    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n",
        "    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n",
        "    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n",
        "    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    hypotheses = []\n",
        "    with torch.no_grad():\n",
        "        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n",
        "            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n",
        "\n",
        "            hypotheses.append(example_hyps)\n",
        "\n",
        "    if was_training: model.train(was_training)\n",
        "\n",
        "    return hypotheses\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\" Main func.\n",
        "    \"\"\"\n",
        "    # args = docopt(__doc__)\n",
        "    args = {}\n",
        "    args['--seed'] = 0\n",
        "    args['--cuda'] = True\n",
        "    args['train'] = True\n",
        "    args['decode'] = False\n",
        "    args['--train-src'] = 'en_es_data/train.es'\n",
        "    args['--train-tgt'] = 'en_es_data/train.en'\n",
        "    args['--dev-src'] = 'en_es_data/dev.es'\n",
        "    args['--dev-tgt'] = 'en_es_data/dev.en'\n",
        "    args['--batch-size'] = 32\n",
        "    args['--clip-grad'] = 5\n",
        "    args['--valid-niter'] = 2000\n",
        "    args['--log-every'] = 10\n",
        "    args['--save-to'] = 'model.bin'\n",
        "    args['--vocab'] = 'vocab.json'\n",
        "    args['--embed-size'] = 256\n",
        "    args['--hidden-size'] = 256\n",
        "    args['--dropout'] = 0.3\n",
        "    args['--no-char-decoder'] = False\n",
        "    args['--uniform-init'] = 0.1\n",
        "    args['--lr'] = 0.001\n",
        "    args['--patience'] = 5\n",
        "    args['--max-num-trial'] = 5\n",
        "    args['--lr-decay'] = 0.5\n",
        "    args['--max-epoch'] = 30\n",
        "    args['TEST_SOURCE_FILE'] = 'en_es_data/test.es'\n",
        "    args['TEST_TARGET_FILE'] = 'en_es_data/test.en'\n",
        "    args['MODEL_PATH'] = 'model.bin'\n",
        "    args['--beam-size'] = 50\n",
        "    args['--max-decoding-time-step'] = 70\n",
        "    args['OUTPUT_FILE'] = 'outputs/test_outputs.txt'\n",
        "\n",
        "\n",
        "    # Check pytorch version\n",
        "    # assert(torch.__version__ == \"1.0.0\"), \"Please update your installation of PyTorch. You have {} and you should have version 1.0.0\".format(torch.__version__)\n",
        "\n",
        "    # seed the random number generators\n",
        "    seed = int(args['--seed'])\n",
        "    torch.manual_seed(seed)\n",
        "    if args['--cuda']:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed * 13 // 7)\n",
        "\n",
        "    if args['train']:\n",
        "        train(args)\n",
        "    elif args['decode']:\n",
        "        decode(args)\n",
        "    else:\n",
        "        raise RuntimeError('invalid run mode')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYfdCEAYWmWl",
        "outputId": "c9a7465a-ea27-417b-e4e7-c2b74d7f48e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "uniformly initialize parameters [-0.100000, +0.100000]\n",
            "use device: cuda:0\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:238: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "begin Maximum Likelihood training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1, iter 10, avg. loss 398.06, avg. ppl 130996977918.85 cum. examples 320, speed 1650.63 words/sec, time elapsed 3.01 sec\n",
            "epoch 1, iter 20, avg. loss 296.39, avg. ppl 491692464.78 cum. examples 640, speed 1494.25 words/sec, time elapsed 6.19 sec\n",
            "epoch 1, iter 30, avg. loss 278.17, avg. ppl 248193937.95 cum. examples 960, speed 1606.24 words/sec, time elapsed 9.05 sec\n",
            "epoch 1, iter 40, avg. loss 316.37, avg. ppl 132122493.92 cum. examples 1280, speed 1753.87 words/sec, time elapsed 12.14 sec\n",
            "epoch 1, iter 50, avg. loss 285.99, avg. ppl 111510735.34 cum. examples 1600, speed 1648.00 words/sec, time elapsed 15.14 sec\n",
            "epoch 1, iter 60, avg. loss 310.07, avg. ppl 86292903.67 cum. examples 1920, speed 1522.91 words/sec, time elapsed 18.70 sec\n",
            "epoch 1, iter 70, avg. loss 314.70, avg. ppl 81301355.95 cum. examples 2240, speed 1892.83 words/sec, time elapsed 21.62 sec\n",
            "epoch 1, iter 80, avg. loss 280.87, avg. ppl 57568637.13 cum. examples 2560, speed 1710.43 words/sec, time elapsed 24.57 sec\n",
            "epoch 1, iter 90, avg. loss 283.44, avg. ppl 38751767.07 cum. examples 2880, speed 1501.57 words/sec, time elapsed 28.02 sec\n",
            "epoch 1, iter 100, avg. loss 257.84, avg. ppl 21417814.16 cum. examples 3200, speed 1783.33 words/sec, time elapsed 30.76 sec\n",
            "epoch 1, iter 110, avg. loss 241.64, avg. ppl 11826512.92 cum. examples 3520, speed 1679.49 words/sec, time elapsed 33.59 sec\n",
            "epoch 1, iter 120, avg. loss 257.23, avg. ppl 6126317.21 cum. examples 3840, speed 1804.69 words/sec, time elapsed 36.51 sec\n",
            "epoch 1, iter 130, avg. loss 265.49, avg. ppl 4901016.54 cum. examples 4160, speed 1891.32 words/sec, time elapsed 39.43 sec\n",
            "epoch 1, iter 140, avg. loss 254.92, avg. ppl 3979537.26 cum. examples 4480, speed 1696.69 words/sec, time elapsed 42.59 sec\n",
            "epoch 1, iter 150, avg. loss 224.88, avg. ppl 2735793.90 cum. examples 4800, speed 1827.67 words/sec, time elapsed 45.25 sec\n",
            "epoch 1, iter 160, avg. loss 257.13, avg. ppl 3317585.40 cum. examples 5120, speed 1870.64 words/sec, time elapsed 48.18 sec\n",
            "epoch 1, iter 170, avg. loss 223.48, avg. ppl 1959112.44 cum. examples 5440, speed 1465.78 words/sec, time elapsed 51.54 sec\n",
            "epoch 1, iter 180, avg. loss 240.66, avg. ppl 2296203.80 cum. examples 5760, speed 1618.31 words/sec, time elapsed 54.79 sec\n",
            "epoch 1, iter 190, avg. loss 226.55, avg. ppl 1930037.66 cum. examples 6080, speed 1742.56 words/sec, time elapsed 57.67 sec\n",
            "epoch 1, iter 200, avg. loss 237.03, avg. ppl 1659848.75 cum. examples 6400, speed 1697.40 words/sec, time elapsed 60.79 sec\n",
            "epoch 1, iter 210, avg. loss 220.19, avg. ppl 1303918.11 cum. examples 6720, speed 1825.07 words/sec, time elapsed 63.53 sec\n",
            "epoch 1, iter 220, avg. loss 218.04, avg. ppl 1357610.79 cum. examples 7040, speed 1439.37 words/sec, time elapsed 66.96 sec\n",
            "epoch 1, iter 230, avg. loss 227.73, avg. ppl 1270084.61 cum. examples 7360, speed 1751.53 words/sec, time elapsed 69.92 sec\n",
            "epoch 1, iter 240, avg. loss 210.92, avg. ppl 1241551.77 cum. examples 7680, speed 1663.32 words/sec, time elapsed 72.82 sec\n",
            "epoch 1, iter 250, avg. loss 223.42, avg. ppl 773257.53 cum. examples 8000, speed 1552.75 words/sec, time elapsed 76.21 sec\n",
            "epoch 1, iter 260, avg. loss 232.92, avg. ppl 921977.19 cum. examples 8320, speed 1902.60 words/sec, time elapsed 79.06 sec\n",
            "epoch 1, iter 270, avg. loss 194.12, avg. ppl 671184.26 cum. examples 8640, speed 1761.58 words/sec, time elapsed 81.69 sec\n",
            "epoch 2, iter 280, avg. loss 240.73, avg. ppl 687056.39 cum. examples 8945, speed 1820.77 words/sec, time elapsed 84.69 sec\n",
            "epoch 2, iter 290, avg. loss 224.04, avg. ppl 589012.49 cum. examples 9265, speed 1778.81 words/sec, time elapsed 87.73 sec\n",
            "epoch 2, iter 300, avg. loss 206.78, avg. ppl 613822.56 cum. examples 9585, speed 1808.12 words/sec, time elapsed 90.47 sec\n",
            "epoch 2, iter 310, avg. loss 201.05, avg. ppl 531709.20 cum. examples 9905, speed 1599.39 words/sec, time elapsed 93.52 sec\n",
            "epoch 2, iter 320, avg. loss 219.13, avg. ppl 676965.07 cum. examples 10225, speed 1904.20 words/sec, time elapsed 96.27 sec\n",
            "epoch 2, iter 330, avg. loss 210.90, avg. ppl 484995.60 cum. examples 10545, speed 1648.15 words/sec, time elapsed 99.39 sec\n",
            "epoch 2, iter 340, avg. loss 210.73, avg. ppl 417906.53 cum. examples 10865, speed 1802.02 words/sec, time elapsed 102.29 sec\n",
            "epoch 2, iter 350, avg. loss 220.68, avg. ppl 372510.07 cum. examples 11185, speed 1740.90 words/sec, time elapsed 105.45 sec\n",
            "epoch 2, iter 360, avg. loss 204.57, avg. ppl 359726.81 cum. examples 11505, speed 1729.01 words/sec, time elapsed 108.41 sec\n",
            "epoch 2, iter 370, avg. loss 193.93, avg. ppl 359488.94 cum. examples 11825, speed 1588.99 words/sec, time elapsed 111.46 sec\n",
            "epoch 2, iter 380, avg. loss 195.92, avg. ppl 317447.52 cum. examples 12145, speed 1634.39 words/sec, time elapsed 114.49 sec\n",
            "epoch 2, iter 390, avg. loss 186.70, avg. ppl 277341.20 cum. examples 12465, speed 1625.38 words/sec, time elapsed 117.42 sec\n",
            "epoch 2, iter 400, avg. loss 211.45, avg. ppl 293207.39 cum. examples 12785, speed 1968.13 words/sec, time elapsed 120.16 sec\n",
            "epoch 2, iter 410, avg. loss 190.84, avg. ppl 275712.06 cum. examples 13105, speed 1693.16 words/sec, time elapsed 123.03 sec\n",
            "epoch 2, iter 420, avg. loss 192.26, avg. ppl 220773.03 cum. examples 13425, speed 1887.56 words/sec, time elapsed 125.68 sec\n",
            "epoch 2, iter 430, avg. loss 194.47, avg. ppl 213084.67 cum. examples 13745, speed 1870.09 words/sec, time elapsed 128.40 sec\n",
            "epoch 2, iter 440, avg. loss 196.12, avg. ppl 245346.81 cum. examples 14065, speed 1937.18 words/sec, time elapsed 131.01 sec\n",
            "epoch 2, iter 450, avg. loss 187.96, avg. ppl 181551.24 cum. examples 14385, speed 1506.17 words/sec, time elapsed 134.30 sec\n",
            "epoch 2, iter 460, avg. loss 184.82, avg. ppl 198154.16 cum. examples 14705, speed 1627.63 words/sec, time elapsed 137.28 sec\n",
            "epoch 2, iter 470, avg. loss 192.96, avg. ppl 207235.32 cum. examples 15025, speed 1554.94 words/sec, time elapsed 140.53 sec\n",
            "epoch 2, iter 480, avg. loss 190.63, avg. ppl 201310.74 cum. examples 15345, speed 1826.59 words/sec, time elapsed 143.26 sec\n",
            "epoch 2, iter 490, avg. loss 201.08, avg. ppl 164270.82 cum. examples 15665, speed 1435.58 words/sec, time elapsed 146.99 sec\n",
            "epoch 2, iter 500, avg. loss 199.86, avg. ppl 203111.47 cum. examples 15985, speed 1840.89 words/sec, time elapsed 149.84 sec\n",
            "epoch 2, iter 510, avg. loss 200.76, avg. ppl 171631.35 cum. examples 16305, speed 1626.19 words/sec, time elapsed 153.12 sec\n",
            "epoch 2, iter 520, avg. loss 189.16, avg. ppl 180578.51 cum. examples 16625, speed 1735.46 words/sec, time elapsed 156.00 sec\n",
            "epoch 2, iter 530, avg. loss 183.78, avg. ppl 153478.90 cum. examples 16945, speed 1496.16 words/sec, time elapsed 159.29 sec\n",
            "epoch 2, iter 540, avg. loss 197.82, avg. ppl 158083.53 cum. examples 17265, speed 1664.27 words/sec, time elapsed 162.47 sec\n",
            "epoch 3, iter 550, avg. loss 172.26, avg. ppl 124952.20 cum. examples 17570, speed 1754.97 words/sec, time elapsed 165.02 sec\n",
            "epoch 3, iter 560, avg. loss 198.93, avg. ppl 130578.59 cum. examples 17890, speed 1851.76 words/sec, time elapsed 167.94 sec\n",
            "epoch 3, iter 570, avg. loss 200.14, avg. ppl 118035.59 cum. examples 18210, speed 1643.91 words/sec, time elapsed 171.27 sec\n",
            "epoch 3, iter 580, avg. loss 187.36, avg. ppl 101044.50 cum. examples 18530, speed 1613.34 words/sec, time elapsed 174.50 sec\n",
            "epoch 3, iter 590, avg. loss 169.30, avg. ppl 110816.30 cum. examples 18850, speed 1757.64 words/sec, time elapsed 177.15 sec\n",
            "epoch 3, iter 600, avg. loss 173.99, avg. ppl 110645.02 cum. examples 19170, speed 1652.25 words/sec, time elapsed 180.05 sec\n",
            "epoch 3, iter 610, avg. loss 196.55, avg. ppl 114161.06 cum. examples 19490, speed 1668.99 words/sec, time elapsed 183.29 sec\n",
            "epoch 3, iter 620, avg. loss 199.63, avg. ppl 101614.25 cum. examples 19810, speed 1819.61 words/sec, time elapsed 186.33 sec\n",
            "epoch 3, iter 630, avg. loss 182.09, avg. ppl 103987.43 cum. examples 20130, speed 1693.88 words/sec, time elapsed 189.31 sec\n",
            "epoch 3, iter 640, avg. loss 191.10, avg. ppl 95870.24 cum. examples 20450, speed 1775.16 words/sec, time elapsed 192.32 sec\n",
            "epoch 3, iter 650, avg. loss 178.47, avg. ppl 116120.92 cum. examples 20770, speed 1531.07 words/sec, time elapsed 195.51 sec\n",
            "epoch 3, iter 660, avg. loss 181.93, avg. ppl 98165.43 cum. examples 21090, speed 1759.85 words/sec, time elapsed 198.39 sec\n",
            "epoch 3, iter 670, avg. loss 187.94, avg. ppl 72926.72 cum. examples 21410, speed 1626.58 words/sec, time elapsed 201.69 sec\n",
            "epoch 3, iter 680, avg. loss 176.86, avg. ppl 100879.79 cum. examples 21730, speed 1864.16 words/sec, time elapsed 204.33 sec\n",
            "epoch 3, iter 690, avg. loss 175.57, avg. ppl 75848.35 cum. examples 22050, speed 1766.72 words/sec, time elapsed 207.16 sec\n",
            "epoch 3, iter 700, avg. loss 175.48, avg. ppl 84872.11 cum. examples 22370, speed 1434.18 words/sec, time elapsed 210.61 sec\n",
            "epoch 3, iter 710, avg. loss 181.80, avg. ppl 89779.50 cum. examples 22690, speed 1669.25 words/sec, time elapsed 213.67 sec\n",
            "epoch 3, iter 720, avg. loss 188.26, avg. ppl 71751.85 cum. examples 23010, speed 1765.18 words/sec, time elapsed 216.72 sec\n",
            "epoch 3, iter 730, avg. loss 177.84, avg. ppl 61084.20 cum. examples 23330, speed 1517.90 words/sec, time elapsed 220.12 sec\n",
            "epoch 3, iter 740, avg. loss 173.05, avg. ppl 60938.14 cum. examples 23650, speed 1657.01 words/sec, time elapsed 223.15 sec\n",
            "epoch 3, iter 750, avg. loss 164.38, avg. ppl 62114.21 cum. examples 23970, speed 1626.14 words/sec, time elapsed 226.09 sec\n",
            "epoch 3, iter 760, avg. loss 193.16, avg. ppl 92376.71 cum. examples 24290, speed 1743.17 words/sec, time elapsed 229.19 sec\n",
            "epoch 3, iter 770, avg. loss 180.67, avg. ppl 72194.23 cum. examples 24610, speed 1618.84 words/sec, time elapsed 232.38 sec\n",
            "epoch 3, iter 780, avg. loss 185.16, avg. ppl 57300.24 cum. examples 24930, speed 1645.90 words/sec, time elapsed 235.66 sec\n",
            "epoch 3, iter 790, avg. loss 165.31, avg. ppl 68789.34 cum. examples 25250, speed 1815.23 words/sec, time elapsed 238.28 sec\n",
            "epoch 3, iter 800, avg. loss 157.91, avg. ppl 62020.19 cum. examples 25570, speed 1638.33 words/sec, time elapsed 241.08 sec\n",
            "epoch 3, iter 810, avg. loss 172.88, avg. ppl 63840.50 cum. examples 25890, speed 1678.81 words/sec, time elapsed 244.05 sec\n",
            "epoch 4, iter 820, avg. loss 162.13, avg. ppl 54397.31 cum. examples 26195, speed 1548.78 words/sec, time elapsed 246.98 sec\n",
            "epoch 4, iter 830, avg. loss 165.50, avg. ppl 44993.14 cum. examples 26515, speed 1670.15 words/sec, time elapsed 249.94 sec\n",
            "epoch 4, iter 840, avg. loss 171.06, avg. ppl 60048.07 cum. examples 26835, speed 1681.09 words/sec, time elapsed 252.90 sec\n",
            "epoch 4, iter 850, avg. loss 194.38, avg. ppl 63456.20 cum. examples 27155, speed 1770.67 words/sec, time elapsed 256.08 sec\n",
            "epoch 4, iter 860, avg. loss 151.51, avg. ppl 45438.77 cum. examples 27475, speed 1676.07 words/sec, time elapsed 258.78 sec\n",
            "epoch 4, iter 870, avg. loss 175.39, avg. ppl 58500.96 cum. examples 27795, speed 1678.53 words/sec, time elapsed 261.82 sec\n",
            "epoch 4, iter 880, avg. loss 160.45, avg. ppl 42462.80 cum. examples 28115, speed 1997.99 words/sec, time elapsed 264.23 sec\n",
            "epoch 4, iter 890, avg. loss 165.63, avg. ppl 51632.39 cum. examples 28435, speed 1566.93 words/sec, time elapsed 267.35 sec\n",
            "epoch 4, iter 900, avg. loss 194.29, avg. ppl 61526.94 cum. examples 28755, speed 1773.48 words/sec, time elapsed 270.53 sec\n",
            "epoch 4, iter 910, avg. loss 178.81, avg. ppl 52368.01 cum. examples 29075, speed 1919.01 words/sec, time elapsed 273.28 sec\n",
            "epoch 4, iter 920, avg. loss 177.88, avg. ppl 44505.44 cum. examples 29395, speed 1800.40 words/sec, time elapsed 276.23 sec\n",
            "epoch 4, iter 930, avg. loss 166.63, avg. ppl 44388.10 cum. examples 29715, speed 1717.76 words/sec, time elapsed 279.13 sec\n",
            "epoch 4, iter 940, avg. loss 162.63, avg. ppl 45623.59 cum. examples 30035, speed 1588.63 words/sec, time elapsed 282.18 sec\n",
            "epoch 4, iter 950, avg. loss 160.90, avg. ppl 39054.64 cum. examples 30355, speed 1818.91 words/sec, time elapsed 284.86 sec\n",
            "epoch 4, iter 960, avg. loss 155.32, avg. ppl 42366.78 cum. examples 30675, speed 1750.11 words/sec, time elapsed 287.53 sec\n",
            "epoch 4, iter 970, avg. loss 156.91, avg. ppl 39515.08 cum. examples 30995, speed 1638.20 words/sec, time elapsed 290.42 sec\n",
            "epoch 4, iter 980, avg. loss 162.52, avg. ppl 43442.36 cum. examples 31315, speed 1903.68 words/sec, time elapsed 292.98 sec\n",
            "epoch 4, iter 990, avg. loss 185.37, avg. ppl 44762.21 cum. examples 31635, speed 1505.89 words/sec, time elapsed 296.66 sec\n",
            "epoch 4, iter 1000, avg. loss 169.19, avg. ppl 42248.33 cum. examples 31955, speed 1739.31 words/sec, time elapsed 299.58 sec\n",
            "epoch 4, iter 1010, avg. loss 177.00, avg. ppl 46511.48 cum. examples 32275, speed 1601.11 words/sec, time elapsed 302.87 sec\n",
            "epoch 4, iter 1020, avg. loss 171.03, avg. ppl 38451.40 cum. examples 32595, speed 1621.69 words/sec, time elapsed 306.07 sec\n",
            "epoch 4, iter 1030, avg. loss 175.61, avg. ppl 35607.37 cum. examples 32915, speed 1399.72 words/sec, time elapsed 309.90 sec\n",
            "epoch 4, iter 1040, avg. loss 176.42, avg. ppl 39894.08 cum. examples 33235, speed 1693.02 words/sec, time elapsed 313.05 sec\n",
            "epoch 4, iter 1050, avg. loss 170.20, avg. ppl 30414.67 cum. examples 33555, speed 1696.88 words/sec, time elapsed 316.16 sec\n",
            "epoch 4, iter 1060, avg. loss 170.58, avg. ppl 35343.80 cum. examples 33875, speed 1908.29 words/sec, time elapsed 318.89 sec\n",
            "epoch 4, iter 1070, avg. loss 164.74, avg. ppl 35457.76 cum. examples 34195, speed 1747.99 words/sec, time elapsed 321.77 sec\n",
            "epoch 4, iter 1080, avg. loss 163.80, avg. ppl 30284.02 cum. examples 34515, speed 1752.53 words/sec, time elapsed 324.67 sec\n",
            "epoch 5, iter 1090, avg. loss 186.88, avg. ppl 35860.49 cum. examples 34820, speed 1699.89 words/sec, time elapsed 327.87 sec\n",
            "epoch 5, iter 1100, avg. loss 172.32, avg. ppl 35301.05 cum. examples 35140, speed 1658.84 words/sec, time elapsed 331.04 sec\n",
            "epoch 5, iter 1110, avg. loss 157.65, avg. ppl 30046.60 cum. examples 35460, speed 1732.16 words/sec, time elapsed 333.86 sec\n",
            "epoch 5, iter 1120, avg. loss 147.56, avg. ppl 31486.53 cum. examples 35780, speed 1708.59 words/sec, time elapsed 336.53 sec\n",
            "epoch 5, iter 1130, avg. loss 155.96, avg. ppl 31045.65 cum. examples 36100, speed 2020.38 words/sec, time elapsed 338.92 sec\n",
            "epoch 5, iter 1140, avg. loss 163.67, avg. ppl 33548.54 cum. examples 36420, speed 1649.62 words/sec, time elapsed 341.97 sec\n",
            "epoch 5, iter 1150, avg. loss 174.29, avg. ppl 29773.21 cum. examples 36740, speed 1795.47 words/sec, time elapsed 344.98 sec\n",
            "epoch 5, iter 1160, avg. loss 163.34, avg. ppl 30820.95 cum. examples 37060, speed 1619.81 words/sec, time elapsed 348.11 sec\n",
            "epoch 5, iter 1170, avg. loss 166.79, avg. ppl 24364.96 cum. examples 37380, speed 1724.12 words/sec, time elapsed 351.17 sec\n",
            "epoch 5, iter 1180, avg. loss 163.33, avg. ppl 33382.55 cum. examples 37700, speed 1760.27 words/sec, time elapsed 354.02 sec\n",
            "epoch 5, iter 1190, avg. loss 183.50, avg. ppl 34485.34 cum. examples 38020, speed 1785.91 words/sec, time elapsed 357.17 sec\n",
            "epoch 5, iter 1200, avg. loss 155.63, avg. ppl 25675.65 cum. examples 38340, speed 1870.67 words/sec, time elapsed 359.79 sec\n",
            "epoch 5, iter 1210, avg. loss 176.83, avg. ppl 26632.65 cum. examples 38660, speed 1581.02 words/sec, time elapsed 363.30 sec\n",
            "epoch 5, iter 1220, avg. loss 152.73, avg. ppl 27453.91 cum. examples 38980, speed 1723.27 words/sec, time elapsed 366.08 sec\n",
            "epoch 5, iter 1230, avg. loss 162.16, avg. ppl 28549.47 cum. examples 39300, speed 1744.59 words/sec, time elapsed 368.98 sec\n",
            "epoch 5, iter 1240, avg. loss 177.44, avg. ppl 29220.62 cum. examples 39620, speed 1871.61 words/sec, time elapsed 371.93 sec\n",
            "epoch 5, iter 1250, avg. loss 176.06, avg. ppl 31465.09 cum. examples 39940, speed 1915.63 words/sec, time elapsed 374.77 sec\n",
            "epoch 5, iter 1260, avg. loss 155.08, avg. ppl 25861.96 cum. examples 40260, speed 1464.55 words/sec, time elapsed 378.10 sec\n",
            "epoch 5, iter 1270, avg. loss 154.89, avg. ppl 23861.41 cum. examples 40580, speed 1727.01 words/sec, time elapsed 380.95 sec\n",
            "epoch 5, iter 1280, avg. loss 166.64, avg. ppl 28818.90 cum. examples 40900, speed 1815.91 words/sec, time elapsed 383.81 sec\n",
            "epoch 5, iter 1290, avg. loss 157.19, avg. ppl 24262.54 cum. examples 41220, speed 1589.40 words/sec, time elapsed 386.95 sec\n",
            "epoch 5, iter 1300, avg. loss 155.44, avg. ppl 22033.89 cum. examples 41540, speed 1813.00 words/sec, time elapsed 389.69 sec\n",
            "epoch 5, iter 1310, avg. loss 139.15, avg. ppl 21097.66 cum. examples 41860, speed 1578.09 words/sec, time elapsed 392.52 sec\n",
            "epoch 5, iter 1320, avg. loss 153.92, avg. ppl 19048.82 cum. examples 42180, speed 1671.58 words/sec, time elapsed 395.51 sec\n",
            "epoch 5, iter 1330, avg. loss 174.73, avg. ppl 24706.55 cum. examples 42500, speed 1784.19 words/sec, time elapsed 398.61 sec\n",
            "epoch 5, iter 1340, avg. loss 171.73, avg. ppl 27652.56 cum. examples 42820, speed 1543.52 words/sec, time elapsed 402.09 sec\n",
            "epoch 5, iter 1350, avg. loss 177.36, avg. ppl 28174.93 cum. examples 43140, speed 1779.24 words/sec, time elapsed 405.21 sec\n",
            "epoch 5, iter 1360, avg. loss 143.21, avg. ppl 26853.75 cum. examples 43445, speed 1710.87 words/sec, time elapsed 407.71 sec\n",
            "epoch 6, iter 1370, avg. loss 163.11, avg. ppl 20785.29 cum. examples 43765, speed 1624.03 words/sec, time elapsed 410.94 sec\n",
            "epoch 6, iter 1380, avg. loss 186.22, avg. ppl 21412.91 cum. examples 44085, speed 1673.90 words/sec, time elapsed 414.51 sec\n",
            "epoch 6, iter 1390, avg. loss 162.19, avg. ppl 19362.69 cum. examples 44405, speed 1904.20 words/sec, time elapsed 417.27 sec\n",
            "epoch 6, iter 1400, avg. loss 147.28, avg. ppl 22112.75 cum. examples 44725, speed 1610.74 words/sec, time elapsed 420.20 sec\n",
            "epoch 6, iter 1410, avg. loss 157.68, avg. ppl 19924.98 cum. examples 45045, speed 1766.28 words/sec, time elapsed 423.08 sec\n",
            "epoch 6, iter 1420, avg. loss 147.44, avg. ppl 20089.99 cum. examples 45365, speed 1724.04 words/sec, time elapsed 425.85 sec\n",
            "epoch 6, iter 1430, avg. loss 152.99, avg. ppl 20795.71 cum. examples 45685, speed 1834.12 words/sec, time elapsed 428.53 sec\n",
            "epoch 6, iter 1440, avg. loss 154.53, avg. ppl 20489.39 cum. examples 46005, speed 1636.10 words/sec, time elapsed 431.58 sec\n",
            "epoch 6, iter 1450, avg. loss 150.02, avg. ppl 19735.28 cum. examples 46325, speed 1639.58 words/sec, time elapsed 434.54 sec\n",
            "epoch 6, iter 1460, avg. loss 164.97, avg. ppl 19980.34 cum. examples 46645, speed 1744.03 words/sec, time elapsed 437.59 sec\n",
            "epoch 6, iter 1470, avg. loss 140.44, avg. ppl 20209.15 cum. examples 46965, speed 1928.50 words/sec, time elapsed 439.94 sec\n",
            "epoch 6, iter 1480, avg. loss 153.67, avg. ppl 19697.96 cum. examples 47285, speed 1580.94 words/sec, time elapsed 443.09 sec\n",
            "epoch 6, iter 1490, avg. loss 148.08, avg. ppl 17615.15 cum. examples 47605, speed 1811.35 words/sec, time elapsed 445.77 sec\n",
            "epoch 6, iter 1500, avg. loss 157.95, avg. ppl 21320.44 cum. examples 47925, speed 1883.07 words/sec, time elapsed 448.46 sec\n",
            "epoch 6, iter 1510, avg. loss 153.47, avg. ppl 18993.56 cum. examples 48245, speed 1456.33 words/sec, time elapsed 451.88 sec\n",
            "epoch 6, iter 1520, avg. loss 155.12, avg. ppl 20860.59 cum. examples 48565, speed 1565.83 words/sec, time elapsed 455.07 sec\n",
            "epoch 6, iter 1530, avg. loss 151.04, avg. ppl 19500.24 cum. examples 48885, speed 1735.12 words/sec, time elapsed 457.89 sec\n",
            "epoch 6, iter 1540, avg. loss 163.22, avg. ppl 19116.23 cum. examples 49205, speed 1878.61 words/sec, time elapsed 460.71 sec\n",
            "epoch 6, iter 1550, avg. loss 168.68, avg. ppl 17903.84 cum. examples 49525, speed 1729.15 words/sec, time elapsed 463.90 sec\n",
            "epoch 6, iter 1560, avg. loss 162.99, avg. ppl 23104.17 cum. examples 49845, speed 1799.48 words/sec, time elapsed 466.78 sec\n",
            "epoch 6, iter 1570, avg. loss 151.96, avg. ppl 20332.51 cum. examples 50165, speed 1877.19 words/sec, time elapsed 469.39 sec\n",
            "epoch 6, iter 1580, avg. loss 154.43, avg. ppl 20036.55 cum. examples 50485, speed 1726.20 words/sec, time elapsed 472.29 sec\n",
            "epoch 6, iter 1590, avg. loss 158.55, avg. ppl 22186.30 cum. examples 50805, speed 1656.59 words/sec, time elapsed 475.35 sec\n",
            "epoch 6, iter 1600, avg. loss 177.08, avg. ppl 18444.32 cum. examples 51125, speed 1699.07 words/sec, time elapsed 478.74 sec\n",
            "epoch 6, iter 1610, avg. loss 163.80, avg. ppl 21074.54 cum. examples 51445, speed 1885.68 words/sec, time elapsed 481.53 sec\n",
            "epoch 6, iter 1620, avg. loss 161.85, avg. ppl 17760.20 cum. examples 51765, speed 1646.58 words/sec, time elapsed 484.75 sec\n",
            "epoch 6, iter 1630, avg. loss 157.26, avg. ppl 19061.80 cum. examples 52085, speed 1734.99 words/sec, time elapsed 487.69 sec\n",
            "epoch 7, iter 1640, avg. loss 169.93, avg. ppl 14969.81 cum. examples 52390, speed 1992.43 words/sec, time elapsed 490.40 sec\n",
            "epoch 7, iter 1650, avg. loss 159.59, avg. ppl 17209.98 cum. examples 52710, speed 1765.21 words/sec, time elapsed 493.37 sec\n",
            "epoch 7, iter 1660, avg. loss 142.93, avg. ppl 13802.06 cum. examples 53030, speed 1847.04 words/sec, time elapsed 495.96 sec\n",
            "epoch 7, iter 1670, avg. loss 140.46, avg. ppl 14407.25 cum. examples 53350, speed 1517.72 words/sec, time elapsed 499.06 sec\n",
            "epoch 7, iter 1680, avg. loss 148.82, avg. ppl 15644.74 cum. examples 53670, speed 1751.83 words/sec, time elapsed 501.87 sec\n",
            "epoch 7, iter 1690, avg. loss 150.78, avg. ppl 16256.91 cum. examples 53990, speed 1671.37 words/sec, time elapsed 504.85 sec\n",
            "epoch 7, iter 1700, avg. loss 145.98, avg. ppl 16443.77 cum. examples 54310, speed 1611.29 words/sec, time elapsed 507.84 sec\n",
            "epoch 7, iter 1710, avg. loss 164.22, avg. ppl 16132.83 cum. examples 54630, speed 1802.62 words/sec, time elapsed 510.84 sec\n",
            "epoch 7, iter 1720, avg. loss 176.87, avg. ppl 22063.83 cum. examples 54950, speed 1671.64 words/sec, time elapsed 514.23 sec\n",
            "epoch 7, iter 1730, avg. loss 161.34, avg. ppl 15087.52 cum. examples 55270, speed 1819.24 words/sec, time elapsed 517.18 sec\n",
            "epoch 7, iter 1740, avg. loss 154.78, avg. ppl 15053.44 cum. examples 55590, speed 1513.43 words/sec, time elapsed 520.58 sec\n",
            "epoch 7, iter 1750, avg. loss 137.12, avg. ppl 13219.42 cum. examples 55910, speed 1534.33 words/sec, time elapsed 523.60 sec\n",
            "epoch 7, iter 1760, avg. loss 142.99, avg. ppl 12462.36 cum. examples 56230, speed 1768.93 words/sec, time elapsed 526.34 sec\n",
            "epoch 7, iter 1770, avg. loss 158.78, avg. ppl 18474.12 cum. examples 56550, speed 1844.51 words/sec, time elapsed 529.14 sec\n",
            "epoch 7, iter 1780, avg. loss 165.48, avg. ppl 16378.93 cum. examples 56870, speed 1848.83 words/sec, time elapsed 532.09 sec\n",
            "epoch 7, iter 1790, avg. loss 153.79, avg. ppl 17234.22 cum. examples 57190, speed 1837.30 words/sec, time elapsed 534.84 sec\n",
            "epoch 7, iter 1800, avg. loss 143.90, avg. ppl 15112.65 cum. examples 57510, speed 1570.60 words/sec, time elapsed 537.89 sec\n",
            "epoch 7, iter 1810, avg. loss 153.48, avg. ppl 14713.24 cum. examples 57830, speed 1513.67 words/sec, time elapsed 541.27 sec\n",
            "epoch 7, iter 1820, avg. loss 163.83, avg. ppl 13866.81 cum. examples 58150, speed 1795.28 words/sec, time elapsed 544.33 sec\n",
            "epoch 7, iter 1830, avg. loss 148.12, avg. ppl 13623.16 cum. examples 58470, speed 1712.90 words/sec, time elapsed 547.24 sec\n",
            "epoch 7, iter 1840, avg. loss 154.10, avg. ppl 14060.87 cum. examples 58790, speed 1857.50 words/sec, time elapsed 550.02 sec\n",
            "epoch 7, iter 1850, avg. loss 151.72, avg. ppl 15143.61 cum. examples 59110, speed 1828.36 words/sec, time elapsed 552.78 sec\n",
            "epoch 7, iter 1860, avg. loss 149.91, avg. ppl 15770.04 cum. examples 59430, speed 1654.71 words/sec, time elapsed 555.78 sec\n",
            "epoch 7, iter 1870, avg. loss 146.30, avg. ppl 13381.65 cum. examples 59750, speed 1740.31 words/sec, time elapsed 558.61 sec\n",
            "epoch 7, iter 1880, avg. loss 163.34, avg. ppl 17276.42 cum. examples 60070, speed 1718.53 words/sec, time elapsed 561.73 sec\n",
            "epoch 7, iter 1890, avg. loss 163.83, avg. ppl 16278.36 cum. examples 60390, speed 1755.94 words/sec, time elapsed 564.80 sec\n",
            "epoch 7, iter 1900, avg. loss 136.77, avg. ppl 13306.06 cum. examples 60710, speed 1645.28 words/sec, time elapsed 567.61 sec\n",
            "epoch 8, iter 1910, avg. loss 142.06, avg. ppl 13465.89 cum. examples 61015, speed 1723.86 words/sec, time elapsed 570.25 sec\n",
            "epoch 8, iter 1920, avg. loss 166.28, avg. ppl 14939.39 cum. examples 61335, speed 1695.84 words/sec, time elapsed 573.51 sec\n",
            "epoch 8, iter 1930, avg. loss 139.21, avg. ppl 13256.12 cum. examples 61655, speed 1619.48 words/sec, time elapsed 576.41 sec\n",
            "epoch 8, iter 1940, avg. loss 155.18, avg. ppl 10911.67 cum. examples 61975, speed 1784.34 words/sec, time elapsed 579.41 sec\n",
            "epoch 8, iter 1950, avg. loss 155.07, avg. ppl 14254.38 cum. examples 62295, speed 1591.35 words/sec, time elapsed 582.67 sec\n",
            "epoch 8, iter 1960, avg. loss 151.89, avg. ppl 13003.65 cum. examples 62615, speed 1533.19 words/sec, time elapsed 586.01 sec\n",
            "epoch 8, iter 1970, avg. loss 151.79, avg. ppl 12382.48 cum. examples 62935, speed 1574.54 words/sec, time elapsed 589.29 sec\n",
            "epoch 8, iter 1980, avg. loss 156.29, avg. ppl 13300.38 cum. examples 63255, speed 1722.84 words/sec, time elapsed 592.34 sec\n",
            "epoch 8, iter 1990, avg. loss 158.35, avg. ppl 13476.22 cum. examples 63575, speed 1686.61 words/sec, time elapsed 595.50 sec\n",
            "epoch 8, iter 2000, avg. loss 144.70, avg. ppl 11141.74 cum. examples 63895, speed 1789.98 words/sec, time elapsed 598.28 sec\n",
            "epoch 8, iter 2000, cum. loss 182.03, cum. ppl 90458.12 cum. examples 63895\n",
            "begin validation ...\n",
            "validation: iter 2000, dev. ppl 16494.927485\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 8, iter 2010, avg. loss 136.48, avg. ppl 13279.37 cum. examples 320, speed 840.65 words/sec, time elapsed 603.75 sec\n",
            "epoch 8, iter 2020, avg. loss 162.86, avg. ppl 12361.78 cum. examples 640, speed 1800.27 words/sec, time elapsed 606.82 sec\n",
            "epoch 8, iter 2030, avg. loss 146.75, avg. ppl 11007.57 cum. examples 960, speed 1636.45 words/sec, time elapsed 609.91 sec\n",
            "epoch 8, iter 2040, avg. loss 148.22, avg. ppl 12949.61 cum. examples 1280, speed 1672.74 words/sec, time elapsed 612.90 sec\n",
            "epoch 8, iter 2050, avg. loss 162.93, avg. ppl 13912.24 cum. examples 1600, speed 1942.14 words/sec, time elapsed 615.72 sec\n",
            "epoch 8, iter 2060, avg. loss 149.08, avg. ppl 13126.86 cum. examples 1920, speed 1801.79 words/sec, time elapsed 618.51 sec\n",
            "epoch 8, iter 2070, avg. loss 144.71, avg. ppl 8605.03 cum. examples 2240, speed 1746.97 words/sec, time elapsed 621.43 sec\n",
            "epoch 8, iter 2080, avg. loss 160.28, avg. ppl 15273.39 cum. examples 2560, speed 1776.53 words/sec, time elapsed 624.43 sec\n",
            "epoch 8, iter 2090, avg. loss 155.01, avg. ppl 12712.09 cum. examples 2880, speed 1776.41 words/sec, time elapsed 627.39 sec\n",
            "epoch 8, iter 2100, avg. loss 132.74, avg. ppl 12133.37 cum. examples 3200, speed 1688.29 words/sec, time elapsed 630.06 sec\n",
            "epoch 8, iter 2110, avg. loss 158.01, avg. ppl 14604.83 cum. examples 3520, speed 1555.09 words/sec, time elapsed 633.45 sec\n",
            "epoch 8, iter 2120, avg. loss 155.92, avg. ppl 13189.55 cum. examples 3840, speed 1780.86 words/sec, time elapsed 636.41 sec\n",
            "epoch 8, iter 2130, avg. loss 151.63, avg. ppl 12513.32 cum. examples 4160, speed 1478.61 words/sec, time elapsed 639.88 sec\n",
            "epoch 8, iter 2140, avg. loss 149.81, avg. ppl 11585.37 cum. examples 4480, speed 1492.00 words/sec, time elapsed 643.32 sec\n",
            "epoch 8, iter 2150, avg. loss 141.33, avg. ppl 10913.16 cum. examples 4800, speed 1668.58 words/sec, time elapsed 646.23 sec\n",
            "epoch 8, iter 2160, avg. loss 149.87, avg. ppl 13519.13 cum. examples 5120, speed 1749.43 words/sec, time elapsed 649.11 sec\n",
            "epoch 8, iter 2170, avg. loss 141.41, avg. ppl 11557.96 cum. examples 5440, speed 1651.11 words/sec, time elapsed 652.04 sec\n",
            "epoch 9, iter 2180, avg. loss 146.51, avg. ppl 10935.68 cum. examples 5745, speed 1734.26 words/sec, time elapsed 654.82 sec\n",
            "epoch 9, iter 2190, avg. loss 157.08, avg. ppl 10533.89 cum. examples 6065, speed 1813.24 words/sec, time elapsed 657.81 sec\n",
            "epoch 9, iter 2200, avg. loss 163.41, avg. ppl 11116.73 cum. examples 6385, speed 1849.47 words/sec, time elapsed 660.84 sec\n",
            "epoch 9, iter 2210, avg. loss 144.48, avg. ppl 11672.99 cum. examples 6705, speed 1656.22 words/sec, time elapsed 663.82 sec\n",
            "epoch 9, iter 2220, avg. loss 142.21, avg. ppl 10960.96 cum. examples 7025, speed 1839.30 words/sec, time elapsed 666.48 sec\n",
            "epoch 9, iter 2230, avg. loss 159.95, avg. ppl 11365.77 cum. examples 7345, speed 1727.13 words/sec, time elapsed 669.66 sec\n",
            "epoch 9, iter 2240, avg. loss 154.67, avg. ppl 11717.01 cum. examples 7665, speed 1763.03 words/sec, time elapsed 672.65 sec\n",
            "epoch 9, iter 2250, avg. loss 159.35, avg. ppl 9365.96 cum. examples 7985, speed 1474.73 words/sec, time elapsed 676.44 sec\n",
            "epoch 9, iter 2260, avg. loss 149.71, avg. ppl 11406.93 cum. examples 8305, speed 1706.15 words/sec, time elapsed 679.44 sec\n",
            "epoch 9, iter 2270, avg. loss 152.01, avg. ppl 10216.37 cum. examples 8625, speed 1739.81 words/sec, time elapsed 682.47 sec\n",
            "epoch 9, iter 2280, avg. loss 143.38, avg. ppl 11304.85 cum. examples 8945, speed 1730.67 words/sec, time elapsed 685.31 sec\n",
            "epoch 9, iter 2290, avg. loss 151.43, avg. ppl 10289.81 cum. examples 9265, speed 1776.35 words/sec, time elapsed 688.26 sec\n",
            "epoch 9, iter 2300, avg. loss 151.46, avg. ppl 14344.70 cum. examples 9585, speed 1697.88 words/sec, time elapsed 691.25 sec\n",
            "epoch 9, iter 2310, avg. loss 137.21, avg. ppl 11802.67 cum. examples 9905, speed 1604.03 words/sec, time elapsed 694.17 sec\n",
            "epoch 9, iter 2320, avg. loss 155.60, avg. ppl 10440.49 cum. examples 10225, speed 2031.09 words/sec, time elapsed 696.82 sec\n",
            "epoch 9, iter 2330, avg. loss 153.10, avg. ppl 11310.64 cum. examples 10545, speed 1492.41 words/sec, time elapsed 700.33 sec\n",
            "epoch 9, iter 2340, avg. loss 144.13, avg. ppl 9922.29 cum. examples 10865, speed 1757.53 words/sec, time elapsed 703.18 sec\n",
            "epoch 9, iter 2350, avg. loss 150.90, avg. ppl 10049.26 cum. examples 11185, speed 1825.90 words/sec, time elapsed 706.05 sec\n",
            "epoch 9, iter 2360, avg. loss 147.90, avg. ppl 10398.91 cum. examples 11505, speed 1763.62 words/sec, time elapsed 708.96 sec\n",
            "epoch 9, iter 2370, avg. loss 142.20, avg. ppl 12965.81 cum. examples 11825, speed 1830.46 words/sec, time elapsed 711.58 sec\n",
            "epoch 9, iter 2380, avg. loss 123.58, avg. ppl 8803.37 cum. examples 12145, speed 1451.23 words/sec, time elapsed 714.58 sec\n",
            "epoch 9, iter 2390, avg. loss 149.14, avg. ppl 9525.29 cum. examples 12465, speed 1847.72 words/sec, time elapsed 717.40 sec\n",
            "epoch 9, iter 2400, avg. loss 144.50, avg. ppl 8725.39 cum. examples 12785, speed 1545.21 words/sec, time elapsed 720.70 sec\n",
            "epoch 9, iter 2410, avg. loss 130.71, avg. ppl 9767.03 cum. examples 13105, speed 1642.04 words/sec, time elapsed 723.47 sec\n",
            "epoch 9, iter 2420, avg. loss 141.05, avg. ppl 11114.48 cum. examples 13425, speed 1720.79 words/sec, time elapsed 726.29 sec\n",
            "epoch 9, iter 2430, avg. loss 146.29, avg. ppl 10479.86 cum. examples 13745, speed 1678.29 words/sec, time elapsed 729.30 sec\n",
            "epoch 9, iter 2440, avg. loss 160.61, avg. ppl 9382.11 cum. examples 14065, speed 1956.27 words/sec, time elapsed 732.17 sec\n",
            "epoch 10, iter 2450, avg. loss 145.70, avg. ppl 11840.99 cum. examples 14370, speed 1692.89 words/sec, time elapsed 734.97 sec\n",
            "epoch 10, iter 2460, avg. loss 169.48, avg. ppl 10259.08 cum. examples 14690, speed 1908.82 words/sec, time elapsed 738.05 sec\n",
            "epoch 10, iter 2470, avg. loss 143.98, avg. ppl 9100.62 cum. examples 15010, speed 1507.06 words/sec, time elapsed 741.40 sec\n",
            "epoch 10, iter 2480, avg. loss 146.09, avg. ppl 9777.91 cum. examples 15330, speed 1601.95 words/sec, time elapsed 744.58 sec\n",
            "epoch 10, iter 2490, avg. loss 143.49, avg. ppl 8029.08 cum. examples 15650, speed 1655.90 words/sec, time elapsed 747.66 sec\n",
            "epoch 10, iter 2500, avg. loss 141.96, avg. ppl 9255.45 cum. examples 15970, speed 1727.22 words/sec, time elapsed 750.54 sec\n",
            "epoch 10, iter 2510, avg. loss 127.60, avg. ppl 8200.06 cum. examples 16290, speed 1674.94 words/sec, time elapsed 753.25 sec\n",
            "epoch 10, iter 2520, avg. loss 131.02, avg. ppl 10163.38 cum. examples 16610, speed 1756.73 words/sec, time elapsed 755.84 sec\n",
            "epoch 10, iter 2530, avg. loss 149.32, avg. ppl 9205.25 cum. examples 16930, speed 1792.08 words/sec, time elapsed 758.76 sec\n",
            "epoch 10, iter 2540, avg. loss 145.69, avg. ppl 9299.15 cum. examples 17250, speed 1658.95 words/sec, time elapsed 761.83 sec\n",
            "epoch 10, iter 2550, avg. loss 143.78, avg. ppl 8938.01 cum. examples 17570, speed 1735.05 words/sec, time elapsed 764.75 sec\n",
            "epoch 10, iter 2560, avg. loss 144.27, avg. ppl 9648.49 cum. examples 17890, speed 1819.97 words/sec, time elapsed 767.51 sec\n",
            "epoch 10, iter 2570, avg. loss 153.84, avg. ppl 9213.60 cum. examples 18210, speed 1927.76 words/sec, time elapsed 770.31 sec\n",
            "epoch 10, iter 2580, avg. loss 126.70, avg. ppl 9396.15 cum. examples 18530, speed 1642.13 words/sec, time elapsed 773.01 sec\n",
            "epoch 10, iter 2590, avg. loss 136.58, avg. ppl 8969.71 cum. examples 18850, speed 1610.09 words/sec, time elapsed 775.99 sec\n",
            "epoch 10, iter 2600, avg. loss 160.82, avg. ppl 10076.01 cum. examples 19170, speed 1710.10 words/sec, time elapsed 779.26 sec\n",
            "epoch 10, iter 2610, avg. loss 157.38, avg. ppl 10648.42 cum. examples 19490, speed 1886.92 words/sec, time elapsed 782.13 sec\n",
            "epoch 10, iter 2620, avg. loss 138.35, avg. ppl 9038.43 cum. examples 19810, speed 1660.72 words/sec, time elapsed 785.06 sec\n",
            "epoch 10, iter 2630, avg. loss 137.41, avg. ppl 7042.96 cum. examples 20130, speed 1650.00 words/sec, time elapsed 788.07 sec\n",
            "epoch 10, iter 2640, avg. loss 133.19, avg. ppl 8052.07 cum. examples 20450, speed 1692.01 words/sec, time elapsed 790.87 sec\n",
            "epoch 10, iter 2650, avg. loss 140.52, avg. ppl 7559.06 cum. examples 20770, speed 1550.07 words/sec, time elapsed 794.12 sec\n",
            "epoch 10, iter 2660, avg. loss 169.84, avg. ppl 10714.78 cum. examples 21090, speed 1602.03 words/sec, time elapsed 797.77 sec\n",
            "epoch 10, iter 2670, avg. loss 149.94, avg. ppl 9377.47 cum. examples 21410, speed 1644.79 words/sec, time elapsed 800.96 sec\n",
            "epoch 10, iter 2680, avg. loss 145.42, avg. ppl 9377.15 cum. examples 21730, speed 1648.81 words/sec, time elapsed 804.05 sec\n",
            "epoch 10, iter 2690, avg. loss 145.06, avg. ppl 9638.59 cum. examples 22050, speed 1747.17 words/sec, time elapsed 806.95 sec\n",
            "epoch 10, iter 2700, avg. loss 141.17, avg. ppl 8151.02 cum. examples 22370, speed 1833.62 words/sec, time elapsed 809.68 sec\n",
            "epoch 10, iter 2710, avg. loss 148.55, avg. ppl 10236.50 cum. examples 22690, speed 1786.80 words/sec, time elapsed 812.56 sec\n",
            "epoch 10, iter 2720, avg. loss 147.30, avg. ppl 8399.26 cum. examples 22995, speed 1556.84 words/sec, time elapsed 815.76 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run.py (test)"
      ],
      "metadata": {
        "id": "aeetitblKpeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\" Main func.\n",
        "    \"\"\"\n",
        "    # args = docopt(__doc__)\n",
        "    args = {}\n",
        "    args['--seed'] = 0\n",
        "    args['--cuda'] = True\n",
        "    args['train'] = False\n",
        "    args['decode'] = True\n",
        "    args['--train-src'] = 'en_es_data/train.es'\n",
        "    args['--train-tgt'] = 'en_es_data/train.en'\n",
        "    args['--dev-src'] = 'en_es_data/dev.es'\n",
        "    args['--dev-tgt'] = 'en_es_data/dev.en'\n",
        "    args['--batch-size'] = 32\n",
        "    args['--clip-grad'] = 5\n",
        "    args['--valid-niter'] = 2000\n",
        "    args['--log-every'] = 10\n",
        "    args['--save-to'] = 'model.bin'\n",
        "    args['--vocab'] = 'vocab.json'\n",
        "    args['--embed-size'] = 256\n",
        "    args['--hidden-size'] = 256\n",
        "    args['--dropout'] = 0.3\n",
        "    args['--no-char-decoder'] = False\n",
        "    args['--uniform-init'] = 0.1\n",
        "    args['--lr'] = 0.001\n",
        "    args['--patience'] = 5\n",
        "    args['--max-num-trial'] = 5\n",
        "    args['--lr-decay'] = 0.5\n",
        "    args['--max-epoch'] = 1\n",
        "    args['TEST_SOURCE_FILE'] = 'en_es_data/test.es'\n",
        "    args['TEST_TARGET_FILE'] = 'en_es_data/test.en'\n",
        "    args['MODEL_PATH'] = 'model.bin'\n",
        "    args['--beam-size'] = 50\n",
        "    args['--max-decoding-time-step'] = 70\n",
        "    args['OUTPUT_FILE'] = 'outputs/test_outputs.txt'\n",
        "\n",
        "    # Check pytorch version\n",
        "    # assert(torch.__version__ == \"1.0.0\"), \"Please update your installation of PyTorch. You have {} and you should have version 1.0.0\".format(torch.__version__)\n",
        "\n",
        "    # seed the random number generators\n",
        "    seed = int(args['--seed'])\n",
        "    torch.manual_seed(seed)\n",
        "    if args['--cuda']:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed * 13 // 7)\n",
        "\n",
        "    if args['train']:\n",
        "        train(args)\n",
        "    elif args['decode']:\n",
        "        decode(args)\n",
        "    else:\n",
        "        raise RuntimeError('invalid run mode')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMApNKaTKsjF",
        "outputId": "fc4d9403-7528-4a2d-f605-64b46610bab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "load test source sentences from [en_es_data/test.es]\n",
            "load test target sentences from [en_es_data/test.en]\n",
            "load model from model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rDecoding:   0%|          | 0/1607 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:327: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoding: 100%|██████████| 1607/1607 [53:37<00:00,  2.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Corpus BLEU: 0.11854739275395311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## michael.py"
      ],
      "metadata": {
        "id": "c-4Qm3wyfKIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import csv\n",
        "# import sys\n",
        "# import json\n",
        "\n",
        "# csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "# episode = 2\n",
        "# line_text = 4\n",
        "# speaker = 5\n",
        "# d = {}\n",
        "\n",
        "# name = \"en_es_data/the_office_scripts.csv\"\n",
        "# train_speaker = open(\"en_es_data/train.es\", \"w\")\n",
        "# test_speaker = open(\"en_es_data/test.es\", \"w\")\n",
        "# dev_speaker = open(\"en_es_data/dev.es\", \"w\")\n",
        "# train_michael = open(\"en_es_data/train.en\", \"w\")\n",
        "# test_michael = open(\"en_es_data/test.en\", \"w\")\n",
        "# dev_michael = open(\"en_es_data/dev.en\", \"w\")\n",
        "\n",
        "# f = open(name)\n",
        "\n",
        "# reader = csv.reader(f)\n",
        "# next(reader)\n",
        "# prevRow = next(reader)\n",
        "\n",
        "# speaker_vocab = {\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2, \"<unk>\": 3}\n",
        "# michael_vocab = {\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2, \"<unk>\": 3}\n",
        "# data = {\"src_word2id\": speaker_vocab, \"tgt_word2id\": michael_vocab}\n",
        "# speaker_count = 1\n",
        "# michael_count = 1\n",
        "\n",
        "# for currRow in reader:\n",
        "#     if currRow[speaker] == \"Michael\" and prevRow[episode] == currRow[episode]:\n",
        "#         if int(currRow[episode]) <= 15:\n",
        "#             train_speaker.write(prevRow[line_text] + \"\\n\")\n",
        "#             train_michael.write(currRow[line_text] + \"\\n\")\n",
        "#         elif int(currRow[episode]) <= 19:\n",
        "#             test_speaker.write(prevRow[line_text] + \"\\n\")\n",
        "#             test_michael.write(currRow[line_text] + \"\\n\")\n",
        "#         else:\n",
        "#             dev_speaker.write(prevRow[line_text] + \"\\n\")\n",
        "#             dev_michael.write(currRow[line_text] + \"\\n\")\n",
        "#     for word in prevRow[line_text].split(\" \"):\n",
        "#         if word not in speaker_vocab.keys():\n",
        "#             speaker_vocab[word] = speaker_count\n",
        "#             speaker_count += 1\n",
        "#     for word in currRow[line_text].split(\" \"):\n",
        "#         if word not in michael_vocab.keys():\n",
        "#             michael_vocab[word] = michael_count\n",
        "#             michael_count += 1\n",
        "#     prevRow = currRow\n",
        "\n",
        "# write_file = open(\"vocab.json\", \"w\")\n",
        "# json.dump(data, write_file)"
      ],
      "metadata": {
        "id": "9jTAk2vEbH2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n84T8uaffQRh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}