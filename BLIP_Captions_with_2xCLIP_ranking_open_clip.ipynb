{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BLIP Captions with 2xCLIP ranking - open_clip.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlordier/colabs/blob/main/BLIP_Captions_with_2xCLIP_ranking_open_clip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using BLIP and CLIP Ranking (and augmentations) for image captioning "
      ],
      "metadata": {
        "id": "LmnvtNAN9fyu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhUzaaCQQbQP"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies { vertical-output: true }\n",
        "import torch\n",
        "!pip install grammar-check #https://pypi.org/project/grammar-check/\n",
        "!pip install clip-anytorch open_clip_torch\n",
        "#!gdown https://drive.google.com/uc?id=1DysE570DY-THu1puJnVpj0GaoeAs45vE\n",
        "#!unzip people1.zip\n",
        "\n",
        "#!gdown https://drive.google.com/uc?id=1Iqe0Sy3PF0UW_YDWu2zcNIpmWqGY0_2B\n",
        "#!unzip image-photo.zip\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !pip3 install transformers==4.15.0 timm==0.4.12 fairscale==0.4.4\n",
        "    !git clone https://github.com/christophschuhmann/BLIP/\n",
        "    %cd BLIP\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U albumentations\n",
        "!pip uninstall -y opencv-python-headless==4.5.5.62\n",
        "!pip install opencv-python-headless==4.5.2.52\n"
      ],
      "metadata": {
        "id": "GcGrNX2U8-VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from albumentations.augmentations.crops.transforms import RandomResizedCrop\n",
        "from torchvision import transforms\n",
        "import albumentations as A\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "from models.blip import blip_decoder\n",
        "import glob\n",
        "\n",
        "#@title Captioning Images of various Types { vertical-output: true }\n",
        "#hide\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "#import clip\n",
        "import open_clip\n",
        "import random\n",
        "\n",
        "rep_pen = 1.8\n",
        "\n",
        "image_size = 384\n",
        "number_of_caption_per_image = 10\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "clip_model='ViT-B-32-quickgelu'\n",
        "model_checkpoint='laion400m_e32'\n",
        "\n",
        "#https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_avg-8a00ab3c.pt\n",
        "model_clip, _, preprocess_clip = open_clip.create_model_and_transforms(clip_model, pretrained=model_checkpoint, device=device)\n",
        "\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'\n",
        "    \n",
        "model = blip_decoder(pretrained=model_url, image_size=image_size, vit='large')\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=(image_size,image_size), scale=(0.8, 1.0), interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    ]) \n",
        "\n",
        "# augment PIL image\n",
        "transform_A = A.Compose([\n",
        "    A.RandomResizedCrop(image_size, image_size, scale=(0.8, 1.0)),\n",
        "    A.Blur(blur_limit=3, p=0.1),\n",
        "    A.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.05, always_apply=False, p=1.0),\n",
        "    A.GaussNoise(p=0.5),\n",
        "    A.CoarseDropout(min_holes = 1, max_holes = 3, max_height = int(image_size/4), max_width= int(image_size/4), p=1.0),\n",
        "    ])\n",
        "\n",
        "def cos_sim_2d(x, y):\n",
        "    norm_x = x / np.linalg.norm(x, axis=1, keepdims=True)\n",
        "    norm_y = y / np.linalg.norm(y, axis=1, keepdims=True)\n",
        "    return np.matmul(norm_x, norm_y.T)\n",
        "\n",
        "def clip_rank(image_pil, text_list): #, clip_model='ViT-B-32-quickgelu', model_checkpoint='laion400m_avg'):\n",
        "    np_im = np.array(image_pil)\n",
        "    transformed = transform_A(image=np_im)\n",
        "    transformed_image = transformed[\"image\"]\n",
        "    image_pil = Image.fromarray(np.uint8(transformed_image)).convert('RGB')\n",
        "\n",
        "    similarities= []\n",
        "    image = preprocess_clip(image_pil).unsqueeze(0).to(device)\n",
        "\n",
        "    image_features = model_clip.encode_image(image).cpu().detach().numpy()\n",
        "\n",
        "    for txt in text_list:\n",
        "      text = open_clip.tokenize(txt ).to(device)\n",
        "      text_features = model_clip.encode_text(text).cpu().detach().numpy()\n",
        "\n",
        "      sim_= float(cos_sim_2d(text_features, image_features)[0]) \n",
        "      similarities.append(sim_)\n",
        "    return similarities\n",
        "\n",
        "\n",
        "files= glob.glob(\"/content/*.jpeg\")\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for f in files[:50]:\n",
        "    \n",
        "    raw_image = Image.open(f).convert('RGB')   \n",
        "    w,h = raw_image.size\n",
        "\n",
        "    display(raw_image.resize((300, int(300* h/w))))\n",
        "    print(\"BEST CAPTION AFTER RANKING WITH CLIP ViT-B-32-laion400m_avg\")\n",
        "\n",
        "    captions = []\n",
        "\n",
        "    for n in range(number_of_caption_per_image):\n",
        "\n",
        "      # Get a random transform\n",
        "      image = transform(raw_image).unsqueeze(0).to(device)     \n",
        "\n",
        "      max_length = random.choice([30, 40, 50])\n",
        "      min_length = random.choice([15, 20, 25])\n",
        "\n",
        "      topP = random.choice([0.2, 0.3, 0.4, 0.5, 0.6, 0.7])\n",
        "\n",
        "      caption = model.generate(image, \n",
        "                               sample=True, \n",
        "                               max_length=max_length, \n",
        "                               min_length=min_length, \n",
        "                               top_p=topP, \n",
        "                               repetition_penalty=rep_pen)      \n",
        "      captions.append(caption[0])\n",
        "\n",
        "      # Get a random transform\n",
        "      image = transform(raw_image).unsqueeze(0).to(device)     \n",
        "\n",
        "      max_length = random.randint(25, 60)\n",
        "      min_length = random.randint(10, max_length)\n",
        "\n",
        "      beam_n = random.choice([2,3,4,5,6,7])\n",
        "      caption = model.generate(image,\n",
        "                               sample=False,\n",
        "                               num_beams=beam_n,\n",
        "                               max_length=max_length,\n",
        "                               min_length=min_length,\n",
        "                               repetition_penalty=rep_pen)\n",
        "      captions.append(caption[0])\n",
        "\n",
        "\n",
        "    best_cannidates=[]\n",
        "    sims = clip_rank(raw_image, captions)\n",
        "\n",
        "    argmax = np.argsort(np.asarray(sims))[:number_of_caption_per_image]\n",
        "\n",
        "    for a in argmax :\n",
        "      print(captions[a])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cfX2WRRAePdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from albumentations.augmentations.crops.transforms import RandomResizedCrop\n",
        "from torchvision import transforms\n",
        "import albumentations as A\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "from models.blip import blip_decoder\n",
        "import glob\n",
        "\n",
        "#@title Captioning Images of various Types { vertical-output: true }\n",
        "#hide\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "#import open_clip\n",
        "import random\n",
        "\n",
        "rep_pen = 1.8\n",
        "\n",
        "image_size = 384\n",
        "number_of_caption_per_image = 10\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "clip_model='ViT-L/14'\n",
        "#model_checkpoint='laion400m_avg'\n",
        "#model_clip, _, preprocess_clip = open_clip.create_model_and_transforms(clip_model, pretrained=model_checkpoint, device=device)\n",
        "\n",
        "model_clip, preprocess_clip = clip.load(clip_model, device=device)\n",
        "\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'\n",
        "    \n",
        "model = blip_decoder(pretrained=model_url, image_size=image_size, vit='large')\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=(image_size,image_size), scale=(0.8, 1.0), interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    ]) \n",
        "\n",
        "# augment PIL image\n",
        "transform_A = A.Compose([\n",
        "    A.RandomResizedCrop(image_size, image_size, scale=(0.8, 1.0)),\n",
        "    A.Blur(blur_limit=3, p=0.1),\n",
        "    A.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.05, always_apply=False, p=1.0),\n",
        "    A.GaussNoise(p=0.5),\n",
        "    A.CoarseDropout(min_holes = 1, max_holes = 3, max_height = int(image_size/4), max_width= int(image_size/4), p=1.0),\n",
        "    ])\n",
        "\n",
        "def cos_sim_2d(x, y):\n",
        "    norm_x = x / np.linalg.norm(x, axis=1, keepdims=True)\n",
        "    norm_y = y / np.linalg.norm(y, axis=1, keepdims=True)\n",
        "    return np.matmul(norm_x, norm_y.T)\n",
        "\n",
        "def clip_rank(image_pil, text_list): #, clip_model='ViT-B-32-quickgelu', model_checkpoint='laion400m_avg'):\n",
        "    np_im = np.array(image_pil)\n",
        "    transformed = transform_A(image=np_im)\n",
        "    transformed_image = transformed[\"image\"]\n",
        "    image_pil = Image.fromarray(np.uint8(transformed_image)).convert('RGB')\n",
        "\n",
        "    similarities= []\n",
        "    image = preprocess_clip(image_pil).unsqueeze(0).to(device)\n",
        "\n",
        "    image_features = model_clip.encode_image(image).cpu().detach().numpy()\n",
        "\n",
        "    for txt in text_list:\n",
        "      text = open_clip.tokenize(txt ).to(device)\n",
        "      text_features = model_clip.encode_text(text).cpu().detach().numpy()\n",
        "\n",
        "      sim_= float(cos_sim_2d(text_features, image_features)[0]) \n",
        "      similarities.append(sim_)\n",
        "    return similarities\n",
        "\n",
        "\n",
        "files= glob.glob(\"/content/*.jpg\")\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for f in files[:50]:\n",
        "    \n",
        "    raw_image = Image.open(f).convert('RGB')   \n",
        "    w,h = raw_image.size\n",
        "\n",
        "    display(raw_image.resize((300, int(300* h/w))))\n",
        "    print(\"BEST CAPTION AFTER RANKING WITH CLIP ViT-L/14\")\n",
        "\n",
        "    captions = []\n",
        "\n",
        "    for n in range(number_of_caption_per_image):\n",
        "\n",
        "      # Get a random transform\n",
        "      image = transform(raw_image).unsqueeze(0).to(device)     \n",
        "\n",
        "      max_length = random.choice([30, 40, 50])\n",
        "      min_length = random.choice([15, 20, 25])\n",
        "\n",
        "      topP = random.choice([0.2, 0.3, 0.4, 0.5, 0.6, 0.7])\n",
        "\n",
        "      caption = model.generate(image, \n",
        "                               sample=True, \n",
        "                               max_length=max_length, \n",
        "                               min_length=min_length, \n",
        "                               top_p=topP, \n",
        "                               repetition_penalty=rep_pen)      \n",
        "      captions.append(caption[0])\n",
        "\n",
        "      # Get a random transform\n",
        "      image = transform(raw_image).unsqueeze(0).to(device)     \n",
        "\n",
        "      max_length = random.randint(25, 60)\n",
        "      min_length = random.randint(10, max_length)\n",
        "\n",
        "      beam_n = random.choice([2,3,4,5,6,7])\n",
        "      caption = model.generate(image,\n",
        "                               sample=False,\n",
        "                               num_beams=beam_n,\n",
        "                               max_length=max_length,\n",
        "                               min_length=min_length,\n",
        "                               repetition_penalty=rep_pen)\n",
        "      captions.append(caption[0])\n",
        "\n",
        "\n",
        "    best_cannidates=[]\n",
        "    sims = clip_rank(raw_image, captions)\n",
        "\n",
        "    argmax = np.argsort(np.asarray(sims))[:number_of_caption_per_image]\n",
        "\n",
        "    for a in argmax :\n",
        "      print(captions[a])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "27Ue0sxCHwdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Captioning Images that show People { vertical-output: true }\n",
        "#hide\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "rep_pen=1.4\n",
        "\n",
        "def cos_sim_2d(x, y):\n",
        "    norm_x = x / np.linalg.norm(x, axis=1, keepdims=True)\n",
        "    norm_y = y / np.linalg.norm(y, axis=1, keepdims=True)\n",
        "    return np.matmul(norm_x, norm_y.T)\n",
        "\n",
        "\n",
        "def clip_rank(image_pil,text_list, clip_model=\"ViT-L/14\"):\n",
        "\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model, preprocess = clip.load(clip_model, device=device)\n",
        "    #model2, preprocess2 = clip.load(\"RN50x64\", device=device)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    similarities= []\n",
        "    image = preprocess(image_pil).unsqueeze(0).to(device)\n",
        "    #image2 = preprocess2(image_pil).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image).cpu().detach().numpy()\n",
        "        #image_features2 = model2.encode_image(image2).cpu().detach().numpy()\n",
        "\n",
        "        \n",
        "    with torch.no_grad():\n",
        "      \n",
        "      #print(cos_sim_2d(text_features, image_features))\n",
        "      for txt in text_list:\n",
        "        text = clip.tokenize(txt ).to(device)\n",
        "        text_features = model.encode_text(text).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "        #text_features2 = model2.encode_text(text).cpu().detach().numpy()\n",
        "        sim_= float(cos_sim_2d(text_features, image_features)[0]) \n",
        "\n",
        "        #sim_= float(cos_sim_2d(text_features, image_features)[0]) + float(cos_sim_2d(text_features2, image_features2)[0])\n",
        "        similarities.append(sim_)\n",
        "    return similarities\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    #!pip3 install transformers==4.15.0 timm==0.4.12 fairscale==0.4.4\n",
        "    #!git clone https://github.com/salesforce/BLIP\n",
        "    %cd BLIP\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "from models.blip import blip_decoder\n",
        "import glob\n",
        "image_size = 384\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    ]) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'\n",
        "    \n",
        "model = blip_decoder(pretrained=model_url, image_size=384, vit='large')\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "files= glob.glob(\"/content/*.jpg\")\n",
        "for f in files[:40]:\n",
        "    \n",
        "  raw_image = Image.open(f).convert('RGB')   \n",
        "  w,h = raw_image.size\n",
        "\n",
        "  display(raw_image.resize((100,int(100* h/w))))\n",
        "  image = transform(raw_image).unsqueeze(0).to(device)     \n",
        "  \n",
        "\n",
        "  captions = []\n",
        "\n",
        "  for topP in [0.1,  0.2, 0.3, 0.4, 0.5,0.6, 0.7]:\n",
        "    #[0.05,0.1, 0.15, 0.2,0.25, 0.3,0.35, 0.4, 0.45, 0.5,0.55, 0.6,0.65, 0.7,0.75, 0.8,0.85, 0.9, 0.95]\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        caption = model.generate(image, sample=True, max_length=30, min_length=10,top_p=topP,repetition_penalty=rep_pen)\n",
        "        #def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0)\n",
        "        captions.append(caption)\n",
        "\n",
        "  for beam_n in [1,2,3,4,5,6,7,8]:\n",
        "    #[0.05,0.1, 0.15, 0.2,0.25, 0.3,0.35, 0.4, 0.45, 0.5,0.55, 0.6,0.65, 0.7,0.75, 0.8,0.85, 0.9, 0.95]\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        caption = model.generate(image, sample=False, num_beams=beam_n, max_length=30, min_length=10,repetition_penalty=rep_pen)\n",
        "        #def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0)\n",
        "        captions.append(caption)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for topP in [0.1,  0.2, 0.3, 0.4, 0.5,0.6, 0.7]:\n",
        "    #[0.05,0.1, 0.15, 0.2,0.25, 0.3,0.35, 0.4, 0.45, 0.5,0.55, 0.6,0.65, 0.7,0.75, 0.8,0.85, 0.9, 0.95]\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        caption = model.generate(image, sample=True, max_length=45, min_length=30,top_p=topP,repetition_penalty=rep_pen)\n",
        "        #def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0)\n",
        "        captions.append(caption)\n",
        "\n",
        "  for beam_n in [1,2,3,4,5,6,7,8]:\n",
        "    #[0.05,0.1, 0.15, 0.2,0.25, 0.3,0.35, 0.4, 0.45, 0.5,0.55, 0.6,0.65, 0.7,0.75, 0.8,0.85, 0.9, 0.95]\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        caption = model.generate(image, sample=False, num_beams=beam_n, max_length=45, min_length=30,repetition_penalty=rep_pen)\n",
        "        #def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0)\n",
        "        captions.append(caption)\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  for topP in [0.1,  0.2, 0.3, 0.4, 0.5,0.6, 0.7,0.8]:\n",
        "    #[0.05,0.1, 0.15, 0.2,0.25, 0.3,0.35, 0.4, 0.45, 0.5,0.55, 0.6,0.65, 0.7,0.75, 0.8,0.85, 0.9, 0.95]\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        caption = model.generate(image, sample=True, max_length=60, min_length=45,top_p=topP,repetition_penalty=rep_pen)\n",
        "        #def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0)\n",
        "        captions.append(caption)\n",
        "\n",
        "  for beam_n in [1,2,3,4,5,6]:\n",
        "  \n",
        "    #[0.05,0.1, 0.15, 0.2,0.25, 0.3,0.35, 0.4, 0.45, 0.5,0.55, 0.6,0.65, 0.7,0.75, 0.8,0.85, 0.9, 0.95]\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        caption = model.generate(image, sample=False, num_beams=beam_n, max_length=60, min_length=45,repetition_penalty=rep_pen)\n",
        "        #def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0)\n",
        "        captions.append(caption)\n",
        "  \"\"\"\n",
        "  best_cannidates=[]\n",
        "  sims= clip_rank(raw_image,captions)\n",
        "  argmax_ = np.argmax(np.asarray(sims))\n",
        "  #print(\"Caption with highest sim\")\n",
        "  #print (captions[argmax_][0])\n",
        "  best_cannidates.append(captions[argmax_][0])\n",
        "  #print(sims[argmax_])\n",
        "  del sims[argmax_]\n",
        "  del captions[argmax_]\n",
        "  argmax_ = np.argmax(np.asarray(sims))\n",
        "  #print(\"Caption with 2nd highest sim\")\n",
        "  #print (captions[argmax_][0])\n",
        "  best_cannidates.append(captions[argmax_][0])\n",
        "  #print(sims[argmax_])\n",
        "  del sims[argmax_]\n",
        "  del captions[argmax_]\n",
        "  argmax_ = np.argmax(np.asarray(sims))\n",
        "  #print(\"Caption with 3nd highest sim\")\n",
        "  #print (captions[argmax_][0])\n",
        "  best_cannidates.append(captions[argmax_][0])\n",
        "  del sims[argmax_]\n",
        "  del captions[argmax_]\n",
        "  argmax_ = np.argmax(np.asarray(sims))\n",
        "  #print(\"Caption with 3nd highest sim\")\n",
        "  #print (captions[argmax_][0])\n",
        "  best_cannidates.append(captions[argmax_][0])\n",
        "  #print(sims[argmax_])\n",
        "\n",
        "  sims= clip_rank(raw_image,best_cannidates,clip_model=\"RN50x64\")\n",
        "  \n",
        "  argmax_ = np.argmax(np.asarray(sims))\n",
        "  print(\"BEST CAPTION AFTER RANKING WITH CLIP ViT L 14  & RESNET50x64:\")\n",
        "  print (best_cannidates[argmax_])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j2ccmZF4zkAd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}