{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlordier/colabs/blob/main/Simple_diarizer_youtube5(JSON_Included)_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JUST YOU NEED TO UPLOAD DATASET (/CONTENT/DIARIZERDATASET1)AND RUN ALL"
      ],
      "metadata": {
        "id": "ki5Zk8VF2thc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIQG45jaV9Ny"
      },
      "source": [
        "# **Install Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sixM6_MyRxXM",
        "outputId": "74504c4f-0c7d-4bc4-b541-05fa5092c343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting simple-diarizer\n",
            "  Downloading simple_diarizer-0.0.9-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.7/dist-packages (from simple-diarizer) (1.0.2)\n",
            "Collecting ipython==7.30.1\n",
            "  Downloading ipython-7.30.1-py3-none-any.whl (791 kB)\n",
            "\u001b[K     |████████████████████████████████| 791 kB 15.6 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.10.0\n",
            "  Downloading beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting youtube-dl==2021.12.17\n",
            "  Downloading youtube_dl-2021.12.17-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas==1.3.5 in /usr/local/lib/python3.7/dist-packages (from simple-diarizer) (1.3.5)\n",
            "Collecting validators==0.18.2\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Collecting matplotlib==3.5.1\n",
            "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 41.5 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.10.1\n",
            "  Downloading torchaudio-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 33.4 MB/s \n",
            "\u001b[?25hCollecting speechbrain==0.5.11\n",
            "  Downloading speechbrain-0.5.11-py3-none-any.whl (408 kB)\n",
            "\u001b[K     |████████████████████████████████| 408 kB 48.7 MB/s \n",
            "\u001b[?25hCollecting pytube==11.0.2\n",
            "  Downloading pytube-11.0.2-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.1-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==7.30.1->simple-diarizer) (4.4.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython==7.30.1->simple-diarizer) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==7.30.1->simple-diarizer) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython==7.30.1->simple-diarizer) (57.4.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==7.30.1->simple-diarizer) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==7.30.1->simple-diarizer) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython==7.30.1->simple-diarizer) (2.6.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython==7.30.1->simple-diarizer) (0.1.3)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython==7.30.1->simple-diarizer) (0.18.1)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.28-py3-none-any.whl (380 kB)\n",
            "\u001b[K     |████████████████████████████████| 380 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->simple-diarizer) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->simple-diarizer) (1.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->simple-diarizer) (3.0.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->simple-diarizer) (1.19.5)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.29.1-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 21.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->simple-diarizer) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->simple-diarizer) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->simple-diarizer) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5->simple-diarizer) (2018.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.2->simple-diarizer) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.2->simple-diarizer) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.2->simple-diarizer) (1.1.0)\n",
            "Collecting hyperpyyaml\n",
            "  Downloading HyperPyYAML-1.0.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch<=1.11,>=1.7 in /usr/local/lib/python3.7/dist-packages (from speechbrain==0.5.11->simple-diarizer) (1.10.0+cu111)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from speechbrain==0.5.11->simple-diarizer) (4.62.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 45.7 MB/s \n",
            "\u001b[?25hCollecting torch<=1.11,>=1.7\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/20/8a/c1e970cf64a1fa105bc5064b353ecabe77974b69029a80d04580fee38d5f/torch-1.10.1-cp37-cp37m-manylinux1_x86_64.whl\u001b[0m\n",
            "  Downloading torch-1.10.1-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.5 MB/s eta 0:00:33tcmalloc: large alloc 1147494400 bytes == 0x55a08e906000 @  0x7f0cdfd05615 0x55a0544533bc 0x55a05453418a 0x55a0544561cd 0x55a054548b3d 0x55a0544ca458 0x55a0544c502f 0x55a054457aba 0x55a0544ca2c0 0x55a0544c502f 0x55a054457aba 0x55a0544c6cd4 0x55a054549986 0x55a0544c6350 0x55a054549986 0x55a0544c6350 0x55a054549986 0x55a0544c6350 0x55a054457f19 0x55a05449ba79 0x55a054456b32 0x55a0544ca1dd 0x55a0544c502f 0x55a054457aba 0x55a0544c6cd4 0x55a0544c502f 0x55a054457aba 0x55a0544c5eae 0x55a0544579da 0x55a0544c6108 0x55a0544c502f\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.11,>=1.7->speechbrain==0.5.11->simple-diarizer) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from validators==0.18.2->simple-diarizer) (1.15.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython==7.30.1->simple-diarizer) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython==7.30.1->simple-diarizer) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.30.1->simple-diarizer) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain==0.5.11->simple-diarizer) (4.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain==0.5.11->simple-diarizer) (3.4.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain==0.5.11->simple-diarizer) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain==0.5.11->simple-diarizer) (2.23.0)\n",
            "Collecting ruamel.yaml>=0.15\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 39.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.2 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub->speechbrain==0.5.11->simple-diarizer) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain==0.5.11->simple-diarizer) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain==0.5.11->simple-diarizer) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain==0.5.11->simple-diarizer) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain==0.5.11->simple-diarizer) (3.0.4)\n",
            "Installing collected packages: ruamel.yaml.clib, torch, ruamel.yaml, pyyaml, torchaudio, soupsieve, sentencepiece, prompt-toolkit, hyperpyyaml, huggingface-hub, fonttools, youtube-dl, validators, speechbrain, pytube, matplotlib, ipython, beautifulsoup4, simple-diarizer\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.10.0+cu111\n",
            "    Uninstalling torchaudio-0.10.0+cu111:\n",
            "      Successfully uninstalled torchaudio-0.10.0+cu111\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.10.1 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.10.1 which is incompatible.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.30.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed beautifulsoup4-4.10.0 fonttools-4.29.1 huggingface-hub-0.4.0 hyperpyyaml-1.0.0 ipython-7.30.1 matplotlib-3.5.1 prompt-toolkit-3.0.28 pytube-11.0.2 pyyaml-6.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 sentencepiece-0.1.96 simple-diarizer-0.0.9 soupsieve-2.3.1 speechbrain-0.5.11 torch-1.10.1 torchaudio-0.10.1 validators-0.18.2 youtube-dl-2021.12.17\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "matplotlib",
                  "mpl_toolkits",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib==3.1.3\n",
            "  Downloading matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 13.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (3.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.5.1\n",
            "    Uninstalling matplotlib-3.5.1:\n",
            "      Successfully uninstalled matplotlib-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "simple-diarizer 0.0.9 requires matplotlib==3.5.1, but you have matplotlib 3.1.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed matplotlib-3.1.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install simple-diarizer\n",
        "!pip install matplotlib==3.1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT62IaSrCvix"
      },
      "outputs": [],
      "source": [
        "from simple_diarizer.diarizer import Diarizer\n",
        "from simple_diarizer.utils import (check_wav_16khz_mono, convert_wavfile, parse_ttml, \n",
        "                                   waveplot, combined_waveplot, waveplot_perspeaker, get_youtube_id, \n",
        "                                   download_youtube_wav, download_youtube_ttml)\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "from pprint import pprint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "\n",
        "from IPython.display import Audio, display, HTML\n",
        "from tqdm.autonotebook import tqdm\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import traceback\n",
        "os.mkdir('dialogues')\n",
        "os.mkdir('processed')\n",
        "os.mkdir('orignal_caption')\n",
        "os.mkdir('orignal_processed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk_A5C6B-L0D"
      },
      "source": [
        "## **Speaker Diarizer Inferences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62fgdyLq3TMW"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"/content/diarizer_dataset1.csv\")\n",
        "df_links=df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuEiCCgBVd-V"
      },
      "outputs": [],
      "source": [
        "links=[]\n",
        "beg=[]\n",
        "end=[]\n",
        "for i in range(len(df)):\n",
        "  if ((str((df.iloc[i,1])).strip()==\"Beginning\") | ((str(df.iloc[i,1])).strip()==\"beginning\")):\n",
        "    beg.append(df.iloc[i,2])\n",
        "  if ((str(df.iloc[i,1]).strip()==\"End\")or ((str(df.iloc[i,1])).strip()==\"end\")):  \n",
        "    end.append(df.iloc[i,2]) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9CLMT02p9xm",
        "outputId": "e1e6b845-fded-4dba-d039-49eefc8a64be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import datetime\n",
        "import time\n",
        "from datetime import timedelta\n",
        "def seconds(t):\n",
        "  if(len(t)<=5): \n",
        "    if(len(t)==5):\n",
        "\n",
        "      x = time.strptime('00:'+t,'%H:%M:%S')\n",
        "      sec=datetime.timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec).total_seconds()\n",
        "  \n",
        "    if(len(t)==4):\n",
        "      x = time.strptime('00:0'+t,'%H:%M:%S')\n",
        "      sec=datetime.timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec).total_seconds()\n",
        "  else:\n",
        "    if(len(t)==8):\n",
        "      if(int(t[0:2])>=24):\n",
        "        print(\"hours are greater than 24\") \n",
        "        \n",
        "        sec=int(t[0:2])*3600\n",
        "      else:\n",
        "        print(t[0:2])\n",
        "        x = time.strptime(t,'%H:%M:%S')\n",
        "        sec=datetime.timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec).total_seconds()\n",
        "    if(len(t)==7):\n",
        "      x = time.strptime('0'+t,'%H:%M:%S')\n",
        "      sec=datetime.timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec).total_seconds()\n",
        "      \n",
        "   \n",
        "  return int(sec)\n",
        "seconds(beg[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5XgnagfwIvG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import traceback\n",
        "diar = Diarizer(\n",
        "        embed_model='ecapa', # supported types: ['xvec', 'ecapa']\n",
        "        cluster_method='sc', # supported types: ['ahc', 'sc']\n",
        "        window=1.5, # size of window to extract embeddings (in seconds)\n",
        "        period=0.75 # hop of window (in seconds)\n",
        "    )\n",
        "def speaker_diarizer(link_id,i,s,e):\n",
        "  YOUTUBE_link = \"https://www.youtube.com/embed/\"+link_id +'?start='+str(int(s))+'&end='+str(int(e)) # The YouTube ID of the video you wish to diarize\n",
        "  print(YOUTUBE_link)\n",
        "  #https://www.youtube.com/embed/MUtRnMMOTmo?start=36&end=65\n",
        "  NUM_SPEAKERS = 2 # The number of speakers\n",
        "  try:\n",
        "    outih_segments, outih_worded_segments, outih_wav = diar.diarize_youtube(YOUTUBE_link, \n",
        "                                                                        num_speakers=None, silence_tolerance=0.2,\n",
        "                                                                        threshold=3, \n",
        "                                                                        outfolder='./outiih')\n",
        "    !youtube-dl --write-auto-sub --sub-lang en --sub-format ttml --skip-download -o \"%(id)s.%(ext)s\" $link_id\n",
        "    text_segments = parse_ttml('./{}.en.ttml'.format(link_id))\n",
        "    yt_worded_segments = diar.match_diarization_to_transcript(outih_segments, text_segments)\n",
        "    diar.nice_text_output(yt_worded_segments, '/content/dialogues/yt_test'+str(i)+\".txt\")\n",
        "    diar.nice_text_output(yt_worded_segments, '/content/orignal/orignal'+str(i)+\".txt\")\n",
        "  except Exception as error:\n",
        "    traceback.print_exc()\n",
        "    print(\"there is no caption in video\")\n",
        "link_list=[]\n",
        "for i in range(len(df_links)):\n",
        "  #print(i)\n",
        "  link_list.append(df_links.iloc[i,0])\n",
        "  id=df_links.iloc[i,0].split(\"=\",1)[1] \n",
        "  s_seconds=seconds(beg[i])\n",
        "  e_seconds=seconds(end[i])\n",
        "  speaker_diarizer(id,i,s_seconds,e_seconds) #it is just a one video to test the model performance\n",
        " # speaker_diarizer(id,i,s_seconds,e_seconds)   Use this while working with panadas dataframe (youtube links)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing**"
      ],
      "metadata": {
        "id": "-4GNJZqKbOzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install transformers\n",
        "!pip install -q rpunct\n",
        "!pip install -q language_tool_python\n",
        "!pip install -q clean-text[gpl]\n",
        "!pip install -q textblob\n",
        "!pip install -q nlpaug\n",
        "!pip install -q praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3fWZN5hbOWn",
        "outputId": "119b7657-dcd8-43d8-863e-7280c51c9ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 23.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 24.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.2\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.5 kB/s \n",
            "\u001b[K     |████████████████████████████████| 213 kB 38.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 981 kB 36.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 37.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 47.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 35.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 311 kB 50.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.7 MB 31.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 50.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 38.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 51.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 50.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 50.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 164 kB 52.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 111 kB 56.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 53.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 33.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 128 kB 51.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 49.1 MB/s \n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchaudio 0.10.1 requires torch==1.10.1, but you have torch 1.8.1 which is incompatible.\n",
            "simple-diarizer 0.0.9 requires matplotlib==3.5.1, but you have matplotlib 3.1.3 which is incompatible.\n",
            "simple-diarizer 0.0.9 requires pandas==1.3.5, but you have pandas 1.2.4 which is incompatible.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.9.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.30.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 174 kB 12.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 235 kB 47.7 MB/s \n",
            "\u001b[?25h  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 410 kB 13.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 176 kB 11.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import json\n",
        "import torch\n",
        "import argparse \n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "# from comet_atomic2020_bart.utils import calculate_rouge, use_task_specific_params, calculate_bleu_score, trim_batch\n",
        "from rpunct import RestorePuncts\n",
        "import language_tool_python\n",
        "import sys\n",
        "import warnings\n",
        "from spacy.lang.en import English\n",
        "from cleantext import clean\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "\n",
        "import pandas as pd\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "from os import listdir\n",
        "from os.path import isfile, join"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9GAe-xjbUJv",
        "outputId": "5f84caa9-39aa-42fb-f6bc-36b8b9613d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_URL(sample):\n",
        "    \"\"\"Remove URLs from a sample string\"\"\"\n",
        "    return re.sub(r\"http\\S+\", \"\", sample)\n",
        "\n",
        "\n",
        "def textNormalization(text):\n",
        "    processed_text=clean(text,\n",
        "    fix_unicode=True,               # fix various unicode errors\n",
        "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
        "    lower=False,                     # lowercase text\n",
        "    no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
        "    no_urls=  True,                  # replace all URLs with a special token\n",
        "    no_emails=True,                # replace all email addresses with a special token\n",
        "    no_phone_numbers=True,         # replace all phone numbers with a special token\n",
        "    no_numbers=False,               # replace all numbers with a special token\n",
        "    no_digits=False,                # replace all digits with a special token\n",
        "    no_currency_symbols=True,      # replace all currency symbols with a special token\n",
        "    no_punct=False,                 # remove punctuations\n",
        "    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
        "    replace_with_url=\"\",\n",
        "    replace_with_email=\"<EMAIL>\",\n",
        "    replace_with_phone_number=\"<PHONE>\",\n",
        "    replace_with_number=\"<NUMBER>\",\n",
        "    replace_with_digit=\"0\",\n",
        "    replace_with_currency_symbol=\" dollars\",\n",
        "    lang=\"en\")\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "def remove_emoji(processed_text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    processed_text=emoji_pattern.sub(r'', processed_text)\n",
        "    return processed_text\n",
        "def remStopWordsOur(lineIn):\n",
        "    stopWords= {'i','a','and','about','an','are','as','at','be','by','com','for','from','how','in','is','it','not','of','on','or','that','the','this','to','was','what','when','where','who','will','with','the','www','your','is','am','some','you','your','I','A','And','About','An','Are','As','At','Be','By','Com','For','From','How','In','Is','It','Not','Of','On','Or','That','The','This','To','Was','What','When','Where','Who','Will','With','The','Www','Your','Is','Am','Some','You','Your','Was'}\n",
        "    rmdStopWordsLn = ' '.join(w for w in lineIn.split() if w.lower() not in stopWords)\n",
        "    return rmdStopWordsLn\n",
        "\n",
        "\n",
        "def clean_text(text,remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n",
        "\n",
        "# if is_lower:\n",
        "#    text=text.lower()\n",
        "  if remove_repeat_text:\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text) \n",
        "\n",
        "  text = str(text).replace(\"\\n\", \" \")\n",
        "  #text = re.sub(r'[^\\w\\s]',' ',text)\n",
        "  #text = re.sub('[0-9]',\"\",text)\n",
        "  text = re.sub(\" +\", \" \", text)\n",
        "  text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
        "  return text\n",
        "\n",
        "\n",
        "def rem_username(text):\n",
        "  text = ' '.join(re.sub(\"([@#][A-Za-z0-9_]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
        "  return text\n",
        "\n",
        "\n",
        "#Main preprocessing block\n",
        "def preprocessing_block(text):\n",
        "# The default language is 'english'\n",
        "  #1. Remove username\n",
        "  processed_text=rem_username(text)\n",
        "  #2. Text Normalization and remove URLs\n",
        "  processed_text=textNormalization(processed_text)\n",
        "  #3. Grametetical error correction and text cleaning\n",
        "  tool = language_tool_python.LanguageTool('en')\n",
        "  processed_text = tool.correct(processed_text)\n",
        "  processed_text=clean_text(processed_text)\n",
        "  #4 remove emoji in text\n",
        "  processed_text=remove_emoji(str(processed_text))\n",
        "  #5. punctuations restoring\n",
        "  rpunct = RestorePuncts()\n",
        "  processed_text=rpunct.punctuate(processed_text)\n",
        "  #6. sentence spliting\n",
        "  nlp = English()\n",
        "  sbd = nlp.create_pipe('sentencizer')\n",
        "  nlp.add_pipe(sbd)\n",
        "  doc = nlp(str(processed_text))\n",
        "  sents_list = []\n",
        "  for sent in doc.sents:\n",
        "     sents_list.append(sent.text)\n",
        "  return sents_list\n"
      ],
      "metadata": {
        "id": "gkMJV7hvbdRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dialogue Post Processing on each generated file**"
      ],
      "metadata": {
        "id": "SNidoC4XaHoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code will iterate over each file of generated dialogues from youtube links"
      ],
      "metadata": {
        "id": "tj6IjBOx-zt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1) Get rid of repeated Speakers"
      ],
      "metadata": {
        "id": "zGRZpXv3wfBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Speaker 0:\n",
        "Speaker 0:\n",
        "Speaker 1:)\n",
        "(Speaker 0:\n",
        "Speaker 1: )"
      ],
      "metadata": {
        "id": "HYVIKEMu0ml9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def post_processing(path1,path2,i):\n",
        "  f = open(path2+str(i), 'w')\n",
        "  with open(path1) as topo_file:\n",
        "      i=0\n",
        "      speaker_list=[]\n",
        "      for line in topo_file:\n",
        "          i=i+1 \n",
        "          try:\n",
        "            speaker=str(line).split(\"Speaker\",1)[1][1:2]\n",
        "            speaker_list.append(speaker)\n",
        "            if(speaker_list[len(speaker_list)-2]==speaker and i!=1):\n",
        "              continue\n",
        "            f.write(speaker+' \\n')  # python will convert \\n to os.linesep\n",
        "          except Exception:\n",
        "           # processed=preprocessing_block(line)\n",
        "            f.write(line+'\\n') \n",
        "  f.close() "
      ],
      "metadata": {
        "id": "LagR1S-lBTID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) Remove Empty Lines"
      ],
      "metadata": {
        "id": "j9Uy0tCnwudB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def rem_empty_lines(path):\n",
        "  with open(path, 'r+') as fd:\n",
        "    lines = fd.readlines()\n",
        "    fd.seek(0)\n",
        "    fd.writelines(line for line in lines if line.strip())\n",
        "    fd.truncate()"
      ],
      "metadata": {
        "id": "ndPaCAtqMbWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Concatenate 'Speaker:' Text to Speaker IDs and concatenate sentences in different lines"
      ],
      "metadata": {
        "id": "QMrVcE5Ew2Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def concatenate_speaker_str(path):\n",
        "  with open(path) as f:\n",
        "      exampleText = f.read()\n",
        "\n",
        "  new = ''\n",
        "\n",
        "  for line in exampleText.split('\\n'):\n",
        "      if line and line[0].strip().isdigit():\n",
        "          new += '\\n'+'Speaker '+ line +': \\n'\n",
        "      if re.search('[a-zA-Z]', line) and line[0].strip().isdigit()==False :  # check if there is text\n",
        "          new += line + ' '\n",
        "  f = open(path, 'w')\n",
        "\n",
        "  f.write(new) \n",
        "  f.close() \n"
      ],
      "metadata": {
        "id": "EBMXRhdJmjiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialogues_path=\"/content/dialogues/\"\n",
        "save_path='/content/processed/myfile'\n",
        "onlyfiles = [f for f in listdir(dialogues_path) if isfile(join(dialogues_path, f))]\n",
        "for i in range(len(onlyfiles)):\n",
        "  post_processing(dialogues_path+onlyfiles[i],save_path,i)\n",
        "  rem_empty_lines(save_path+str(i))\n",
        "  concatenate_speaker_str(save_path+str(i))\n"
      ],
      "metadata": {
        "id": "ToI766oeq9kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def listToString(s): \n",
        "    \n",
        "    # initialize an empty string\n",
        "    str1 = \"\" \n",
        "    \n",
        "    # traverse in the string  \n",
        "    for ele in s: \n",
        "        str1 += ele  \n",
        "    \n",
        "    # return string  \n",
        "    return str1 \n",
        "        \n",
        "     \n",
        "def text_processing2(path):\n",
        "  with open(path) as f:\n",
        "      exampleText = f.read()\n",
        "\n",
        "  new = ''\n",
        "\n",
        "  for line in exampleText.split('\\n'):\n",
        "    try:\n",
        "      s=preprocessing_block(line)\n",
        "      string=listToString(s)\n",
        "      print(string)\n",
        "\n",
        "    except Exception as error:\n",
        "   #   traceback.print_exc()\n",
        "      print('..')\n",
        "      string='\\n'\n",
        "    try:  \n",
        "      speaker_id=str(line).split(\"Speaker\",1)[1][1:2]\n",
        "    except:\n",
        "      speaker_id=''\n",
        "      #s=['hello','world']\n",
        "    #string=listToString(s)\n",
        "\n",
        "    if line and speaker_id.isdigit():\n",
        "      new+='\\n'+line\n",
        "    else:\n",
        "      new+=string\n",
        " # f = open(path, 'w')\n",
        "  print(new)\n",
        "  return new\n",
        "  #f.write(new) \n",
        " #f.close() \n",
        "dialogues_path=\"/content/dialogues/\"\n",
        "save_path='/content/processed/'\n",
        "text_list=[]\n",
        "onlyfiles = [f for f in listdir(save_path) if isfile(join(save_path, f))]\n",
        "for i in range(len(onlyfiles)):\n",
        "  text=text_processing2(save_path+onlyfiles[i])\n",
        "  text_list.append(text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWmvYKOhHqa5",
        "outputId": "7c18a102-b4a3-450a-caa5-af5ada876807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Orignal Captions"
      ],
      "metadata": {
        "id": "C-zTd5Q8vdTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dialogues_path=\"/content/orignal_caption/\"\n",
        "save_path='/content/orignal_processed/orignal'\n",
        "onlyfiles = [f for f in listdir(dialogues_path) if isfile(join(dialogues_path, f))]\n",
        "for i in range(len(onlyfiles)):\n",
        "  post_processing(dialogues_path+onlyfiles[i],save_path,i)\n",
        "  rem_empty_lines(save_path+str(i))\n",
        "  concatenate_speaker_str(save_path+str(i))\n"
      ],
      "metadata": {
        "id": "oSNoLDf3vjKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def listToString(s): \n",
        "    \n",
        "    # initialize an empty string\n",
        "    str1 = \"\" \n",
        "    \n",
        "    # traverse in the string  \n",
        "    for ele in s: \n",
        "        str1 += ele  \n",
        "    \n",
        "    # return string  \n",
        "    return str1 \n",
        "        \n",
        "     \n",
        "def text_processing2(path):\n",
        "  with open(path) as f:\n",
        "      exampleText = f.read()\n",
        "\n",
        "  new = ''\n",
        "\n",
        "  for line in exampleText.split('\\n'):\n",
        "    try:\n",
        "     # s=preprocessing_block(line)\n",
        "      string=line\n",
        "      print(string)\n",
        "\n",
        "    except Exception as error:\n",
        "   #   traceback.print_exc()\n",
        "      print('..')\n",
        "      string='\\n'\n",
        "    try:  \n",
        "      speaker_id=str(line).split(\"Speaker\",1)[1][1:2]\n",
        "    except:\n",
        "      speaker_id=''\n",
        "      #s=['hello','world']\n",
        "    #string=listToString(s)\n",
        "\n",
        "    if line and speaker_id.isdigit():\n",
        "      new+='\\n'+line\n",
        "    else:\n",
        "      new+=string\n",
        " # f = open(path, 'w')\n",
        "  print(new)\n",
        "  return new\n",
        "  #f.write(new) \n",
        " #f.close() \n",
        "dialogues_path=\"/content/orignal_caption/\"\n",
        "save_path='/content/orignal_processed/'\n",
        "orignal_text_list=[]\n",
        "onlyfiles = [f for f in listdir(save_path) if isfile(join(save_path, f))]\n",
        "for i in range(len(onlyfiles)):\n",
        "  orignal_text=text_processing2(save_path+onlyfiles[i])\n",
        "  orignal_text_list.append(orignal_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCfHwYlyxULM",
        "outputId": "9e638c2d-9b91-4252-b26f-8de541943998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Speaker 0 : \n",
            "hey what's up mkbhd here so as you may have seen barack obama's been on a pretty massive book tour for his new book a promised land uh it's been a pretty great read so far for me but then youtube reached out and they're like hey do you want to ask obama anything about what he talks about in this book and so of course i said yes that would be awesome but that's that's a big moment for me like what do i ask the president if i could ask him anything so anyway what i have for you are the two questions that i chose and this is actually part of a larger youtube originals project called booktube where he answers a bunch of questions from a bunch of other creators about that book and what he wants to talk about including a really good one from mark rober about nasa and the origins of the white house science fair which i thought were really interesting so i'll link that below for you to check it out but this is a sort of extended cut of a little chat and uh i wanted to share enjoy [Music] mr president my name is marquez brownlee i run a youtube channel called mkbhd so that's mkb which is my initials plus hd that's high definition that's where i get to be a huge nerd basically i talk all kinds of tech products review tech try to make high quality videos and we like to have fun with it you know these days tech is in everything it feels like it's a part of the fabric of the culture we live in so on my channel it's everything from smartphones to 5g to computers to electric cars basically anything with an on button all the way to interviews with people with unique perspectives on tech it's a lot of fun i'm 26 years old i've been doing this for about 10 years now over 10 years so a long time in my own life and i also happen to be a professional ultimate frisbee player so shout out to the new york empire we had an undefeated season last year in 2019 and i'm also an avid golfer and a pretty big nba basketball fan so i gotta say i'm pretty excited to be able to talk about your book a promised land which is your third book and uh first in a two-part presidential memoir so i started my youtube channel in 2009 so a year after your inauguration which i was at by the way um i wish i had some some pictures or something from that but i didn't have a camera then believe it or not but i do remember it was very cold the timing of this isn't lost on me i feel like i feel like i owe a lot of my position now to the rise of technology and its accessibility and i feel like maybe to an extent you can say a lot of the same like a lot of people have called you the first social media president but more than that it's just you know you look at the administration before you they hadn't quite used it hadn't quite cracked it then you came along and all of your teams using social media embracing technology embracing this new stuff so can you speak to that level of embracing technology and and how it affects our politics and activating new audiences and speaking to new people you know obviously we can see it has its ups and downs but i feel like \n",
            "Speaker 1 : \n",
            "a unique challenge of yours was uh finding a balance there that we hadn't really seen before you thanks mark has unbelievable hops uh on that clip of you in uh ultimate frisbee you're right technology had a big impact on my campaign first of all the reason we were really early adapters of a lot of technology as a tool for reaching out to voters was because i had a bunch of folks in their 20s working for me young people who would come to me and say listen we had this thing called myspace and meetup what it turned out was that those early iterations of social media became a really powerful way for us to build a volunteer base and a movement it made me hugely optimistic about the possibilities of technology as a as a force for democratization during the presidency what we learned was that that same technology could be used for ill or for good you know hatred could be communicated as well as love and compassion that's something that i think we still struggle with and i write about in a promised land how do we harness \n",
            "Speaker 0 : \n",
            "this technology for good and try to mitigate uh some of the problems that uh that it creates so a lot of the worlds that i run in haven't been incredibly diverse right i mentioned golf earlier ultimate frisbee technology and what i can say is at a youth level and i think at a lot of the beginner levels these spaces are more diverse than ever which is awesome but you'll notice at a high level ultimate frisbee not very diverse at a high level golf not very diverse and at a high level technology still not a very diverse place and i feel like that makes the spotlight just a little bit hotter as a black man moving through these spaces moving through these worlds i talked about this a little bit in a video actually i made on my channel in the past year so from your book and just from observing it's clear that you've felt that spotlight over time as well i mean not just as the first black president of course but i mean just look at politics as a whole right so that got me super curious just how did that make you feel how did that affect you mentally do you feel like it pushed you to do better uh do you feel like it made the way you present things a little bit different you're extra careful in the way you \n",
            "Speaker 1 : \n",
            "articulate certain things yeah just what's been your feeling about achievement in less diverse spaces you're absolutely right i i write about the fact that there were occasions where i would have to deal with expectations or stereotypes or attitudes that perhaps if i had been white i would not have had to deal with not just me but a lot of the african-americans or hispanics asian americans or in some cases women were the first in their positions in my administration all of them had to navigate spaces where traditionally there hadn't been a lot of folks who looked like them in those situations and it is a extra burden but my attitude always was that it was also a privilege to be in these environments where you can actually help these spaces where you're one of the firsts and that requires you speaking out and and having the confidence that you can uh that you can propose solutions that make the whole team or organization work better \n",
            "\n",
            "Speaker 0 : hey what's up mkbhd here so as you may have seen barack obama's been on a pretty massive book tour for his new book a promised land uh it's been a pretty great read so far for me but then youtube reached out and they're like hey do you want to ask obama anything about what he talks about in this book and so of course i said yes that would be awesome but that's that's a big moment for me like what do i ask the president if i could ask him anything so anyway what i have for you are the two questions that i chose and this is actually part of a larger youtube originals project called booktube where he answers a bunch of questions from a bunch of other creators about that book and what he wants to talk about including a really good one from mark rober about nasa and the origins of the white house science fair which i thought were really interesting so i'll link that below for you to check it out but this is a sort of extended cut of a little chat and uh i wanted to share enjoy [Music] mr president my name is marquez brownlee i run a youtube channel called mkbhd so that's mkb which is my initials plus hd that's high definition that's where i get to be a huge nerd basically i talk all kinds of tech products review tech try to make high quality videos and we like to have fun with it you know these days tech is in everything it feels like it's a part of the fabric of the culture we live in so on my channel it's everything from smartphones to 5g to computers to electric cars basically anything with an on button all the way to interviews with people with unique perspectives on tech it's a lot of fun i'm 26 years old i've been doing this for about 10 years now over 10 years so a long time in my own life and i also happen to be a professional ultimate frisbee player so shout out to the new york empire we had an undefeated season last year in 2019 and i'm also an avid golfer and a pretty big nba basketball fan so i gotta say i'm pretty excited to be able to talk about your book a promised land which is your third book and uh first in a two-part presidential memoir so i started my youtube channel in 2009 so a year after your inauguration which i was at by the way um i wish i had some some pictures or something from that but i didn't have a camera then believe it or not but i do remember it was very cold the timing of this isn't lost on me i feel like i feel like i owe a lot of my position now to the rise of technology and its accessibility and i feel like maybe to an extent you can say a lot of the same like a lot of people have called you the first social media president but more than that it's just you know you look at the administration before you they hadn't quite used it hadn't quite cracked it then you came along and all of your teams using social media embracing technology embracing this new stuff so can you speak to that level of embracing technology and and how it affects our politics and activating new audiences and speaking to new people you know obviously we can see it has its ups and downs but i feel like \n",
            "Speaker 1 : a unique challenge of yours was uh finding a balance there that we hadn't really seen before you thanks mark has unbelievable hops uh on that clip of you in uh ultimate frisbee you're right technology had a big impact on my campaign first of all the reason we were really early adapters of a lot of technology as a tool for reaching out to voters was because i had a bunch of folks in their 20s working for me young people who would come to me and say listen we had this thing called myspace and meetup what it turned out was that those early iterations of social media became a really powerful way for us to build a volunteer base and a movement it made me hugely optimistic about the possibilities of technology as a as a force for democratization during the presidency what we learned was that that same technology could be used for ill or for good you know hatred could be communicated as well as love and compassion that's something that i think we still struggle with and i write about in a promised land how do we harness \n",
            "Speaker 0 : this technology for good and try to mitigate uh some of the problems that uh that it creates so a lot of the worlds that i run in haven't been incredibly diverse right i mentioned golf earlier ultimate frisbee technology and what i can say is at a youth level and i think at a lot of the beginner levels these spaces are more diverse than ever which is awesome but you'll notice at a high level ultimate frisbee not very diverse at a high level golf not very diverse and at a high level technology still not a very diverse place and i feel like that makes the spotlight just a little bit hotter as a black man moving through these spaces moving through these worlds i talked about this a little bit in a video actually i made on my channel in the past year so from your book and just from observing it's clear that you've felt that spotlight over time as well i mean not just as the first black president of course but i mean just look at politics as a whole right so that got me super curious just how did that make you feel how did that affect you mentally do you feel like it pushed you to do better uh do you feel like it made the way you present things a little bit different you're extra careful in the way you \n",
            "Speaker 1 : articulate certain things yeah just what's been your feeling about achievement in less diverse spaces you're absolutely right i i write about the fact that there were occasions where i would have to deal with expectations or stereotypes or attitudes that perhaps if i had been white i would not have had to deal with not just me but a lot of the african-americans or hispanics asian americans or in some cases women were the first in their positions in my administration all of them had to navigate spaces where traditionally there hadn't been a lot of folks who looked like them in those situations and it is a extra burden but my attitude always was that it was also a privilege to be in these environments where you can actually help these spaces where you're one of the firsts and that requires you speaking out and and having the confidence that you can uh that you can propose solutions that make the whole team or organization work better \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make a JSON file"
      ],
      "metadata": {
        "id": "ZsoEZl09TdwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data = {}\n",
        "data['video_link'] = link_list\n",
        "data['processed_text'] = text_list\n",
        "data['orignal']=orignal_text_list\n",
        "json_data = json.dumps(data)"
      ],
      "metadata": {
        "id": "RhnkHSb8HrpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('processed.json', 'w') as outfile:\n",
        "    outfile.write(json_data)"
      ],
      "metadata": {
        "id": "kvOJTaw4fbzA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Simple_diarizer_youtube5(JSON_Included)_2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}