{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chandler_model_9.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlordier/colabs/blob/main/Chandler_model_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqXmfUhF7dR0",
        "outputId": "9d839fd1-95d5-47e2-9e70-f4981e6bd5b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'News-Headlines-Dataset-For-Sarcasm-Detection'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 66 (delta 0), reused 0 (delta 0), pack-reused 62\u001b[K\n",
            "Unpacking objects: 100% (66/66), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rajlm10/Chandler.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ennOAvWK-jUQ",
        "outputId": "6f6021c3-595b-4c48-9573-3c73ff4ff419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Chandler'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 82 (delta 30), reused 33 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (82/82), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DTrimarchi10/confusion_matrix.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKpts3xsZzFK",
        "outputId": "e6dc4694-cc84-4256-d070-0b6649e04c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'confusion_matrix'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 0 (delta 0), pack-reused 15\u001b[K\n",
            "Unpacking objects: 100% (15/15), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/Chandler/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYBCkPMn-_13",
        "outputId": "c6afa6f2-e2c8-4cb3-ad4e-271ceea2a608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sx8nX5EIOQF",
        "outputId": "84656a3b-2c79-4725-99b9-13c725d5eb2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.4.0\n",
            "  Downloading tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl (394.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.7 MB 18 kB/s \n",
            "\u001b[?25hCollecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.37.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.2.0)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.12.0)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.7.0)\n",
            "Collecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 38.7 MB/s \n",
            "\u001b[?25hCollecting h5py~=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 34.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (3.1.1)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68720 sha256=95b69225ea3f966bd26879178a0e3226863dcf5626a046555e171a9347e5427f\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built wrapt\n",
            "Installing collected packages: typing-extensions, grpcio, wrapt, tensorflow-estimator, h5py, gast, flatbuffers, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.43.0\n",
            "    Uninstalling grpcio-1.43.0:\n",
            "      Successfully uninstalled grpcio-1.43.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.13.3\n",
            "    Uninstalling wrapt-1.13.3:\n",
            "      Successfully uninstalled wrapt-1.13.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "Successfully installed flatbuffers-1.12 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlc1UtNrSt30",
        "outputId": "e59ce9ae-9519-4dda-90dc-b059f28eefa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.4.0\n",
            "  Downloading Keras-2.4.0-py2.py3-none-any.whl (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 40 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 51 kB 16.7 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 61 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 71 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 81 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 92 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 102 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 112 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 122 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 133 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 143 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 153 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 163 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 170 kB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (1.4.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.37.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.15.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.3.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.32.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.3.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.12.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.7.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.1.1)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "Successfully installed keras-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESTART RUNTIME HERE AND THEN RUN ALL THE CODEBLOCKS AFTER THIS BLOCK**"
      ],
      "metadata": {
        "id": "yAN619A-WWN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from confusion_matrix import cf_matrix\n",
        "import re\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import string\n",
        "import time\n",
        "import pickle\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHanhwak9tgn",
        "outputId": "fcb8a95c-7d9d-4587-ed38-2acc90fe62eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Chandler/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aehQNinYZ9z3",
        "outputId": "eb9b74f2-bd13-49e9-df03-84124482c019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Chandler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function for decontracting the text data so that it can be fed to the model\n",
        "# and trained properly\n",
        "\n",
        "def decontract(text):\n",
        "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
        "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'s$\", \" is\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    return text\n",
        "stopwords_english = set(stopwords.words('english'))-set(['No','no','not','Not'])"
      ],
      "metadata": {
        "id": "LyDhVfeMIzWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for pre-processing the text data so that it can be fed to the model\n",
        "# and trained properly\n",
        "\n",
        "def preprocess(text,stopwords=stopwords_english):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "    # remove stock market tickers like $GE\n",
        "    text = re.sub(r'\\$\\w*', '', text)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    text = re.sub(r'^RT[\\s]+', '', text)\n",
        "    # remove hyperlinks\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    #Decontract texts\n",
        "    text=decontract(text)\n",
        "    # tokenize texts\n",
        "\n",
        "\n",
        "    texts_clean = []\n",
        "    for word in text.split():\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in set(string.punctuation)-set(['!','?','.','@',':'])):  # remove punctuation\n",
        "            #Lemmatize word \n",
        "            lem_word = lemmatizer.lemmatize(word,\"v\")  # Lemmatizing word\n",
        "            texts_clean.append(lem_word)\n",
        "\n",
        "    return \" \".join(texts_clean)"
      ],
      "metadata": {
        "id": "oCB4bd79I2Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data for training\n",
        "\n",
        "df1 = pd.read_json(\"/content/News-Headlines-Dataset-For-Sarcasm-Detection/Sarcasm_Headlines_Dataset.json\", lines=True) #Reading the dataframe\n",
        "df1 = df1[['headline','is_sarcastic']]"
      ],
      "metadata": {
        "id": "_tfWcLQL91ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Undersampling 10 percent of the sarcastic comments. This improved the f1-score after conducting a few experiments.\n",
        "\n",
        "df = df1.drop(df1[df1['is_sarcastic']==1].sample(frac=0.10).index) \n",
        "df = df.reset_index()"
      ],
      "metadata": {
        "id": "PU0ikl_A-DLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform the inputs and also split the data into training and test sets.\n",
        "#I have used a split of 80-20 to make sure our model learns better since it is deep.\n",
        "\n",
        "inputs=list(df['headline'].apply(lambda x: preprocess(x)))\n",
        "labels=list(df['is_sarcastic'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2)"
      ],
      "metadata": {
        "id": "wCJpHMoY-V0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing using subwords as opposed to characters or entire words. This helps improve performance and was used while training BERT ,\n",
        "#The maxlen of 50 has been experimentally chosen and is shorter than a few of the longest training inputs. This was done \n",
        "# as most inputs are within this length and we can train faster.\n",
        "\n",
        "def subword_tokenize(train_corpus, vocab_size=2**14, max_length=50,tokenizer=None):\n",
        "  # Create the vocabulary using Subword tokenization\n",
        "  if(tokenizer==None):\n",
        "    tokenizer_corpus = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_corpus, target_vocab_size=2**14)\n",
        "  else:\n",
        "    tokenizer_corpus=tokenizer\n",
        "  # Get the final vocab size\n",
        "  vocab_size = tokenizer_corpus.vocab_size \n",
        "  \n",
        "  # Tokenize the corpus\n",
        "  sentences = [tokenizer_corpus.encode(sentence) for sentence in train_corpus]\n",
        "\n",
        "  #Pad the sentences with 0s upto a length of 50\n",
        "  sentences = tf.keras.preprocessing.sequence.pad_sequences(sentences,value=0,padding='post',maxlen=50)\n",
        "  \n",
        "  return sentences, tokenizer_corpus, vocab_size"
      ],
      "metadata": {
        "id": "CvBgGpueAOcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs, tokenizer, vocab_size = subword_tokenize(train_corpus=X_train)\n",
        "tokenized_test_inputs, _, vocab_size2 = subword_tokenize(X_test, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "fVp9NjgfJUOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dataset \n",
        "# A batch size of 128 has been chosen for the task\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((tokenized_inputs, y_train))\n",
        "dataset = dataset.shuffle(len(tokenized_inputs), reshuffle_each_iteration=True).batch(128, drop_remainder=True)\n",
        "\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "lkbXpy3nJbEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemented based on the Attention is all you need paper.\n",
        "\n",
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    # Calculating  QK.T\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    # Get the scale factor\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    # Scale the dot product for improved training speed and stability \n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    # Apply the mask\n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    # dot product of QK.T with Values \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "metadata": {
        "id": "vF23jeLPJqrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, n_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.n_heads == 0\n",
        "        # Calculate the dimension of every head or projection\n",
        "        self.d_head = self.d_model // self.n_heads\n",
        "        # Set the weight matrices for Q, K and V\n",
        "        self.query_weights = layers.Dense(units=self.d_model)\n",
        "        self.key_weights = layers.Dense(units=self.d_model)\n",
        "        self.value_weights = layers.Dense(units=self.d_model)\n",
        "        # Set the weight matrix for the output of the multi-head attention W0\n",
        "        self.final_weights = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        # Set the dimension of the projections\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.n_heads,\n",
        "                 self.d_head)\n",
        "        # Split the input vectors\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, n_heads, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        # Get the batch size\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        # Set the Query, Key and Value matrices\n",
        "        queries = self.query_weights(queries)\n",
        "        keys = self.key_weights(keys)\n",
        "        values = self.value_weights(values)\n",
        "        # Split Q, K y V between the heads or projections\n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        # Apply the scaled dot product\n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        # Get the attention scores\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        # Concat the h heads or projections\n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        # Apply W0 to get the output of the multi-head attention\n",
        "        outputs = self.final_weights(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "mOOxQBonJym6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles # (seq_length, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # input shape batch_size, seq_length, d_model\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        # Calculate the angles given the input\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        # Calculate the positional encodings\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        # Expand the encodings with a new dimension\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        \n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "metadata": {
        "id": "9OnfyHm2JzA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, n_heads, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # Hidden units of the feed forward component\n",
        "        self.FFN_units = FFN_units\n",
        "        # Set the number of projectios or heads\n",
        "        self.n_heads = n_heads\n",
        "        # Dropout rate\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # Build the multihead layer\n",
        "        self.multi_head_attention = MultiHeadAttention(self.n_heads)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        # Layer Normalization\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        # Fully connected feed forward layer\n",
        "        self.ffn1_relu = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.ffn2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        # Layer normalization\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        # Forward pass of the multi-head attention\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        # Call to the residual connection and layer normalization\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        # Call to the FC layer\n",
        "        outputs = self.ffn1_relu(attention)\n",
        "        outputs = self.ffn2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        # Call to residual connection and the layer normalization\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "H0sX-anVJ2DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 FFN_units,\n",
        "                 n_heads,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.n_layers = n_layers\n",
        "        self.d_model = d_model\n",
        "        # The embedding layer\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        # Positional encoding layer\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        # Stack of n layers of multi-head attention and FC\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        n_heads,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(n_layers)]\n",
        "        self.final_layer = layers.Dense(units=2, name=\"final_layer\")\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        # Get the embedding vectors\n",
        "        outputs = self.embedding(inputs)\n",
        "        # Scale the embeddings by sqrt of d_model\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # Positional encodding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        # Call the stacked layers\n",
        "        for i in range(self.n_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "        #print(outputs.shape)\n",
        "\n",
        "        logits=tf.math.reduce_mean(outputs,1)\n",
        "\n",
        "\n",
        "        logits=self.final_layer(logits)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "aTbHaf_uJ4e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=pickle.load(open('models/tokenizer.sav', 'rb'))"
      ],
      "metadata": {
        "id": "_1wzQffCJ6ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq): #seq: (batch_size, seq_length)\n",
        "# Create the mask for padding\n",
        "  mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def loss_function(target, pred): #Notice from_logits=True\n",
        "  return tf.keras.losses.BinaryCrossentropy(from_logits=True)(target,pred)\n",
        "\n",
        "#Use a custom scheduler as per the paper with 4000 warmup steps\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "i2CXib6rNW5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_train(dataset, encoder, n_epochs, print_every=50):\n",
        "  ''' Train the Encoder model for n epochs using the data generator dataset'''\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  # In every epoch\n",
        "  for epoch in range(n_epochs):\n",
        "    print(\"Starting epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    # Reset the losss and accuracy calculations\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    # Get a batch of inputs and targets\n",
        "    for (batch, (inputs, targets)) in enumerate(dataset):\n",
        "\n",
        "        #Since we are using the binary cross entropy loss\n",
        "        targets = pd.get_dummies(targets).astype('float').values\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Call the transformer and get the predicted output\n",
        "            predictions = encoder(inputs, create_padding_mask(inputs), True)\n",
        "            # Calculate the loss\n",
        "            loss = loss_function(targets, predictions)\n",
        "        # Update the weights and optimizer\n",
        "        gradients = tape.gradient(loss, encoder.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, encoder.trainable_variables))\n",
        "        # Save and store the metrics\n",
        "        train_loss(loss)\n",
        "       \n",
        "        train_accuracy(targets, predictions)\n",
        "        \n",
        "        if batch % print_every == 0:\n",
        "            losses.append(train_loss.result())\n",
        "            accuracies.append(train_accuracy.result())\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "\n",
        "\n",
        "  return losses, accuracies"
      ],
      "metadata": {
        "id": "bWp6ZinyVK1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENCODER_LAYERS=6\n",
        "FFN_UNITS=512\n",
        "ATTN_HEADS=8\n",
        "DROPOUT_RATE=0.1\n",
        "EMBEDDING_DIM=256"
      ],
      "metadata": {
        "id": "GGdc5CbiVMl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "# Create the Encoder model\n",
        "encoder = Encoder(ENCODER_LAYERS,FFN_UNITS,ATTN_HEADS,DROPOUT_RATE,vocab_size,EMBEDDING_DIM)\n",
        "\n",
        "\n",
        "# Define a metric to store the mean loss of every epoch\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "# Define a matric to save the accuracy in every epoch\n",
        "train_accuracy = tf.keras.metrics.BinaryAccuracy(name=\"train_accuracy\")\n",
        "# Create the scheduler for learning rate decay\n",
        "leaning_rate = CustomSchedule(256)\n",
        "# Create the Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "FQ-ZhFyXVODp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses, accuracies = main_train(dataset, encoder, 5) #Training beyond 5 epochs causes overfitting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L05zPk8wVQAU",
        "outputId": "b127f058-82b2-48b8-8144-b6b9b6de7219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Epoch 1 Batch 0 Loss 0.8978 Accuracy 0.5000\n",
            "Epoch 1 Batch 50 Loss 0.7403 Accuracy 0.5120\n",
            "Epoch 1 Batch 100 Loss 0.7115 Accuracy 0.5228\n",
            "Epoch 1 Batch 150 Loss 0.6981 Accuracy 0.5360\n",
            "Starting epoch 2\n",
            "Epoch 2 Batch 0 Loss 0.6937 Accuracy 0.5742\n",
            "Epoch 2 Batch 50 Loss 0.6023 Accuracy 0.6517\n",
            "Epoch 2 Batch 100 Loss 0.5764 Accuracy 0.6773\n",
            "Epoch 2 Batch 150 Loss 0.5596 Accuracy 0.6917\n",
            "Starting epoch 3\n",
            "Epoch 3 Batch 0 Loss 0.4598 Accuracy 0.7695\n",
            "Epoch 3 Batch 50 Loss 0.4287 Accuracy 0.7963\n",
            "Epoch 3 Batch 100 Loss 0.4256 Accuracy 0.7968\n",
            "Epoch 3 Batch 150 Loss 0.4179 Accuracy 0.8010\n",
            "Starting epoch 4\n",
            "Epoch 4 Batch 0 Loss 0.2925 Accuracy 0.8711\n",
            "Epoch 4 Batch 50 Loss 0.3000 Accuracy 0.8699\n",
            "Epoch 4 Batch 100 Loss 0.3174 Accuracy 0.8600\n",
            "Epoch 4 Batch 150 Loss 0.3138 Accuracy 0.8617\n",
            "Starting epoch 5\n",
            "Epoch 5 Batch 0 Loss 0.1718 Accuracy 0.9258\n",
            "Epoch 5 Batch 50 Loss 0.2233 Accuracy 0.9071\n",
            "Epoch 5 Batch 100 Loss 0.2192 Accuracy 0.9088\n",
            "Epoch 5 Batch 150 Loss 0.2315 Accuracy 0.9018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(encoder,tokenized_sentences):\n",
        "  logits=encoder(tokenized_sentences,create_padding_mask(tokenized_sentences),False)\n",
        "  predictions=np.argmax(tf.keras.layers.Softmax()(logits),axis=1)\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "VVx3GgJhVSFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(encoder, open(filename, 'wb'))\n",
        "loaded_model = pickle.load(open(filename, 'rb'))"
      ],
      "metadata": {
        "id": "GjfnKlrCZGqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = predict(loaded_model,tokenized_test_inputs)\n",
        "labels = ['Not Sarcasm', 'Sarcasm']\n",
        "print(classification_report(y_pred, y_test, target_names=labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np5ZbybRVa92",
        "outputId": "f12414a8-199e-4210-f7de-b450b9e7f2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " Not Sarcasm       0.73      0.88      0.80      2491\n",
            "     Sarcasm       0.88      0.73      0.80      2961\n",
            "\n",
            "    accuracy                           0.80      5452\n",
            "   macro avg       0.81      0.81      0.80      5452\n",
            "weighted avg       0.81      0.80      0.80      5452\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_cm = confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "w-Ojt-j7XDTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix.make_confusion_matrix(test_cm, figsize=(8,8), categories=labels, percent=False,\n",
        "                                title = 'Confusion Matrix for Test Set')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "aiW65n1dZjyR",
        "outputId": "034ec620-e1f3-4ff6-b148-fbe8a9f238f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5Oc8H5oTZWiK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}